\documentclass{article}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage{verbatim}

\linespread{1.3}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\setcounter{secnumdepth}{0}
\setcounter{MaxMatrixCols}{20}
\renewcommand{\arraystretch}{1.5}

\newcommand{\ts}{\textsuperscript}
\newcommand{\diff}{\mathop{}\!\mathrm{d}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\var}{\text{Var}}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\p{\lparan}{\rparan}

\title{STAT3004 Assignment 1}
\author{Joshua Hwang (44302650)}

\begin{document}
\section{Q1}
We will attempt to show that $\var(X) = \expect \var(X|Y) + \var(\expect[X|Y])$

We first note some useful facts and tools:
\begin{align*}
    \var(X) &= \expect(X^2) - {(\expect X)}^2 \\
    \expect(aX+B) &= a\expect(X) + \expect(B) \\
    \expect(\expect(X|Y)) &= \expect(X) \\
    \expect{(X|Y)}^2 &= \expect(X^2|Y)
\end{align*}

These results have been proven in class and we shall take them as given
for this question.

\begin{align*}
    \expect \var(X|Y) + \var(\expect[X|Y])
    &= \expect\left(\expect({[X|Y]}^2) - {(\expect [X|Y])}^2\right)
        + \expect\left({\left(\expect[X|Y]\right)}^2\right) - {(\expect (\expect[X|Y]))}^2 \\
    &= \expect\left(\expect\left({[X|Y]}^2\right)\right) - \expect\left({(\expect [X|Y])}^2\right)
        + \expect\left({(\expect[X|Y])}^2\right) - {(\expect (\expect[X|Y]))}^2 \\
    &= \expect\left(\expect\left({[X|Y]}^2\right)\right) - {(\expect (\expect[X|Y]))}^2 \\
    &= \expect\left(\expect\left({[X^2|Y]}\right)\right) - {(\expect (X))}^2 \\
    &= \expect\left(X^2\right) - {(\expect (X))}^2 \\
    &= \var(X) \\
\end{align*}

\section{Q2}
\subsection{a}
We should remember that,
\begin{align*}
    \prob(X \geq x) &= \int_x^\infty f(t) \diff t \\
\end{align*}

With this (and Fubini's theorem) we can solve this,
\begin{align*}
    \int_0^\infty \prob(X \geq x) \diff x
    &= \int_0^\infty \int_x^\infty f(t) \diff t \diff x \\
    &= \int_0^\infty f(t) \int_0^t 1 \diff x \diff t
    & \text{Swap integrals} \\
    &= \int_0^\infty t f(t) \diff t \\
    &= \expect X & \text{Definition of expectation}
\end{align*}

\subsection{b}
We now extend our result for $\expect\left[X^\alpha\right]$.
\begin{align*}
    \int_0^\infty \alpha x^{\alpha-1} \prob(X \geq x) \diff x
    &= \int_0^\infty \int_x^\infty \alpha x^{\alpha-1} f(t) \diff t \diff x \\
    &= \alpha \int_0^\infty f(t) \int_0^t x^{\alpha-1} \diff x \diff t
    & \text{Swap integrals} \\
    &= \int_0^\infty t^\alpha f(t) \diff t \\
    &= \expect \left[X^\alpha\right] & \text{Definition of expectation}
\end{align*}

\section{Q3}
ZZZ - yeah got no idea
We recall that the PGF of a branching process gives the probability of
extinction.
\begin{align*}
    \eta_n &= \prob(S_n = 0) \\
    \eta_n &= G(\eta_{n-1}) \\
\end{align*}

\section{Q4}
\subsection{a}
The offspring distribution is either 2 or 0 with probability. Thus we can
model this as Bernoulli.
\begin{align*}
    X &\sim 2 \times Ber(p)
\end{align*}

The PGF is given by,
\begin{align*}
    G(z) &= \expect z^X \\
    &= z^2 \times p + z^0 \times (p-1) \\
    &= z^2 p + p - 1 \\
    &= \left(z^2 + 1\right) p - 1 \\
\end{align*}

\subsection{b}
From the lectures we know $\expect[S_n] = \mu^n$ and
$\var(S_n) = \mu^{n-1} \sigma^2 + \mu^2 \var(S_{n-1})$.

Where $\mu = \expect X$ and $\sigma^2 = \var X$.

As a piecewise function variance is
\begin{align*}
    \var(S_n) &= \sigma^2 \mu^{n-1} \frac{1-\mu^n}{1-\mu}
    & \text{When $\mu \neq 1$} \\
    \var(S_n) &= \sigma^2 n
    & \text{When $\mu = 1$}
\end{align*}

We find how $\mu$ changes with $p$
\begin{align*}
    \expect X = \mu &= 2 \times p + 0 \times (1 - p) \\
    &= 2 \times p \\
\end{align*}

It's clear that $p = 0.5$ it is critical, $p > 0.5$ is subcritical and
$p < 0.5$ is supercritical.

\subsection{c}
From the lectures we know we're looking, $\eta = G(\eta)$. Where $\eta$ is
the chance of ultimate extinction and $G(z)$ is the PGF of the offspring
distribution.
\begin{align*}
    \eta &= G(\eta) \\
    \eta &= \sum \eta^k \prob(X=k) \\
    \eta &= \eta^2 \prob(X=2) + \eta^0 \prob(X=0) \\
    \eta &= \eta^2 p + 1 - p \\
    0 &= \eta^2 p - \eta + 1 - p \\
    0 &= (\eta - 1)(p \eta + p - 1)  \\
\end{align*}

Our final solution now has $\eta = 1$ and $\eta = \frac{1-p}{p}$.
Certain chance is not an interesting result (and in fact it is an unstable
equilibrium) but our $\eta = \frac{1-p}{p}$ gives a stable probability
for the chance of ultimate extinction.

\section{Q5}
\subsection{a}
Can be done
ZZZ - check answer

\section{Q6}
\subsection{a}
ZZZ - does he actually mean $P(X \geq 4) = 0$? Because $P(X \geq 4) < 1$ is always
guaranteed

ZZZ - Also $B_n$ is the distance from the origin but my $S_n$ is about the balloons
popped at generation n. How do I compare them?

\subsection{b}

\end{document}
