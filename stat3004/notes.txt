Intuition: Stochastic just means random.
Examples: Branching processes, Poisson processes, Markov chains
(discrete time and continuous time), Random walks

Definition: A stochastic process (X(t), t in T)
is a collection of random variables indexed by
some indexed set T.
Intuition: Think of an array (finite or infinite) that returns a "random" number
at each t.

T is a totally ordered set. T could be the natural number or even the reals.
T is usually considered time. This implies the stochastic process X(t) has
order, a story, a known chain.

You can have a thing called a random field where T has no order.

Much like with a random variable Y we can specify the probabilistic behaviour
of a stochastic process X via its Finite-dimensional Distribution (FDD).

An FDD is specified by,
P(X(t1) <= x1, X(t2) <= x2, ... , X(tn) <= xn)
Seems familiar to a CDF in STAT2003.
Note: X could return an n-dimensional vector.

If X(t) (subset of) E (where E is some set)
then E is a statespace for X. E is where X takes all its values from.

X(t;Omega) means the stochastic process given Omega. X(t;Omega) is called a
sample path. (name should make usage obvious)

Probability Generating Functions-------------------------------------------
Recall the probability generating function from STAT2003. (It only works for
discrete random variables)
G(z) = Expectation z^X
G(1) = 1

It can also be represented by,
G(z) = sum from k=0 to infinity of z^k P(X=k)

G(0) = P(X=0)

Note that PGF this is also a polynomial!
Probabilities are always positive so we have a purely positive polynomial.
u <= v then G(u) <= G(v) (This is due to the fact that the PGF might be an
infinite series of terms ergo only [0,1] actually converges)

We take the derivatives for the PGF,
G'(0) = P(X=1)
G(differentiate k times) (0) / k! = P(X=k)

Observe that if two random variables share a PGF then they are exactly equal.

The lecturer derives the PGF of Poisson and Geometric distributions.
(See 23 July at 00:00)

Branching Processes----------------------------------------------------
We consider a stochastic process (S) where each "animal" has a random number
of children (X). (See W01-Branching_Processes_II.pdf)
We start the process off with S_0 = 1. X_{i,n} is the number of offspring of the i^{th}
individual of the n^{th} generation.
To simplify we let X = X_{i,n} for all i and n, it's the same probability for
everyone (also all independent).

We immediately have a few starting facts,
S_n = 1 + sum^{S_{n-1}} X

Will the animals become extinct? Does there exist a generation m such that
S_m = 0?

Perhaps an easier question is, the expected population of generation n.

Tricks we will use:
Expectation X = Expectation(Expectation(X | Y))
E[g(x)h(y)|X] = g(x) E[h(y)|X]
If X and Y are independent E[g(x)h(y)|X] = g(x) E[h(Y)]

We base the generation n off the generation n-1.
E(S_n) = E[sum from 1 to S_{n-1} of all X_{i,n-1}] (using trick number 1)
E(S_n) = E[E[sum from 1 to S_{n-1} of all X_{i,n-1} | S_{n-1}]]
E(S_n) = E[sum from 1 to S_{n-1} of all E[X_{i,n-1} | S_{n-1}]] (since X_{i,n-1} and S_{n-1} are independent)
E(S_n) = E[sum from 1 to S_{n-1} of all E[X_{i,n-1}]] (we assumed earlier all X_{i,n} are iid)
E(S_n) = E(X) E[S_{n-1}]

(We shall call E(X) = mu)

Now also using are initial condition S_0 = 1 we get
S_1 = mu
S_2 = mu^2
S_3 = mu^3
...
S_n = mu^n

This is clearly a geometric progression.
mu < 1 then it approaches 0
mu = 1 then population remains 1
mu > 1 then population blows up

We can repeat this process to find the variance of each generation.

Var(S_n) = E(Var(S_n | S_{n-1})) + Var(E[S_n | S_{n-1}]) (This is a law called the law of total variance)

We attempt to solve the first term of this equation:
Var(S_n | S_{n-1}) = Var(sum from 1 to S_{n-1} X_{i,n-1} | S_{n-1}) (Using something similar to what was done previously)
Var(S_n | S_{n-1}) = S_{n-1} sigma^2 (We already found expectation of S_{n-1})
Now we find the expected variance
E[Var(S_n | S_{n-1})] = E[S_{n-1}] sigma^2
(sigma is variance of X)
E[Var(S_n | S_{n-1})] = mu^{n-1} sigma^2 (does not apply for n=0)

We solve the second term:
E[S_n | S_{n-1}] = mu S_{n-1}
Var(E[S_n | S_{n-1}]) = Var(mu S_{n-1})
Var(E[S_n | S_{n-1}]) = mu^2 Var(S_{n-1})

Thus our final solution is,
Var(S_n) = mu^{n-1} sigma^2 + mu^2 Var(S_{n-1})

This is now a recursive definition. We can actually expand it to give us,
Var(S_n) = sigma^2 mu^{n-1} (1 + mu + mu^2 + ... + mu^{n-1}) (not for 0)
An even better form is,
Var(S_n) = sigma^2 n when mu = 1
Var(S_n) = sigma^2 mu^{n-1} (1-mu^n)/(1-mu) for all others

Observe:
mu < 1 then Var(S_n) approaches 0 as n approaches infinity
mu >= 1 then Var(S_n) approaches infinity as n approaches infinity

Now we may ask the earlier question about probability of extinction
eta_n is probability of extinction at nth generation
eta_0 = 0 since S_0 = 1

We want to find P(S_n = 0) when n approaches infinity
P(S_n = 0) = P(S_n < 1)
= 1 - P(S_n >= 1)

We now use Markov inequality

Markov's inequality-----------------------------------------------------------
For ANY random variable X >= 0
and any a > 0
P(X >= a) <= Expectation(X)/a

I{event A} = 1 if event A occurs and 0 otherwise

Proof:
E[X] = E[X I{X<a} + X I{X >= a}]
E[X] = E[X I{X<a}] + E[X I{X >= a}]
E[X I{X<a}] is greater than zero from the initial statements
E[X] <= 0 + E[X I{X >= a}]
E[X] <= 0 + a E[I{X >= a}] [I don't know how E[X I{X >= a}] <= a E[I{X >= a}]]
E[X] <= 0 + a P(X >= a) [I don't know how E[X I{X >= a}] <= a E[I{X >= a}]]

Coming back to our previous question...
1 - P(S_n >= 1)
P(S_n >= 1) <= E(S_n)/1 = mu^n

Thus we can conclude that if mu < 1 then extinction is 100%

We currently know a few things about the probability of extinction.
eta = 1 when mu < 1
eta = 0 mu >= 1 AND variance = 0

We now attempt to find what happens when mu >= 1 and variance != 0.
eta_n = P(S_n = 0)
= E[P(S_n = 0 | S_1)] (We could've done this for any abitrary S_m)
= sum from 0 to infinity P(S_1 = k) P(S_n = 0 | S_1 = k) (definition of expectation)
= sum from 0 to infinity P(X = k) P(S_n = 0 | S_1 = k) (Since S_0 = 1 and recall X is the number of children of a single animal)
The probability we go extinct is determined by all animals in the previous generation failing to bear children thus,
= sum from 0 to infinity P(X = k) P(S_{n-1} = 0)^k
We observe this is the PGF of eta_{n-1}!
= G(eta_{n-1})

(G is the PGF of X)

Since we have a fixed initial state we can actually generate each eta_n.

(See 24 July at 23:00 if things are confusing or require justification)

Now we want to find eta_infinity or just eta which requires eta = G(eta).
This is a fixed point solution. In our solution we already know of G(1) = 1,
which is extinction, but are there more?

We highlight some more useful facts about the PGF. It is an upwards curving
function (at least on [0,1])

This means there are only two possible curves we can get. (See 24 July at 23:00)

Option 1 which has two fixed points. (1,1) (extinction) and somewhere else. It is caused by mu > 1.
Option 2 has the single fixed point at extinction. It is caused by mu <= 1.

By using the fixed point algorithm from COSC2500 we can find the fixed point
solutions. Also note that in Option 1 the interesting fixed point is stable
while the one at extinction is unstable. In comparison, Option 2 has extinction
as a stable fixed point.

[The lecturer then gives some examples of different X variables and solves them 24 July at 43:00]

Random walks-------------------------------------------------------------------
The most elementary stochastic process is an infinite set of iid variables.
(This is not interesting since it's just a bunch of independent states).

Perhaps a more interesting process would be a random walk.
We define the scenario as such. Where S_i is the i^{th} state of the
random walk while X_i is some random variable (the Xs should all be iid).
S_0 = X_0 = x_0 (starting)
S_n = S_{n-1} + X_n

If we were to evaluate and expand this recursive definition we would find
That S_n is just a sum of many iid random variables. By the Central
Limit Theorem this means that S_n approximates a normal distribution.

S and X do not need to have the same state-space (Bernoulli and Binomial).

An example could be coin flips which determine
walk forward or not at all.
Coin flips are Bernouilli [ZZZ spell check] and summing these coin flips gives
Binomial distribution.
It is important to remember that these random variables are dependent of
each other. e.g. S_{n+1} | S_n \sim S_n + Ber(p) and
S_{n+k} | S_n \sim S_n + Bin(k,p)

[see 29 July 30:00 for proof]

Magically we find the the Covariance of S_{n+k} and S_n is np(1-p) for
our above example.

Using this we can find our correlation to be \sqrt{\frac{n}{n+k}}

We explore a new question with random walks. Does the walker reach a desired
destination and if they do when?

Side note, if the walker is required to move every step then the walker is
required to flip odd and even number locations at each time step. (pretty neat)

Let \tau_d be the first time the random walker reaches the desired destination.
If we never reach home then \tau = \infty.
Another way to define \tau_d is the infinum of the set {n \in naturals : S_n = d}
(the set of times n that satisfy the random walker achieving a desired value)

We redefine our question concisely as P(\tau_d < \infty)

We define r_k = P(\tau_d < \infty | S_0 = k)
r is the probability that we reach our destination in finite time knowing
our starting position was k. Our question gets redefined again to become
what is r_0?

We observe a trivial case, r_d (the starting position is the desired position)
gives certain chance.

We also note that r_{d-i} = r_{d+i} there is equal chance getting i steps
away from the starting point on either side.

We perform a method called First-step analysis (aka conditioning).
This method involves considering everything that could happen in one unit
of time.
We expand r_k using conditional probability,
r_k = P(\tau_d < \infty | S_0 = k)
= P(\tau < \infty | S_0 = k, X_1 = +1) P(X_1 = +1 | S_0 = k)
+ P(\tau < \infty | S_0 = k, X_1 = -1) P(X_1 = -1 | S_0 = k)

We note that P(X_1 = +1 | S_0 = k) are independent so in reality it's just
P(X_1 = +1) which we'll assume is \frac{1}{2} for an unbiased random walk.
r_k = \frac{1}{2} (P(\tau < \infty | S_0 = k, X_1 = +1) + P(\tau < \infty | S_0 = k, X_1 = -1))

We further simplify since knowing S_0 = k and X_1 = +1 we in actuality know
S_1 = k+1.
r_k = \frac{1}{2} (P(\tau < \infty | S_1 = k+1) + P(\tau < \infty | S_1 = k-1))

Now we note that time is arbitrary and we let time 1 = 0 and time 0 = -1
r_k = \frac{1}{2} (P(\tau < \infty | S_0 = k+1) + P(\tau < \infty | S_0 = k-1))

From here we realise these are just the r_k definition again
r_k = \frac{1}{2} (r_{k+1} + r_{k-1})

We said earlier that r_{d-i} = r_{d+i} (we're letting d=0)
r_k = r_{k-1}

This works for all k and we also know that r_0 = 1 thus r_k = 1.

[To see another two examples of using single-step analysis see 31 July 00:00]
We run an example involving two \taus, bankrupt and money goal.
He goes indepth on this example and how to solve it.
Another example takes into consideration that time is important (previously
we recentered time but now we can't)

Markov processes--------------------------------------------------------------
We define (X_t, t\in T) as a stochastic process % meant to be a fancy T
Where X is the state at time t. A Markov processes must be closd on its
states. The probability you land somewhere else in the state space is
100%.

A Markov process has a neat property where,
if for any s,t \in T and any A \subset E (Where E is the state space).
Then
\prob(X_{t+s} \in A | T_t) = \prob(X_{t+s} \in A | X_t)
[probability of state X at time t+s knowing all
history up to and including t]

This means that our process only depends on the latest past state. History
beyond this does not matter.

This is known as a discrete time markov chain. This is because we are using
discrete timesteps and discrete states. [Later on we will discuss discrete
state continuous time markov chains]

Insane notation:
P_{ij}^m (n) means going from state i at step n to state j at step n+m

We explore an example using P_{ij}^2 (n), this demo teaches us about
exploiting the simplification of history.
We're currently at time n and we move to time n+1.
\sum_{k\in possible states} P(X_{n+2} = j | X_n = i, X_{n+1} = k) \times P(X_{n+1} = k | X_n = i)
\sum_{k\in possible states} P(X_{n+2} = j | X_{n+1} = k) \times P(X_{n+1} = k | X_n = i)
\sum_{k\in possible states} P_{kj}^1 (n+1) \times P_{ik}^1 (n)

Now we just calculate a single step. Easy.
Amazing observation, this is just matrix multiplication! i,j represents the
location in the matrix. Note: P_{ij}^1 (n) may have a different matrix to
P_{ij}^1 (m) since it is at a different time.

P^m (n) = P^1 (n) \times P^1 (n+1) \ldots P^1(n+m-1)
[keep in mind matrix multiplication is not commutative]

We create an initial probability represented as \pi^{(0)}_x = \prob(X_0 = x)
each element (probability at the state x initially).

[The notation he goes on to make is too hard to do,
he says we won't ever see it again. See 45:00 5 Aug]

Every row of the matrix determines "where could I go given I'm in row i"
[ZZZ redo this thinking]

If you have a countably infinite number of states a matrix won't do you
any good. Instead we call upon the help of linear operators.

Figuring out what happens to a markov chain at infinite time.
We make use of fancy shmancy eigenvalue and eigenvectors.
If the state space is finite and all eigenvalues are distinct then we
have a diagonalisable matrix.
P = V \Lambda V^{-1} where \Lambda is the diagonal matrix with the diagonals
being the eigenvalues.

This diagonalised state is useful since we can add powers which
only adds those powers to the diagonalised part.

Eigenvectors and eigenvalues-------------------------------------------------
[ZZZ spectral analysis is NOT the usual diagonalisation with V and V^{-1}]
We will use the spectral method to find the diagonalisation.
We attempt to solve a more general (and hopefully easier form)
P = V \Lambda W [note that it isn't V^{-1}]

We first denote what a left eigenvalue and a right eigenvalue is.
Right: v_i is a column vector
P v_i = \lambda v_i
Left: w_i is a row vector
w_i P = w_i \lambda_i

We desire our eigenvectors to multiply to give 1; w_i \dot v_i = 1.

We prove that if all \lambda are distinct then (w_j v_i = 0). The eigenvectors
will be orthogonal.
[ZZZ i'm hungry so i didn't write the proof. See 35:15 6 Aug]

We use this fact to prove that W = V^{-1}
[ZZZ proof not written down because i'm hungry See 35:15 6 Aug]

[ZZZ a bunch of properties that i'm guessing are important but I can't seem
to understand the relevance yet See 35:15 6 Aug]

If our markov process behaves nicely (as determined by the stuff up above)
then our process will have a limiting distribution at infinity. We denote
this distribution as a vector \pi.

From the bunch of stuff that happened above we can rewrite our P matrix as
P^n = \pi + \sum \sigma_k^n R_k where R_k = v_k w_k
From observation we can clearly see that |\sigma| < 1 for all our \sigma
then our limit exists and in fact is \pi.

Gershigerin[?] Circle Theorem (brief)
Create closed discs D_i in the complex plain with centres at p_{ii}
and radii r_i = 1 - p_{ii}. Then every eigenvalue \lambda_i of p lies in at
least one of the {D_i} [See 15:00 7 Aug for the intuition]

This theorem means that all the eigenvalues will be less than or equal in
distance of 1 from the origin. (also the limiting distribution is unique
as long as finite states)

We attempt to solve a fixed point system \pi P = \pi
Here \pi is known as a stationary distribution. The fixed point distribution
is a good method for finding the limiting distribution.

[example of solving such an equation at 31:00 7 Aug]

[00:00 12 Aug
The lecturer considers a toy example Markov chain involving two states.
He creates a matrix describing this system and performs spectral analysis on it.
He is able to perform the analysis quickly since he knows a few tricks like
the trace of the original matrix is equal to the sum of the eigenvalues
and since the matrix is normalised he also knows one of the eigenvalues
is 1.]

[18:00 12 Aug we now look at the even simpler state of the identity matrix.
This system means that each state is only allowed to remain in the same
state. This is an interesting system since the initial distribution is also
the limiting distribution (can't go anywhere). It also means that all
distributions are the limiting distribution; also stationary.]

[23:00 12 Aug an actual demonstration of spectral analysis]

The rest of lecture talks about all 5 systems that happen from a two state
system.
* We stay in the same state (identity)
* We travel directly to the next state ((0 1) (1 0))
* We have a chance of changing to a new state but this new state kicks us back
((1-s s) (1 0))
* We have a chance of changing to a new state but this new state traps us
((1-s s) (0 1))
* The normal system ((1-s s) (r 1-r))

We now explore more complicated Markov chain systems.
We create some new notation and revise some old ones.
Recall,
\prob_x() = \prob( | X_0 = x)
\expect_x() = \expect( | X_0 = x)

We now write,
\tau_y = inf{n > 0 : X_n = y}
\tau_y^{(k)} = inf{n > \tau_y^{(k-1)} : X_n = y}

\tau refers to the amount of time, n, taken for us to reach state
y. The \tau_y^{(k)} refers to the kth time we've reached state y.
This is clearly larger than the previous \tau as it has to be
later in time.

r_{xy}^{(k)} = \prob_x (\tau_y^{(k)} < \infty)
This means what's the chance this process goes on forever and
never touches y knowing that it started at state x.
The k involves the kth time we reach state y.
If r = 1 then it's guaranteed that we wll reach state y. If r = 0
then it's guaranteed to never reach state y. If it's anything
in between then there's a chance we never get to state y.

We now implement this new notation.
r_{yy}^{(2)} = \prob_y(\tau_y^{(2)} < \infty)
= \prob_y(\tau_y^{(2)} < \infty | \tau_y^{(1)} < \infty)
\times \prob_y(\tau_y^{(1)} < \infty)1−µ

But \prob_y(\tau_y^{(1)} < \infty) is just the chance it happens
at all so.
= \prob_y(\tau_y^{(2)} < \infty | \tau_y^{(1)} < \infty) \times r_{yy}

Since the Markov system only relies on the immediate history
AND note the \prob_y which means we're starting from y like the
previous time. It's also helpful to know that the previous k actually
exists wihch further confirms our conclusion that the two
probabilities are iid.

= \prob_y(\tau_y^{(1)} < \infty | \tau_y^{(1)} < \infty) \times r_{yy}
[We know \tau_y^{(2)} has the same distribution as \tau_y^{(1)}
that's why we're getting rid of the conditional but keeping the
distribution]
= \prob_y(\tau_y^{(1)} < \infty) \times r_{yy}
= r_{yy} \times r_{yy}
= r_{yy}^2 [This is referred to as the Strong Markov Property]

Intuitively we see the r_{yy}^{(k)} = r_{yy}^k.
We can now see some interesting implications.

If r_{yy} < 1 then the chance of hitting state y after a large
amount of time is now becoming impossible. We call this y a
*transient state*.

If r_{yy} = 1 then we are always guaranteed to hit our state y.
This is called a recurrent state.

We now move on to r_{xy}.

If r_{xy} > 0 then we say that x "leads to" y and write x \to y.
[It's possible that y does not lead back to x]

If x \to y AND y \to x then we say the x and y communicate.

The relation \to is transitive since give x \to y and y \to z then
x \to z. [Proof of this at 32:00 13 Aug]

If x is recurrent and x \to y then y is also recurrent and y \to x.
[Proof at 47:00 13 Aug]

A set of states in a Markov system is called a communicating class
if \forall x,y \in A x \leftrightarrow y and if z not in A then there
is no x \leftrightarrow z.

If the communicating class is the entire state space then the state
space is called irreducible.

A set of states is called closed if r_{xy}=0 for x \in A and y \in A^c.
Equivalently, states in A can only reach other states in A.

Now if A={a} then the state "a" is called absorbing.

We can actually simplify our Markov system and categorise them via
transience or recursiveness.
The a few of the top rows will contain all transient families while
the remaining rows
will be a diagonal of recurrence families. [Better example at 19 Aug 22:00]

If C is a finite, closed, communicating class then all states in this family
are recurrrent.

As an aside if our system is irreducible, positively recurrent
and aperiodic then we have a unique
stationary distribution. i.e. no matter what initial starting we always land
back to the unique stationary distribution.

We construct an identifier
I_x = \{ n \geq 1 : p^n_{xx} > 0}
This means that I is the set such that the probability of ending back
at state x after n steps is greater than 0.

The period of a recurrent state family is determined by the gcd
of all n in the set I_x. If gcd(I_x) = 1 then the set of states is aperiodic.
Interestingly, if x \leftrightarrow y then x and y have the same period.

Redefine recurrent states for infinite state systems-----------------
The idea of \prob_x (\tau_x < \infty) = 1 means that we will certainly
arrive back at x. Before it was simpler because there were only finite
states it meant that arriving back to state x only took finite time.

Instead if we have an infinite number of states we require new
definitions. \expect_x [\tau_x] < \infty then x is positive recurrent.
If \expect [\tau_x] = \infty then x is null recurrent.

Global Balance Equations--------------------------------------------------------
We return to the problem finding a stationary distribution, \pi P = \pi
We know attempt to do this for an infinite number of states. Such a system
is unable to be represented by a matrix.

Instead we try and solve a Global Balance Equation
\sum_{x\in E} \pi_x P_{xy} = \pi y
I believe this equation means for the state y it's equivalent to summing up
every other state multiplied by the probability state x goes to state y.

This equation is hard to solve in general. Instead we'll attempt to solve
a Local Balance Equation. They are of the form,
\pi_x P_{xy} = \pi_y P{yx}
and solving these will eventually give us a solution to the global balance as
well. Local balance seems to be a subset of all possible global balances since
all local balances are global but not all global are local balances.
This is just the balanced flux equation from STAT2004.

We solve an example with our local balancing equations with doubly stochastic
systems. [14:00 20 Aug]

[0:00 21 Aug random walker example with single boundary]
[14:00 21 Aug explains how a branching process IS a markov DTMC]

You can construct a y_m = x_{n-m} where 0 \leq m \leq n.
This y is called the reverse chain. It visits the states that X
did but in reverse.

Let's say we want to find a distribution such that the markov chain
x and its reverse y cannot be told apart. This is called being
reversible. This only occurs when the distribution is stationary.

Kolmogorov cycle condition-------------------------------------------
If X is irreducible then X is reversible iff P_{ij} > 0 implies
that P_{ij} > 0 and for any loop of states (which we can get to) it
holds that
\prod_{i=1}^n p_{x_i x_{i-1}} = \prod_{i=1}^n p_{x_{i-1} x_i}

If Kolmogorov cycle condition holds then X has a stationary
distribution.

If X is irreducible and positive recurrent then there is a
*unique* stationary distribution. Moreover if X is also aperiodic
then all distributions lead to the limiting distribution (which is
also the stationary distribution.

Poisson Processes-------------------------------------------------------------
We first introduce this topic with two simpler, examples.
Consider a random walker that flips a coin and either moves forwards
or stays still. At each time stage n the result of the coin flip is B_n.
The overal distance the random walker has gone after n flips is S_n.
The process S_n is called a *Bernoulli process*.

Now we start to consider continuous time but with discrete counts still.
We now have a function N(t) which is the continuous analogue of S_n with
continuous time t. At each jump from N(t) = 0 to N(t) = 1 we become piecewise.
N(0) = 0 and N(t) \in naturals.
This is called a *counting process*. We define each T_i as the time when
the ith jump happens.
An obvious but vital insight is
{N(t) \geq n} = {T_n \leq t}
Which says, if the counter is larger than n at time t then the time this
happened is less then time t.

Now a Poisson process is a counting process if for any interval I \in reals then
N(I) \sim Poi(\lambda |I|)
Any interval in the counting set has a distribution similar to a poisson
distribution. Also all disjoint intervals must be independent.

[43:14 26 Aug shows some kind of proof/derivation to produce a
Poisson counting process]

For a Poisson process we have various intervals before each jump in count.
i.e. from t=0 to t=A_1 called interval A_1, from t=A_1 to t=A_1+A_2 called
interval A_2. We consider each interval a random process and in fact
are all iid Exponentials with lambda as the parameter.

We claim if we let (N_t, t \geq 0) be a Poisson process with rate \lambda > 0.
Then for any fixed time s > 0, any time from 0 to s (N_r, 0 \leq r \leq s)
and any time in the interval (N(t+s) - N(s), t \geq 0)
[note that N(t+s) - N(s) is equivalent to N(s,t+s]] are independent poisson
processes AND independent of each other. This is very like the memoryless
property we see in exponential distributions and the like.

A Poisson process can be recognised by the following definition.
\prob (N(t+h) = m | N(t) = n) has four possibilities.
If m < n then there's no chance. It's a counting process, it can't go down.
If m = n then 1 - \lambda h + o(h).
if m = n+1 then \lambda h + o(h). Exactly one jump has occurred in the interval.
If m > n+1 then o(h).

This o(h) is the opposite of the O(h) we see in algorithm analysis.
We use it to ensure o(h)/h \to 0 when h \to 0.

If we have a process that follows these probabilities then we define it as
a Poisson process.

[Page 4 Lecture 16 The lecturer then derives a Poisson process from
t/h Bernoulli processes (one for each point in time) as h \to 0 we
end up getting the Poisson process]

We now attempt to prove that the times of our counting events in our counting
process (knowing that the count N(t) = n) has the same distribution as an
ordered list of uniform random variables. [15:00 28 Aug for the proof]

The PDF of n random variables is 1/t^n in the region and 0 otherwise.
The PDF of n sorted random variables is n!/t^n in the region (which is now
smaller btw) and 0 otherwise.
Think of this like the uniform random variables have been realised. There
are n! ways of ordering them and we funnel all n! possibilities into the same
sorted list thus n! \times 1/t^n.

We can simulate a Poisson process in two ways. One which is fast and smart
and another which is naive.

Naive making use of the fact that the event times are random exponentials:
T = {}
T[0] = 0
n = 0
while T[n] < t
    T[n+1] = T[n] + Exp(\lambda)
    n = n+1

Smart making use of what we've just learnt:
N = Poi(\lambda*t)
Generate N Unif(0,t) into list U[]
Sort U[]
Return T[k] = U[k]

Adding two Poisson processes \lambda_1 and \lambda_2 will give another with
\lambda_3 = \lambda_1 + \lambda_2. This is called the superposition of PP.

Non-homogeneous Poisson process-----------------------------------------------
Previously our Poisson process depended on a constant lambda for all time.
Instead we consider lambda a function in time. [It's possible to use the
same derivation as show in Lecture 16]

In essence we get N(a,b] \sim Poi(\int_a^b \lambda(s) \diff s)
We get the integral in there which is in essence just a summation where
h \to 0.

Should be noted some of our previous properties we've found for homogeneous
Poisson processes are no longer true. The interval times are no longer
exponentially distributed (so no longer memoryless) and they are no longer
independent.

Should be noted that disjoint areas are still independent.

[00:00 2 Sep has an example of calculation]

Superposition of non-homogeneous Poisson processes will actually give another
Poisson process.

Spatial Poisson Process---------------------------------------------------
AKA Poisson Random Measure and the Law of Small numbers
[see 35:00 2 Sep for why]
We have previously used Poisson processes
over a single dimension (time for example).
We extend this model for multiple dimensions.

We rebuild the model and derive new but natural extensions of the original
assumptions.

Previously we have that the count should start at 0 at time 0.
Now we have that the empty set (no space) should have 0 count.

The count in area A is determined by N(A) ~ Poi(\lambda |A|) where is the
area/volume measured to a single number naturally (probably integrate).
(This is for homogeneous, for nonhomogeneous expect integration with weighting).

Disjoint areas are still independent.

Thinning Non-homogeneous Poisson Process------------------------------------
This is a subset of the non-homogeneous poisson processes which are easy
to calculate on a computer. [see 00:00 Sep 3 for more information]

Essentially we have an upper bound and constant \lambda and we make it variable by
applying a probability that the point gets accepted p. This p is variable with
time so it might increase/decrease over time. Now \lambda(t) = p \times \bar{\lambda}.

Should be obvious but the \bar{\lambda} we choose must be an strict upper bound to
ensure we can actually and properly map \lambda(t) \to \bar{\lambda} p(t).

We can be sneaky (especially if \lambda has no maximum). Simply cut off our computation
at some time. Once we have finite time we have a guaranteed maximum.

[The lecturer describes the algorithm explicitly at 30:00 3 Sep]

Continuous time markov chains------------------------------------------
We now take our attention back to markov chains and more specifically
continuous time markov chains.

A CTMC must hold the markov property (history doesn't matter only the latest
state does)

We denote transition probabilities which are defined as such:
p_t (i,j) = p(X_t = j | X_0 = i)
Which means "what is the probability that we go from state i to state j in time t".
(Note that we're starting at time 0. In the homogeneous time case this is fine
and equivalent for any time interval. For non-homogeneous this is untrue)

Not necessarily true but we can make use of a "standard property" of transition
probabilities. As the time interval approaches 0 our probability becomes 0 for all
other states except its own. In other words when no time has passed there is a 100%
chance we stay in the same state. In this course we only deal with "standard".

We note that we can split out time interval into two parts such that
p_{t+s} (i,j) = p(X_{t+s} = j | X_0 = i)
= \sum_{k \in E} p_s(i,k) p_t(k,j)
Where k is the intermediate step.
This is called the Chapman-Kolmogorov equations.

From this we can see that we can actually write our transition probabilities into
a matrix as we've done with our DTMCs.
P_{t+s} = P_s P_t
Where we calculate the transitions for each specific time.

Also note that with our standard property that as the time interval goes to 0
our matrix becomes the identity matrix.

We introduce a new variable W_t which measures how long we remain in the current
state at time t.
p(W_t > w | X_t = i)
If our markov chain is in state i now what is the probability we spend w time units
(or more) at this state i. In time homogeneous we equivalently have
p(W_0 > w | X_0 = i)
We create a new function h(w) = p(W_0 > w | X_0 = i).

With the reasoning at [38:00 4 Sep] (mainly just using the Markov property)
we prove that h(u+v) = h(u) h(v) when u,v > 0

From this we get given,
h(u) = e^{-cu}
for some constant c.
This is the only solution for h(u).

Thus for a homogeneous time CTMC
p(W_0 > w | X_0 = i)
follows an exponential distribution. We shall call the parameter for this
distribution q_x. For state x.

Thus to completely specify the finite dimension distributions of a standard
time-homogeneous CTMC, we need only specify two ingredients.
The collection of exponential "holding time" parameters and the one-step transition
probabilities. Note that when we jump we ensure we can't jump back to the same
state, it should be a genuine jump.

Another case we won't explore is if our waiting time has q_x = +\infty.
In this case we are guaranteed to jump instantaneously; zero wait time.
On the other hand q_x = 0 means we won't leave the state in finite time.

Consider K_{x,y} is the probability we jump from state x to state y.
We also define q_{x,y} to be q_x K_{x,y}. So the parameter times the probability
of jumping to state y.
Since the \sum_y K_{x,y} = 1 then \sum_y q_{x,y} = q_x

We prefer this q_{x,y} notation so we'll redefine
K_{x,y} = \frac{q_{x,y}}{q_x} + I\{x=y\}.

[See 25:00 9 Sep for interesting example of markov chain explosion]
Also note that given q_x = \lambda and
K_{x, x+1} = 1 (so guaranteed to jump to the next state)
is precisely a Poisson Process

p(\lim_{n\to\infty} T_n = \infty) = 0
then we say that this is an explosion. The infinite jump has occurred in finite time.

p(\lim_{n\to\infty} T_n = \infty) = 1
then we say the markov chain is regular.

Echoing the same ideas as the pmatrix for transitions we have a qmatrix for the
exponential parameter q_{x,y}. The only problem is that q_{x,x} doesn't exist
so we'll define it as q_{x,x} = -q_{x}

Thus if we were to construct the qmatrix all our rows will sum to zero.
By choosing q_{x,x} = -q_x, we have constructed a singular matrix.
A singular matrix is not invertible and an eigenvalue for this matrix is 0.

Now we can use a bunch of insane reasoning and equations from 10 Sep,
P'_t = Q P_t
Which creates a system of differential equations given the probability matrix
at time t, P_t. This is known as the Kolmorgorov Backwards Equations.

Also under the finite and "standard" assumptions this equation is commutative.
P'_t = P_t Q.
Additionally, P_t = e^{tQ} is the unique solution if we have a finite
state space.
To find the e^A where A is a matrix you make use of the Taylor expansion.
So in reality e^{tQ} = \sum_{k=0}^\infty \frac{(tQ)^k}{k!}

Limiting distribution------------------------------------------------
Previously we had powers of our matrix to perform what we wanted.
But now our P_t = e^{tQ}. Luckily we can perform
a spectral decomposition which leads us to
= e^{\lambda_1 t} R_1 + e^{\lambda_2 t} R_2 + e^{\lambda_3 t} R_3 ...
+ e^{\lambda_1 p} R_p
We know that one of the eigenvalues is 0
= R_1 + \sum_{k=2}^p e^{\alpha_k t} (cos(\beta_k t) + i sin(\beta_k t))

Note that this looks very familiar to the softened spring we used to work with
in MATH1051.
As you remember in special circumstances we can achieve
lim_{t\to\infty} P_t = R_1

We can get the limiting distribution without even getting our P matrix.
Just using the Q matrix if we find a distribution that satisfies
\pi Q = 0 then \pi P_t = \pi
Should be noted that we can only do this for the left eigenvector and only
works if P_t = e^{tQ}. Also observe this only works one way.

Suppose we can split our Q matrix into D(K - I) where K is the jump chain matrix
I is the identity and D is the diagonal matrix with the exponential holding times.
Using only the jump chain matrix (which is essentially a discrete version without
holding times) we can sometimes get the limiting distribution of the CTMC
just from \nu which are the rows of K.
\pi_x \prop \nu_x/q_x
\pi_x = \frac{\nu_x/q_x}{\sum_y \nu_y/q_y}
Just ensure the the denominator doesn't go to infinity. If it is infinity then
then correct solution is 0 for all \pi_i.

Kolmogorov Cycle Condition-----------------------------------
Consider a cycle chain where all states are connected to the two neighbours
and the ends of our chain are connected. The probabilities don't need to be
the same they just need to be non-zero.

Kolmogorov Cycle Condition states that if
\prod_{i=1}^n q(x_{i-1}, x_i) = \prod_{i=1}^n q(x_{i}, x_{i-1})

Then there exists solution to detailed/local balance equations.
Detailed balance equations should be familiar as,
\pi_x q_{xy} = \pi_y q_{yx}

Without even doing any calculations there are a few cases where the KCC holds.
Tree-like transition graphs. Where each edge is undirected and no cycles exist
(the cycle exists by taking the same set of states forwards and backwards).

Categorising states in CTMC----------------------------------
Much like in discrete models we have r_{xy} the only two differences
is that r_{xx} is chance or return and no longer includes chance of staying.
The other is the definition of periodic/aperiodic breaks down since we have
variable holding times.

If the CTMC is irreducible and recurrent then no matter what starting position
we have,
\lim_{t\to\infty} P_x(X_t = y) = \pi_y

The probability that by infinite time we end up in state y has equivalent probability
as the limiting distribution at y. \pi_y > 0 if state y is positive recurrent.

Probability Measure Theory------------------------------------------------------
Much like what we've had at the start of STAT2004.

An event is a subset of \Omega.
An event set is a set of various events. Events are questions of the form
"did one of these outcomes occur".
The empty set is an event.

The most natural event set is the powerset of \Omega

For an event set to make sense we must have the following,
the empty event,
the complement exists for all events,
closed under unions.
This is also sometimes called an algebra.

We call a set of subsets a monotone class if either there is some ordering
of the subsets and each consecutive set is a subset of the next. Or the other
way where each consecutive set is a superset of the next.

We define a \sigma-algebra if \Omega \in event set, the complement exists
and the infinite union of elements set is closed.

Note that a \sigma-algebra then it is also an algebra but not the other way
around.

A set is a \sigma-algebra if and only if it is an algebra and is a monotone
class.

[Working through at 25 Sep]
The intercept of two \sigma-algebras is also a \sigma-algebra. This is not
always true for the union however.

Borel \sigma-algebra is a special type of \sigma-algebra that attempts to
work for the uncountably infinite for example the real numbers.
Within the \mathscr{B} contains all possible intervals (both closed and open
on both ends independently). This is due to the requirements of a \sigma-algebra

The Kolmogorov axioms of probability are,
P(\Omega) = 1
P(A) \geq 0 \forall A \in event set
If A_i are mutually dijoint evets then P(\cup^\infty A_n) = \sum^\infty P(A_n)

From these axioms we have some consequences,
Boole's inequality: P(\cup^\infty A_n) \leq \sum^\infty P(A_n) but A_i may not
be disjoint

Continuity from below: If A_1 \subset A_2 \subset A_3 ... then
P(A_n) = P(\cup^\infty A_i)

Continuity from above: If A_1 \superset A_2 \superset A_3 ... then
P(A_n) = P(\cap^\infty A_i)

Random variables--------------------------------------------------------------
We call a sequence mutually independent if the probability of the intersection
of events if equal to the product of probabilities.
P(\bigcap A_i) = \prod P(A_i)
This is more powerful than the pairwise version since mutual independence
extends for any selection.

We must explore two new concepts. Consider a countably infinite sequence of
events. In normal sequences (involving numbers and not sets) we may have a nice
sequence that approaches something in infinity or we might have a sequence
that fluctuates between two points which means we have a limsup (limit for
the upper bound) and liminf (for the lower bound).

With sets we consider if an element exists in all sets in the sequence
(infinitely often)
or if they exist in a few but still countably infinite number of sets
(all but finitely many).
These create the limsup and liminf respectively.

{A_n i.o.} = \bigcap_{n\geq 1} \bigcup_{n\geq m} A_n
Which means we're taking the union of the tail sequence then
checking that if we were to intersect all of the we would find elements
which are infinitely often.

{A_n, a.b.f.m.} = \bigcup_{m\geq 1} \bigcap_{n \geq m} A_n
Consider all possible tails A_n. We take all tails that contain
an element that remains in all A_n (n \geq m). (Hopefully makes sense)

You may have noticed that these definitions contain intersects and unions
which could be complemented,
\liminf A_n = (\limsup A_n^c)^c

Note that {A_n a.b.f.m.} \subset {A_n i.o.}

Borel-Cantelli Lemma
If the sum of P(A_n) is non-infinite then P(A_n i.o.) = 0.
This means that there is not chance that something happens infinitely often.

If the sum of P(A_n) is infinite and A_n are mutually independent then
P(A_n i.o.) = 1.

Now we define what a random variable is.
A random variable is simply a function that maps \Omega \to the reals.

We also introduce the idea of random vectors [which I'm guessing will
be expanded on next lecture]
This leads to the idea that two random variable (sharing the same
input) can be dependent on each other.

We define the pre image of a random variable X^{-1}(B) as the set of all
elements in \Omega that maps to elements in B (which is a subset of the reals).

Lebesgue's Decomposition Theorem------------------------
States that all cdf F(x) can be decomposed as a weighted sum
F(x) = \alpha F_d(x) + \beta F_{a.c.}(x) + (1 - \alpha - \beta)F_{s.c.}(x)
Where F_d(x) is discrete and creates stepwise cdfs,
F_{a.c.}(x) stands for absolutely continuous which is your standard continuous cdfs.
F_{s.c.}(x) stands for singular continuous which is a weird form of continuous you
haven't seen before.

Convergence------------------------
We recall the classic pointwise convergence
lim_{n\to\infty} X_n(w) = X(w) \forall w \in \Omega

But when dealing with random variables it does not make sense to do this.
Instead we take a different approach,
P(\lim_{n\to\infty} X_n = X) = 1
Thus we call this X_n \to^{a.s.} X
Where a.s. means almost surely.

Convergence by Lp-norm is by taking the formula
\lim_{n\to\infty} E(|X_n - X|^p) = 0
For p \geq 1.
We must also ensure that E(|X_n|^p) < \infty.
The 'p' in Lp-norm is replaced by whatever exponent we chose
so L1-norm or L2-norm etc.
It should be clear that if we have convergence by p then p' > p also converges.
This is stronger than the convergence of probability shown below.
Lp-norm convergence implies convergence of probability but not the other way around.

We make an even weaker claim
\lim_{n\to\infty} P(|X_n - X| \geq \epsilon) = 0
X_n \to^P X
This is a different statement since the limit is on the outside.
This statement is about the convergence of probabilities not the converge
of random variables.

An example of the difference between convergence by probability and convergence
almost surely can be seen as such.
X_n \sim Ber(1/n)
Clearly X = 0 when we approach infinity.
P(|X_n - X| \geq \epsilon) = P(|X_n| \geq \epsilon)
= P(1/n \geq \epsilon) which checks out but for almost surely we find something else.

We denote the set of all {X_n = 1} which are all independent.
P(X_n = 1) = \sum 1/n [harmonic] = \infty
Further we note
P(\limsup_{n\to\infty} X_n = 1) = 1
The probability that we flip a head at infinite time is almost certain but
the probability that we flip a tail at infinite time is also certain.
P(\liminf_{n\to\infty} X_n = 0) = 1
Thus there is no definitive limit in the almost surely convergence.

We go further to weaken convergence so we have the convergence of distributions.
Simply if the cdf of our distribution approaches another for all continuous points
(discrete cdfs get to ignore their discontinuities) then they are convergent.

We demonstrate why this is weaker.
X \sim N(0,1). Denote X_n = -X.
Clearly X_n has the same distribution as X but do not converge in probability.
P(|X_n - X| \geq \epsilon) = P(|-2X| \geq \epsilon) [fails]

Levy's Continuity----------------------------------
Should be noted a characteristic function is any function that transforms our
cdf. The PGF, Fourier transform, etc.
Let F_1, F_2 ... be a sequence of cdfs with corresponding characteristic functions
\Phi_1, \Phi_2 ... Then,
If F_n \to F for some cdf F with characteristic function \Phi then
\Phi_n(t) \to \Phi(t) \forall t
If \Phi(t) = \lim_{n\t\infty} \Phi_n(t) \forall t
AND \Phi(t) is continuous at t = 0 then \Phi is a characteristic function
and if F is its corresponding cdf then F_n \to^{distribution} F
