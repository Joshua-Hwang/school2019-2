Intuition: Stochastic just means random.
Examples: Branching processes, Poisson processes, Markov chains
(discrete time and continuous time), Random walks

Definition: A stochastic process (X(t), t in T)
is a collection of random variables indexed by
some indexed set T.
Intuition: Think of an array (finite or infinite) that returns a "random" number
at each t.

T is a totally ordered set. T could be the natural number or even the reals.
T is usually considered time. This implies the stochastic process X(t) has
order, a story, a known chain.

You can have a thing called a random field where T has no order.

Much like with a random variable Y we can specify the probabilistic behaviour
of a stochastic process X via its Finite-dimensional Distribution (FDD).

An FDD is specified by,
P(X(t1) <= x1, X(t2) <= x2, ... , X(tn) <= xn)
Seems familiar to a CDF in STAT2003.
Note: X could return an n-dimensional vector.

If X(t) (subset of) E (where E is some set)
then E is a statespace for X. E is where X takes all its values from.

X(t;Omega) means the stochastic process given Omega. X(t;Omega) is called a
sample path. (name should make usage obvious)

Probability Generating Functions-------------------------------------------
Recall the probability generating function from STAT2003. (It only works for
discrete random variables)
G(z) = Expectation z^X
G(1) = 1

It can also be represented by,
G(z) = sum from k=0 to infinity of z^k P(X=k)

G(0) = P(X=0)

Note that PGF this is also a polynomial!
Probabilities are always positive so we have a purely positive polynomial.
u <= v then G(u) <= G(v) (This is due to the fact that the PGF might be an
infinite series of terms ergo only [0,1] actually converges)

We take the derivatives for the PGF,
G'(0) = P(X=1)
G(differentiate k times) (0) / k! = P(X=k)

Observe that if two random variables share a PGF then they are exactly equal.

The lecturer derives the PGF of Poisson and Geometric distributions.
(See 23 July at 00:00)

Branching Processes----------------------------------------------------
We consider a stochastic process (S) where each "animal" has a random number
of children (X). (See W01-Branching_Processes_II.pdf)
We start the process off with S_0 = 1. X_{i,n} is the number of offspring of the i^{th}
individual of the n^{th} generation.
To simplify we let X = X_{i,n} for all i and n, it's the same probability for
everyone (also all independent).

We immediately have a few starting facts,
S_n = 1 + sum^{S_{n-1}} X

Will the animals become extinct? Does there exist a generation m such that
S_m = 0?

Perhaps an easier question is, the expected population of generation n.

Tricks we will use:
Expectation X = Expectation(Expectation(X | Y))
E[g(x)h(y)|X] = g(x) E[h(y)|X]
If X and Y are independent E[g(x)h(y)|X] = g(x) E[h(Y)]

We base the generation n off the generation n-1.
E(S_n) = E[sum from 1 to S_{n-1} of all X_{i,n-1}] (using trick number 1)
E(S_n) = E[E[sum from 1 to S_{n-1} of all X_{i,n-1} | S_{n-1}]]
E(S_n) = E[sum from 1 to S_{n-1} of all E[X_{i,n-1} | S_{n-1}]] (since X_{i,n-1} and S_{n-1} are independent)
E(S_n) = E[sum from 1 to S_{n-1} of all E[X_{i,n-1}]] (we assumed earlier all X_{i,n} are iid)
E(S_n) = E(X) E[S_{n-1}]

(We shall call E(X) = mu)

Now also using are initial condition S_0 = 1 we get
S_1 = mu
S_2 = mu^2
S_3 = mu^3
...
S_n = mu^n

This is clearly a geometric progression.
mu < 1 then it approaches 0
mu = 1 then population remains 1
mu > 1 then population blows up

We can repeat this process to find the variance of each generation.

Var(S_n) = E(Var(S_n | S_{n-1})) + Var(E[S_n | S_{n-1}]) (This is a law called the law of total variance)

We attempt to solve the first term of this equation:
Var(S_n | S_{n-1}) = Var(sum from 1 to S_{n-1} X_{i,n-1} | S_{n-1}) (Using something similar to what was done previously)
Var(S_n | S_{n-1}) = S_{n-1} sigma^2 (We already found expectation of S_{n-1})
Now we find the expected variance
E[Var(S_n | S_{n-1})] = E[S_{n-1}] sigma^2
(sigma is variance of X)
E[Var(S_n | S_{n-1})] = mu^{n-1} sigma^2 (does not apply for n=0)

We solve the second term:
E[S_n | S_{n-1}] = mu S_{n-1}
Var(E[S_n | S_{n-1}]) = Var(mu S_{n-1})
Var(E[S_n | S_{n-1}]) = mu^2 Var(S_{n-1})

Thus our final solution is,
Var(S_n) = mu^{n-1} sigma^2 + mu^2 Var(S_{n-1})

This is now a recursive definition. We can actually expand it to give us,
Var(S_n) = sigma^2 mu^{n-1} (1 + mu + mu^2 + ... + mu^{n-1}) (not for 0)
An even better form is,
Var(S_n) = sigma^2 n when mu = 1
Var(S_n) = sigma^2 mu^{n-1} (1-mu^n)/(1-mu) for all others

Observe:
mu < 1 then Var(S_n) approaches 0 as n approaches infinity
mu >= 1 then Var(S_n) approaches infinity as n approaches infinity

Now we may ask the earlier question about probability of extinction
eta_n is probability of extinction at nth generation
eta_0 = 0 since S_0 = 1

We want to find P(S_n = 0) when n approaches infinity
P(S_n = 0) = P(S_n < 1)
= 1 - P(S_n >= 1)

We now use Markov inequality

Markov's inequality-----------------------------------------------------------
For ANY random variable X >= 0
and any a > 0
P(X >= a) <= Expectation(X)/a

I{event A} = 1 if event A occurs and 0 otherwise

Proof:
E[X] = E[X I{X<a} + X I{X >= a}]
E[X] = E[X I{X<a}] + E[X I{X >= a}]
E[X I{X<a}] is greater than zero from the initial statements
E[X] <= 0 + E[X I{X >= a}]
E[X] <= 0 + a E[I{X >= a}] [I don't know how E[X I{X >= a}] <= a E[I{X >= a}]]
E[X] <= 0 + a P(X >= a) [I don't know how E[X I{X >= a}] <= a E[I{X >= a}]]

Coming back to our previous question...
1 - P(S_n >= 1)
P(S_n >= 1) <= E(S_n)/1 = mu^n

Thus we can conclude that if mu < 1 then extinction is 100%

We currently know a few things about the probability of extinction.
eta = 1 when mu < 1
eta = 0 mu >= 1 AND variance = 0

We now attempt to find what happens when mu >= 1 and variance != 0.
eta_n = P(S_n = 0)
= E[P(S_n = 0 | S_1)] (We could've done this for any abitrary S_m)
= sum from 0 to infinity P(S_1 = k) P(S_n = 0 | S_1 = k) (definition of expectation)
= sum from 0 to infinity P(X = k) P(S_n = 0 | S_1 = k) (Since S_0 = 1 and recall X is the number of children of a single animal)
The probability we go extinct is determined by all animals in the previous generation failing to bear children thus,
= sum from 0 to infinity P(X = k) P(S_{n-1} = 0)^k
We observe this is the PGF of eta_{n-1}!
= G(eta_{n-1})

(G is the PGF of X)

Since we have a fixed initial state we can actually generate each eta_n.

(See 24 July at 23:00 if things are confusing or require justification)

Now we want to find eta_infinity or just eta which requires eta = G(eta).
This is a fixed point solution. In our solution we already know of G(1) = 1,
which is extinction, but are there more?

We highlight some more useful facts about the PGF. It is an upwards curving
function (at least on [0,1])

This means there are only two possible curves we can get. (See 24 July at 23:00)

Option 1 which has two fixed points. (1,1) (extinction) and somewhere else. It is caused by mu > 1.
Option 2 has the single fixed point at extinction. It is caused by mu <= 1.

By using the fixed point algorithm from COSC2500 we can find the fixed point
solutions. Also note that in Option 1 the interesting fixed point is stable
while the one at extinction is unstable. In comparison, Option 2 has extinction
as a stable fixed point.

[The lecturer then gives some examples of different X variables and solves them 24 July at 43:00]

Random walks-------------------------------------------------------------------
The most elementary stochastic process is an infinite set of iid variables.
(This is not interesting since it's just a bunch of independent states).

Perhaps a more interesting process would be a random walk.
We define the scenario as such. Where S_i is the i^{th} state of the
random walk while X_i is some random variable (the Xs should all be iid).
S_0 = X_0 = x_0 (starting)
S_n = S_{n-1} + X_n

If we were to evaluate and expand this recursive definition we would find
That S_n is just a sum of many iid random variables. By the Central
Limit Theorem this means that S_n approximates a normal distribution.

S and X do not need to have the same state-space (Bernoulli and Binomial).

An example could be coin flips which determine
walk forward or not at all.
Coin flips are Bernouilli [ZZZ spell check] and summing these coin flips gives
Binomial distribution.
It is important to remember that these random variables are dependent of
each other. e.g. S_{n+1} | S_n \sim S_n + Ber(p) and
S_{n+k} | S_n \sim S_n + Bin(k,p)

[see 29 July 30:00 for proof]

Magically we find the the Covariance of S_{n+k} and S_n is np(1-p) for
our above example.

Using this we can find our correlation to be \sqrt{\frac{n}{n+k}}

We explore a new question with random walks. Does the walker reach a desired
destination and if they do when?

Side note, if the walker is required to move every step then the walker is
required to flip odd and even number locations at each time step. (pretty neat)

Let \tau_d be the first time the random walker reaches the desired destination.
If we never reach home then \tau = \infty.
Another way to define \tau_d is the infinum of the set {n \in naturals : S_n = d}
(the set of times n that satisfy the random walker achieving a desired value)

We redefine our question concisely as P(\tau_d < \infty)

We define r_k = P(\tau_d < \infty | S_0 = k)
r is the probability that we reach our destination in finite time knowing
our starting position was k. Our question gets redefined again to become
what is r_0?

We observe a trivial case, r_d (the starting position is the desired position)
gives certain chance.

We also note that r_{d-i} = r_{d+i} there is equal chance getting i steps
away from the starting point on either side.

We perform a method called First-step analysis (aka conditioning).
This method involves considering everything that could happen in one unit
of time.
We expand r_k using conditional probability,
r_k = P(\tau_d < \infty | S_0 = k)
= P(\tau < \infty | S_0 = k, X_1 = +1) P(X_1 = +1 | S_0 = k)
+ P(\tau < \infty | S_0 = k, X_1 = -1) P(X_1 = -1 | S_0 = k)

We note that P(X_1 = +1 | S_0 = k) are independent so in reality it's just
P(X_1 = +1) which we'll assume is \frac{1}{2} for an unbiased random walk.
r_k = \frac{1}{2} (P(\tau < \infty | S_0 = k, X_1 = +1) + P(\tau < \infty | S_0 = k, X_1 = -1))

We further simplify since knowing S_0 = k and X_1 = +1 we in actuality know
S_1 = k+1.
r_k = \frac{1}{2} (P(\tau < \infty | S_1 = k+1) + P(\tau < \infty | S_1 = k-1))

Now we note that time is arbitrary and we let time 1 = 0 and time 0 = -1
r_k = \frac{1}{2} (P(\tau < \infty | S_0 = k+1) + P(\tau < \infty | S_0 = k-1))

From here we realise these are just the r_k definition again
r_k = \frac{1}{2} (r_{k+1} + r_{k-1})

We said earlier that r_{d-i} = r_{d+i} (we're letting d=0)
r_k = r_{k-1}

This works for all k and we also know that r_0 = 1 thus r_k = 1.

[To see another two examples of using single-step analysis see 31 July 00:00]
We run an example involving two \taus, bankrupt and money goal.
He goes indepth on this example and how to solve it.
Another example takes into consideration that time is important (previously
we recentered time but now we can't)

Markov processes--------------------------------------------------------------
We define (X_t, t\in T) as a stochastic process % meant to be a fancy T
Where X is the state at time t. A Markov processes must be closd on its
states. The probability you land somewhere else in the state space is
100%.

A Markov process has a neat property where,
if for any s,t \in T and any A \subset E (Where E is the state space).
Then
\prob(X_{t+s} \in A | T_t) = \prob(X_{t+s} \in A | X_t)
[probability of state X at time t+s knowing all
history up to and including t]

This means that our process only depends on the latest past state. History
beyond this does not matter.

This is known as a discrete time markov chain. This is because we are using
discrete timesteps and discrete states. [Later on we will discuss discrete
state continuous time markov chains]

Insane notation:
P_{ij}^m (n) means going from state i at step n to state j at step n+m

We explore an example using P_{ij}^2 (n), this demo teaches us about
exploiting the simplification of history.
We're currently at time n and we move to time n+1.
\sum_{k\in possible states} P(X_{n+2} = j | X_n = i, X_{n+1} = k) \times P(X_{n+1} = k | X_n = i)
\sum_{k\in possible states} P(X_{n+2} = j | X_{n+1} = k) \times P(X_{n+1} = k | X_n = i)
\sum_{k\in possible states} P_{kj}^1 (n+1) \times P_{ik}^1 (n)

Now we just calculate a single step. Easy.
Amazing observation, this is just matrix multiplication! i,j represents the
location in the matrix. Note: P_{ij}^1 (n) may have a different matrix to
P_{ij}^1 (m) since it is at a different time.

P^m (n) = P^1 (n) \times P^1 (n+1) \ldots P^1(n+m-1)
[keep in mind matrix multiplication is not commutative]

We create an initial probability represented as \pi^{(0)}_x = \prob(X_0 = x)
each element (probability at the state x initially).

[The notation he goes on to make is too hard to do,
he says we won't ever see it again. See 45:00 5 Aug]

Every row of the matrix determines "where could I go given I'm in row i"
[ZZZ redo this thinking]

If you have a countably infinite number of states a matrix won't do you
any good. Instead we call upon the help of linear operators.

Figuring out what happens to a markov chain at infinite time.
We make use of fancy shmancy eigenvalue and eigenvectors.
If the state space is finite and all eigenvalues are distinct then we
have a diagonalisable matrix.
P = V \Lambda V^{-1} where \Lambda is the diagonal matrix with the diagonals
being the eigenvalues.

This diagonalised state is useful since we can add powers which
only adds those powers to the diagonalised part.

Eigenvectors and eigenvalues-------------------------------------------------
[ZZZ spectral analysis is NOT the usual diagonalisation with V and V^{-1}]
We will use the spectral method to find the diagonalisation.
We attempt to solve a more general (and hopefully easier form)
P = V \Lambda W [note that it isn't V^{-1}]

We first denote what a left eigenvalue and a right eigenvalue is.
Right: v_i is a column vector
P v_i = \lambda v_i
Left: w_i is a row vector
w_i P = w_i \lambda_i

We desire our eigenvectors to multiply to give 1; w_i \dot v_i = 1.

We prove that if all \lambda are distinct then (w_j v_i = 0). The eigenvectors
will be orthogonal.
[ZZZ i'm hungry so i didn't write the proof. See 35:15 6 Aug]

We use this fact to prove that W = V^{-1}
[ZZZ proof not written down because i'm hungry See 35:15 6 Aug]

[ZZZ a bunch of properties that i'm guessing are important but I can't seem
to understand the relevance yet See 35:15 6 Aug]

If our markov process behaves nicely (as determined by the stuff up above)
then our process will have a limiting distribution at infinity. We denote
this distribution as a vector \pi.

From the bunch of stuff that happened above we can rewrite our P matrix as
P^n = \pi + \sum \sigma_k^n R_k where R_k = v_k w_k
From observation we can clearly see that |\sigma| < 1 for all our \sigma
then our limit exists and in fact is \pi.

Gershigerin[?] Circle Theorem (brief)
Create closed discs D_i in the complex plain with centres at p_{ii}
and radii r_i = 1 - p_{ii}. Then every eigenvalue \lambda_i of p lies in at
least one of the {D_i} [See 15:00 7 Aug for the intuition]

This theorem means that all the eigenvalues will be less than or equal in
distance of 1 from the origin. (also the limiting distribution is unique
as long as finite states)

We attempt to solve a fixed point system \pi P = \pi
Here \pi is known as a stationary distribution. The fixed point distribution
is a good method for finding the limiting distribution.

[example of solving such an equation at 31:00 7 Aug]

[00:00 12 Aug
The lecturer considers a toy example Markov chain involving two states.
He creates a matrix describing this system and performs spectral analysis on it.
He is able to perform the analysis quickly since he knows a few tricks like
the trace of the original matrix is equal to the sum of the eigenvalues
and since the matrix is normalised he also knows one of the eigenvalues
is 1.]

[18:00 12 Aug we now look at the even simpler state of the identity matrix.
This system means that each state is only allowed to remain in the same
state. This is an interesting system since the initial distribution is also
the limiting distribution (can't go anywhere). It also means that all
distributions are the limiting distribution; also stationary.]

[23:00 12 Aug an actual demonstration of spectral analysis]

The rest of lecture talks about all 5 systems that happen from a two state
system.
* We stay in the same state (identity)
* We travel directly to the next state ((0 1) (1 0))
* We have a chance of changing to a new state but this new state kicks us back
((1-s s) (1 0))
* We have a chance of changing to a new state but this new state traps us
((1-s s) (0 1))
* The normal system ((1-s s) (r 1-r))

We now explore more complicated Markov chain systems.
We create some new notation and revise some old ones.
Recall,
\prob_x() = \prob( | X_0 = x)
\expect_x() = \expect( | X_0 = x)

We now write,
\tau_y = inf{n > 0 : X_n = y}
\tau_y^{(k)} = inf{n > \tau_y^{(k-1)} : X_n = y}

\tau refers to the amount of time, n, taken for us to reach state
y. The \tau_y^{(k)} refers to the kth time we've reached state y.
This is clearly larger than the previous \tau as it has to be
later in time.

r_{xy}^{(k)} = \prob_x (\tau_y^{(k)} < \infty)
This means what's the chance this process goes on forever and
never touches y knowing that it started at state x.
The k involves the kth time we reach state y.
If r = 1 then it's guaranteed that we wll reach state y. If r = 0
then it's guaranteed to never reach state y. If it's anything
in between then there's a chance we never get to state y.

We now implement this new notation.
r_{yy}^{(2)} = \prob_y(\tau_y^{(2)} < \infty)
= \prob_y(\tau_y^{(2)} < \infty | \tau_y^{(1)} < \infty)
\times \prob_y(\tau_y^{(1)} < \infty)

But \prob_y(\tau_y^{(1)} < \infty) is just the chance it happens
at all so.
= \prob_y(\tau_y^{(2)} < \infty | \tau_y^{(1)} < \infty) \times r_{yy}

Since the Markov system only relies on the immediate history
AND note the \prob_y which means we're starting from y like the
previous time. It's also helpful to know that the previous k actually
exists wihch further confirms our conclusion that the two
probabilities are iid.

= \prob_y(\tau_y^{(1)} < \infty | \tau_y^{(1)} < \infty) \times r_{yy}
[We know \tau_y^{(2)} has the same distribution as \tau_y^{(1)}
that's why we're getting rid of the conditional but keeping the
distribution]
= \prob_y(\tau_y^{(1)} < \infty) \times r_{yy}
= r_{yy} \times r_{yy}
= r_{yy}^2 [This is referred to as the Strong Markov Property]

Intuitively we see the r_{yy}^{(k)} = r_{yy}^k.
We can now see some interesting implications.

If r_{yy} < 1 then the chance of hitting state y after a large
amount of time is now becoming impossible. We call this y a
*transient state*.

If r_{yy} = 1 then we are always guaranteed to hit our state y.
This is called a recurrent state.

We now move on to r_{xy}.

If r_{xy} > 0 then we say that x "leads to" y and write x \to y.
[It's possible that y does not lead back to x]

If x \to y AND y \to x then we say the x and y communicate.

The relation \to is transitive since give x \to y and y \to z then
x \to z. [Proof of this at 32:00 13 Aug]

If x is recurrent and x \to y then y is also recurrent.
[Proof at 47:00 13 Aug]

A set of states in a Markov system is called a communicating class
if \forall x,y \in A x \leftrightarrow y and if z not in A then there
is no x \leftrightarrow z.

If the communicating class is the entire state space then the state
space is called irreducible.

A set of states is called closed if r_{xy}=0 for x \in A and y \in A^c.
Equivalently, states in A can only reach other states in A.

Now if A={a} then the state "a" is called absorbing.

We can actually simplify our Markov system and categorise them via
transience or recursiveness.
The a few of the top rows will contain all transient families while
the remaining rows
will be a diagonal of recurrence families. [Better example at 19 Aug 22:00]

If C is a finite, closed, communicating class then all states in this family
are recurrrent.

We construct an identifier
I_x = \{ n \geq 1 : p^n_{xx} > 0}
This means that I is the set such that the probability of ending back
at state x after n steps is greater than 0.

The period of a recurrent state family is determined by the gcd
of all n in the set I_x. If gcd(I_x) = 1 then the set of states is aperiodic.
Interestingly, if x \leftrightarrow y then x and y have the same period.

Redefine recurrent states for infinite state systems-----------------
The idea of \prob_x (\tau_x < \infty) = 1 means that we will certainly
arrive back at x. Before it was simpler because there were only finite
states it meant that arriving back to state x only took finite time.

Instead if we have an infinite number of states we require new
definitions. \expect_x [\tau_x] < \infty then x is positive recurrent.
If \expect [\tau_x] = \infty then x is null recurrent.

Global Balance Equations--------------------------------------------------------
We return to the problem finding a stationary distribution, \pi P = \pi
We know attempt to do this for an infinite number of states. Such a system
is unable to be represented by a matrix.

Instead we try and solve a Global Balance Equation
\sum_{x\in E} \pi_x P_{xy} = \pi y
I believe this equation means for the state y it's equivalent to summing up
every other state multiplied by the probability state x goes to state y.

This equation is hard to solve in general. Instead we'll attempt to solve
a Local Balance Equation. They are of the form,
\pi_x P_{xy} = \pi_y P{yx}
and solving these will eventually give us a solution to the global balance as
well. Local balance seems to be a subset of all possible global balances since
all local balances are global but not all global are local balances.
This is just the balanced flux equation from STAT2004.

We solve an example with our local balancing equations with doubly stochastic
systems. [14:00 20 Aug]

[ZZZ 20 Aug recording cuts out]

[0:00 21 Aug random walker example with single boundary]
[14:00 21 Aug explains how a branching process IS a markov DTMC]

You can construct a y_m = x_{n-m} where 0 \leq m \leq n.
This y is called the reverse chain. It visits the states that X
did but in reverse.

Let's say we want to find a distribution such that the markov chain
x and its reverse y cannot be told apart. This is called being
reversible. This only occurs when the distribution is stationary.

Kolmogorov cycle condition-------------------------------------------
If X is irreducible then X is reversible iff P_{ij} > 0 implies
that P_{ij} > 0 and for any loop of states (which we can get to) it
holds that
\prod_{i=1}^n p_{x_i x_{i-1}} = \prod_{i=1}^n p_{x_{i-1} x_i}

If Kolmogorov cycle condition holds then X has a stationary
distribution.

If X is irreducible and positive recurrent then there is a
*unique* stationary distribution. Moreover if X is also aperiodic
then all distributions lead to the limiting distribution (which is
also the stationary distribution.

Poisson Processes-------------------------------------------------------------
We first introduce this topic with two simpler, examples.
Consider a random walker that flips a coin and either moves forwards
or stays still. At each time stage n the result of the coin flip is B_n.
The overal distance the random walker has gone after n flips is S_n.
The process S_n is called a *Bernoulli process*.

Now we start to consider continuous time but with discrete counts still.
We now have a function N(t) which is the continuous analogue of S_n with
continuous time t. At each jump from N(t) = 0 to N(t) = 1 we become piecewise.
N(0) = 0 and N(t) \in naturals.
This is called a *counting process*. We define each T_i as the time when
the ith jump happens.
An obvious but vital insight is
{N(t) \geq n} = {T_n \leq t}
Which says, if the counter is larger than n at time t then the time this
happened is less then time t.

Now a Poisson process is a counting process if for any interval I \in reals then
N(I) \sim Poi(\lambda |I|)
Any interval in the counting set has a distribution similar to a poisson
distribution. Also all disjoint intervals must be independent.

[43:14 26 Aug shows some kind of proof/derivation to produce a
Poisson counting process]

[ZZZ Finished 26 Aug]
For a Poisson process we have various intervals before each jump in count.
i.e. from t=0 to t=A_1 called interval A_1, from t=A_1 to t=A_1+A_2 called
interval A_2. We consider each interval a random process and in fact
are all iid Exponentials with lambda as the parameter.

We claim if we let (N_t, t \geq 0) be a Poisson process with rate \lambda > 0.
Then for any fixed time s > 0, any time from 0 to s (N_r, 0 \leq r \leq s)
and any time in the interval (N(t+s) - N(s), t \geq 0)
[note that N(t+s) - N(s) is equivalent to N(s,t+s]]. Are independent poisson
processes AND independent of each other. This is very like the memoryless
property we see in exponential distributions and the like.

A Poisson process can be recognised by the following definition.
\prob (N(t+h) = m | N(t) = n) has four possibilities.
If m < n then there's no chance. It's a counting process, it can't go down.
If m = n then 1 - \lambda h + o(h).
if m = n+1 then \lambda h + o(h). Exactly one jump has occurred in the interval.
If m > n+1 then o(h).

This o(h) is the opposite of the O(h) we see in algorithm analysis.
We use it to ensure o(h)/h \to 0 when h \to 0.

If we have a process that follows these probabilities then we define it as
a Poisson process.

[Page 4 Lecture 16 The lecturer then derives a Poisson process from
t/h Bernoulli processes (one for each point in time) as h \to 0 we
end up getting the Poisson process]

We now attempt to prove that the times of our counting events in our counting
process (knowing that the count N(t) = n) has the same distribution as an
ordered list of uniform random variables. [15:00 28 Aug for the proof]

The PDF of n random variables is 1/t^n in the region and 0 otherwise.
The PDF of n sorted random variables is n!/t^n in the region (which is now
smaller btw) and 0 otherwise.
Think of this like the uniform random variables have been realised. There
are n! ways of ordering them and we funnel all n! possibilities into the same
sorted list thus n! \times 1/t^n.

We can simulate a Poisson process in two ways. One which is fast and smart
and another which is naive.

Naive making use of the fact that the event times are random exponentials:
T = {}
T[0] = 0
n = 0
while T[n] < t
    T[n+1] = T[n] + Exp(\lambda)
    n = n+1

Smart making use of what we've just learnt:
N = Poi(\lambda*t)
Generate N Unif(0,t) into list U[]
Sort U[]
Return T[k] = U[k]

Non-homogeneous Poisson process-----------------------------------------------
Previously our Poisson process depended on a constant lambda for all time.
Instead we consider lambda a function in time. [It's possible to use the
same derivation as show in Lecture 16]

In essence we get N(a,b] \sim Poi(\int_a^b \lambda(s) \diff s)
We get the integral in there which is in essence just a summation where
h \to 0.

Should be noted some of our previous properties we've found for homogeneous
Poisson processes are no longer true. The interval times are no longer
exponentially distributed (so no longer memoryless) and they are no longer
independent.
