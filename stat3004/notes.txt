Intuition: Stochastic just means random.
Examples: Branching processes, Poisson processes, Markov chains (discrete time and continuous time), Random walks
Definition: A stochastic process (X(t), t in T)
is a collection of random variables indexed by
some indexed set T.
Intuition: Think of an array (finite or infinite) that returns a "random" number
at each t.

T is a totally ordered set. T could be the natural number or even the reals.
T is usually considered time. This implies the stochastic process X(t) has
order, a story, a known chain.

You can have a thing called a random field where T has no order.

Much like with a random variable Y we can specify the probabilistic behaviour
of a stochastic process X via its Finite-dimensional Distribution (FDD).

An FDD is specified by,
P(X(t1) <= x1, X(t2) <= x2, ... , X(tn) <= xn)
Seems familiar to a CDF in STAT2003.
Note: X could return an n-dimensional vector.

If X(t) (subset of) E (where E is some set)
then E is a statespace for X. E is where X takes all its values from.

X(t;Omega) means the stochastic process given Omega. X(t;Omega) is called a
sample path. (name should make usage obvious)

Probability Generating Functions-------------------------------------------
Recall the probability generating function from STAT2003. (It only works for
discrete random variables)
G(z) = Expectation z^X
G(1) = 1

It can also be represented by,
G(z) = sum from k=0 to infinity of z^k P(X=k)

G(0) = P(X=0)

Note that PGF this is also a polynomial!
Probabilities are always positive so we have a purely positive polynomial.
u <= v then G(u) <= G(v) (This is due to the fact that the PGF might be an
infinite series of terms ergo only [0,1] actually converges)

We take the derivatives for the PGF,
G'(0) = P(X=1)
G(differentiate k times) (0) / k! = P(X=k)

Observe that if two random variables share a PGF then they are exactly equal.

The lecturer derives the PGF of Poisson and Geometric distributions.
(See 23 July at 00:00)

Branching Processes----------------------------------------------------
We consider a stochastic process (S) where each "animal" has a random number
of children (X). (See W01-Branching_Processes_II.pdf)
We start the process off with S_0 = 1. X_{i,n} is the number of offspring of the i^{th}
individual of the n^{th} generation.
To simplify we let X = X_{i,n} for all i and n, it's the same probability for
everyone (also all independent).

We immediately have a few starting facts,
S_n = 1 + sum^{S_{n-1}} X

Will the animals become extinct? Does there exist a generation m such that
S_m = 0?

Perhaps an easier question is, the expected population of generation n.

Tricks we will use:
Expectation X = Expectation(Expectation(X | Y))
E[g(x)h(y)|X] = g(x) E[h(y)|X]
If X and Y are independent E[g(x)h(y)|X] = g(x) E[h(Y)]

We base the generation n off the generation n-1.
E(S_n) = E[sum from 1 to S_{n-1} of all X_{i,n-1}] (using trick number 1)
E(S_n) = E[E[sum from 1 to S_{n-1} of all X_{i,n-1} | S_{n-1}]]
E(S_n) = E[sum from 1 to S_{n-1} of all E[X_{i,n-1} | S_{n-1}]] (since X_{i,n-1} and S_{n-1} are independent)
E(S_n) = E[sum from 1 to S_{n-1} of all E[X_{i,n-1}]] (we assumed earlier all X_{i,n} are iid)
E(S_n) = E(X) E[S_{n-1}]

(We shall call E(X) = mu)

Now also using are initial condition S_0 = 1 we get
S_1 = mu
S_2 = mu^2
S_3 = mu^3
...
S_n = mu^n

This is clearly a geometric progression.
mu < 1 then it approaches 0
mu = 1 then population remains 1
mu > 1 then population blows up

We can repeat this process to find the variance of each generation.

Var(S_n) = E(Var(S_n | S_{n-1})) + Var(E[S_n | S_{n-1}]) (This is a law called the law of total variance)

We attempt to solve the first term of this equation:
Var(S_n | S_{n-1}) = Var(sum from 1 to S_{n-1} X_{i,n-1} | S_{n-1}) (Using something similar to what was done previously)
Var(S_n | S_{n-1}) = S_{n-1} sigma^2 (We already found expectation of S_{n-1})
Now we find the expected variance
E[Var(S_n | S_{n-1})] = E[S_{n-1}] sigma^2
(sigma is variance of X)
E[Var(S_n | S_{n-1})] = mu^{n-1} sigma^2 (does not apply for n=0)

We solve the second term:
E[S_n | S_{n-1}] = mu S_{n-1}
Var(E[S_n | S_{n-1}]) = Var(mu S_{n-1})
Var(E[S_n | S_{n-1}]) = mu^2 Var(S_{n-1})

Thus our final solution is,
Var(S_n) = mu^{n-1} sigma^2 + mu^2 Var(S_{n-1})

This is now a recursive definition. We can actually expand it to give us,
Var(S_n) = sigma^2 mu^{n-1} (1 + mu + mu^2 + ... + mu^{n-1}) (not for 0)
An even better form is,
Var(S_n) = sigma^2 n when mu = 1
Var(S_n) = sigma^2 mu^{n-1} (1-mu^n)/(1-mu) for all others

Observe:
mu < 1 then Var(S_n) approaches 0 as n approaches infinity
mu >= 1 then Var(S_n) approaches infinity as n approaches infinity

Now we may ask the earlier question about probability of extinction
eta_n is probability of extinction at nth generation
eta_0 = 0 since S_0 = 1

We want to find P(S_n = 0) when n approaches infinity
P(S_n = 0) = P(S_n < 1)
= 1 - P(S_n >= 1)

We now use Markov inequality

Markov's inequality-----------------------------------------------------------
For ANY random variable X >= 0
and any a > 0
P(X >= a) <= Expectation(X)/a

I{event A} = 1 if event A occurs and 0 otherwise

Proof:
E[X] = E[X I{X<a} + X I{X >= a}]
E[X] = E[X I{X<a}] + E[X I{X >= a}]
E[X I{X<a}] is greater than zero from the initial statements
E[X] <= 0 + E[X I{X >= a}]
E[X] <= 0 + a E[I{X >= a}] [I don't know how E[X I{X >= a}] <= a E[I{X >= a}]]
E[X] <= 0 + a P(X >= a) [I don't know how E[X I{X >= a}] <= a E[I{X >= a}]]

Coming back to our previous question...
1 - P(S_n >= 1)
P(S_n >= 1) <= E(S_n)/1 = mu^n

Thus we can conclude that if mu < 1 then extinction is 100%

We currently know a few things about the probability of extinction.
eta = 1 when mu < 1
eta = 0 mu >= 1 AND variance = 0

We now attempt to find what happens when mu >= 1 and variance != 0.
eta_n = P(S_n = 0)
= E[P(S_n = 0 | S_1)] (We could've done this for any abitrary S_m)
= sum from 0 to infinity P(S_1 = k) P(S_n = 0 | S_1 = k) (definition of expectation)
= sum from 0 to infinity P(X = k) P(S_n = 0 | S_1 = k) (Since S_0 = 1 and recall X is the number of children of a single animal)
The probability we go extinct is determined by all animals in the previous generation failing to bear children thus,
= sum from 0 to infinity P(X = k) P(S_{n-1} = 0)^k
We observe this is the PGF of eta_{n-1}!
= G(eta_{n-1})

(G is the PGF of X)

Since we have a fixed initial state we can actually generate each eta_n.

(See 24 July at 23:00 if things are confusing or require justification)

Now we want to find eta_infinity or just eta which requires eta = G(eta).
This is a fixed point solution. In our solution we already know of G(1) = 1,
which is extinction, but are there more?

We highlight some more useful facts about the PGF. It is an upwards curving
function (at least on [0,1])

This means there are only two possible curves we can get. (See 24 July at 23:00)

Option 1 which has two fixed points. (1,1) (extinction) and somewhere else. It is caused by mu > 1.
Option 2 has the single fixed point at extinction. It is caused by mu <= 1.

By using the fixed point algorithm from COSC2500 we can find the fixed point
solutions. Also note that in Option 1 the interesting fixed point is stable
while the one at extinction is unstable. In comparison, Option 2 has extinction
as a stable fixed point.

(The lecturer then gives some examples of different X variables and solves them 24 July at 43:00)

Random walks-------------------------------------------------------------------
The most elementary stochastic process is an infinite set of iid variables.
(This is not interesting since it's just a bunch of independent states).

Perhaps a more interesting process would be a random walk.
We define the scenario as such. Where S_i is the i^{th} state of the
random walk while X_i is some random variable (the Xs should all be iid).
S_0 = X_0 = x_0 (starting)
S_n = S_{n-1} + X_n

If we were to evaluate and expand this recursive definition we would find
That S_n is just a sum of many iid random variables. By the Central
Limit Theorem this means that S_n approximates a normal distribution.

S and X do not need to have the same state-space (Bernoulli and Binomial).

An example could be coin flips which determine
walk forward or not at all.
Coin flips are Bernouilli [spell check] and summing these coin flips gives
Binomial distribution.
It is important to remember that these random variables are dependent of
each other. e.g. S_{n+1} | S_n \sim S_n + Ber(p) and
S_{n+k} | S_n \sim S_n + Bin(k,p)

[see 29 July 30:00 for proof]

Magically we find the the Covariance of S_{n+k} and S_n is np(1-p) for
our above example.

Using this we can find our correlation to be \sqrt{\frac{n}{n+k}}

We explore a new question with random walks. Does the walker reach a desired
destination and if they do when?

Side note, if the walker is required to move every step then the walker is
required to flip odd and even number locations at each time step. (pretty neat)

Let \tau_d be the first time the random walker reaches the desired destination.
If we never reach home then \tau = \infty.
Another way to define \tau_d is the infinum of the set {n \in naturals : S_n = d}
(the set of times n that satisfy the random walker achieving a desired value)

We redefine our question concisely as P(\tau_d < \infty)

We define r_k = P(\tau_d < \infty | S_0 = k)
r is the probability that we reach our destination in finite time knowing
our starting position was k. Our question gets redefined again to become
what is r_0?

We observe a trivial case, r_d (the starting position is the desired position)
gives certain chance.

We also note that r_{d-i} = r_{d+i} there is equal chance getting i steps
away from the starting point on either side.

We perform a method called First-step analysis (aka conditioning).
This method involves considering everything that could happen in one unit
of time.
We expand r_k using conditional probability,
r_k = P(\tau_d < \infty | S_0 = k)
= P(\tau < \infty | S_0 = k, X_1 = +1) P(X_1 = +1 | S_0 = k)
+ P(\tau < \infty | S_0 = k, X_1 = -1) P(X_1 = -1 | S_0 = k)

We note that P(X_1 = +1 | S_0 = k) are independent so in reality it's just
P(X_1 = +1) which we'll assume is \frac{1}{2} for an unbiased random walk.
r_k = \frac{1}{2} (P(\tau < \infty | S_0 = k, X_1 = +1) + P(\tau < \infty | S_0 = k, X_1 = -1))

We further simplify since knowing S_0 = k and X_1 = +1 we in actuality know
S_1 = k+1.
r_k = \frac{1}{2} (P(\tau < \infty | S_1 = k+1) + P(\tau < \infty | S_1 = k-1))

Now we note that time is arbitrary and we let time 1 = 0 and time 0 = -1
r_k = \frac{1}{2} (P(\tau < \infty | S_0 = k+1) + P(\tau < \infty | S_0 = k-1))

From here we realise these are just the r_k definition again
r_k = \frac{1}{2} (r_{k+1} + r_{k-1})

We said earlier that r_{d-i} = r_{d+i} (we're letting d=0)
r_k = r_{k-1}

This works for all k and we also know that r_0 = 1 thus r_k = 1.

[To see another two examples of using single-step analysis see 31 July 00:00]
We run an example involving two \taus, bankrupt and money goal.
He goes indepth on this example and how to solve it.
Another example takes into consideration that time is important (previously
we recentered time but now we can't)

[ZZZ stopped at 31 July]

Markov processes--------------------------------------------------------------
We define (X_t, t\in T) as a stochastic process % meant to be a fancy T
Where X is the state at time t. A Markov processes must be closd on its
states. The probability you land somewhere else in the state space is
100%.

A Markov process has a neat property where,
if for any s,t \in T and any A \subset E (Where E is the state space).
Then
\prob(X_{t+s} \in A | T_t) = \prob(X_{t+s} \in A | X_t)
[probability of state X at time t+s knowing all
history up to and including t]

This means that our process only depends on the latest past state. History
beyond this does not matter.

This is known as a discrete time markov chain. This is because we are using
discrete timesteps and discrete states. [Later on we will discuss discrete
state continuous time markov chains]

Insane notation:
P_{ij}^m (n) means going from state i at step n to state j at step n+m

We explore an example using P_{ij}^2 (n), this demo teaches us about
exploiting the simplification of history.
We're currently at time n and we move to time n+1.
\sum_{k\in possible states} P(X_{n+2} = j | X_n = i, X_{n+1} = k) \times P(X_{n+1} = k | X_n = i)
\sum_{k\in possible states} P(X_{n+2} = j | X_{n+1} = k) \times P(X_{n+1} = k | X_n = i)
\sum_{k\in possible states} P_{kj}^1 (n+1) \times P_{ik}^1 (n)

Now we just calculate a single step. Easy.
Amazing observation, this is just matrix multiplication! i,j represents the
location in the matrix. Note: P_{ij}^1 (n) may have a different matrix to
P_{ij}^1 (m) since it is at a different time.

P^m (n) = P^1 (n) \times P^1 (n+1) \ldots P^1(n+m-1)
[keep in mind matrix multiplication is not commutative]

We create an initial probability represented as \pi^{(0)}_x = \prob(X_0 = x)
each element (probability at the state x initially).

[The notation he goes on to make is too hard to do,
he says we won't ever see it again. See 45:00 5 Aug]

Every row of the matrix determines "where could I go given I'm in row i"
[ZZZ redo this thinking]

If you have a countably infinite number of states a matrix won't do you
any good. Instead we call upon the help of linear operators.

Figuring out what happens to a markov chain at infinite time.
We make use of fancy shmancy eigenvalue and eigenvectors.
If the state space is finite and all eigenvalues are distinct then we
have a diagonalisable matrix.
P = V \Lambda V^{-1} where \Lambda is the diagonal matrix with the diagonals
being the eigenvalues.

This diagonalised state is useful since we can add powers which
only adds those powers to the diagonalised part.

Eigenvectors and eigenvalues-------------------------------------------------
[ZZZ spectral analysis is NOT the usual diagonalisation with V and V^{-1}]
We will use the spectral method to find the diagonalisation.
We attempt to solve a more general (and hopefully easier form)
P = V \Lambda W [note that it isn't V^{-1}]

We first denote what a left eigenvalue and a right eigenvalue is.
Right: v_i is a column vector
P v_i = \lambda v_i
Left: w_i is a row vector
w_i P = w_i \lambda_i

We desire our eigenvectors to multiply to give 1; w_i \dot v_i = 1.

We prove that if all \lambda are distinct then (w_j v_i = 0). The eigenvectors
will be orthogonal.
[ZZZ i'm hungry so i didn't write the proof. See 35:15 6 Aug]

We use this fact to prove that W = V^{-1}
[ZZZ proof not written down because i'm hungry See 35:15 6 Aug]

[ZZZ a bunch of properties that i'm guessing are important but I can't seem
to understand the relevance yet See 35:15 6 Aug]

If our markov process behaves nicely (as determined by the stuff up above)
then our process will have a limiting distribution at infinity. We denote
this distribution as a vector \pi.

From the bunch of stuff that happened above we can rewrite our P matrix as
P^n = \pi + \sum \sigma_k^n R_k where R_k = v_k w_k
From observation we can clearly see that |\sigma| < 1 for all our \sigma
then our limit exists and in fact is \pi.

Gershigerin[?] Circle Theorem (brief)
Create closed discs D_i in the complex plain with centres at p_{ii}
and radii r_i = 1 - p_{ii}. Then every eigenvalue \lambda_i of p lies in at
least one of the {D_i} [See 15:00 7 Aug for the intuition]

This theorem means that all the eigenvalues will be less than or equal in
distance of 1 from the origin. (also the limiting distribution is unique
as long as finite states)

We attempt to solve a fixed point system \pi P = \pi
Here \pi is known as a stationary distribution. The fixed point distribution
is a good method for finding the limiting distribution.

[example of solving such an equation at 31:00 7 Aug]

[stopped at 7 Aug]
