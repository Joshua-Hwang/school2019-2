# Journal entry for 6 December 2019
## What do I intend to do this week.
- [x] Rewrite the program in C for a small boost in speed.
  - [x] ~Add threadding~
  - [x] Also document it well.

- [x] Measure perfomance
  - [x] Measure comparison in python and C (not useful)
  - [ ] ~Measure speed if all velocities the same~
  - [x] ~If you can compare # threads vs performance~

- [x] I would like to visualise the graph made from the collisions of grains.

- [x] Also would like to be able to compare the models. A technique called coupling
  you force two independent models to become dependent so you may easily compare them.
  Run the simulation on the points normally then one where you ignore birth time etc.
  * This should be very easy considering how I've made the new program

- [x] Figure out what I want to analyse.
  - [x] Refer to previous journal entry for information

- [x] Other questions can be asked like
  - Average radius
  - Average density (define the bounds though)
  - Largest connected component

- [ ] ~Look into smart ways of transforming the problem into another form.
Maybe what Thomas had in mind by changing the problem into a multigraph and
finding some kind of optimisation on it.~

- [x] E-mail Christian Hirsch about the algorithm he used

- [ ] ~It would be vey useful to hold the best growing pair minimum~
  - ~Only issue with that is that if the new spair was the minpair.
  Maybe it will allow the cull at spair > minpair.
  To search for minimum iterate over grains instead of all pairs
  since each grain is holding its own minimum.~

- [ ] Figure out some way to identify each circle.
  - Create a color for each circle then provide a legend

- [ ] ~Since the bottleneck is from integration look for different ways to
parallelize integration.~
  - ~Cython?~
  - ~Split the region into sections and run threads on each?~

- [x] Got a new paper so take a look at it

## What have I done so far.
* E-mailed Christian. Now we wait for his response.

* After completing `attempt2` I found the major issue was with the
method of thinning. Since only a fraction of points get accepted. If I want to
continue improving the perfomance I'll have to consider a different way of
generating points.

* I'm deciding to skip on the threading and other performance measures since
since the performance of the new implementation is great it's the
thinning that's the bottleneck.

* Since the bottleneck was point generation I parallelised the process so I'm
making use of many more cores. Should be noted that at certain thresholds this
doesn't help since these threads need to be spun up and each needs their own
random number generator which might take a while to build.

* Noticed there's a small bug in the C program that relates to grains that have
very high growth rates will have to investigate that part of the program.

* ~I'm deciding to look into percolation. This seems to be the simplest and most
straightforward problem I could look into to.
  * What happens if we increase the radii by some delta?
  * What happens if we use a different shape?
  * What happens if we discretise the space?
  * After some investigation there seems to be two metrics we can measure
  percolation by two tests.
    * Creating a large enough component.
    * Connecting the top and bottom together.
      * To achieve this one our analysis program needs to consider the highest
      grain and the lowest grain (or maybe something else idk)

* Yeah turns out you don't even need integration

* Added lines between centres to determine collisions.

* Very easily created a bunch of smaller programs which force the velocity
to a certain rate or set the birth rate to what I want

* Started working on a program that can determine some basic properties of a system.
Things like connected component are still difficult but we can now investigate
Apollolian packing if we want.

* Turns out it was very easy to implement Lp norms. Haven't looked into anything
regarding them though.

## What do I intend to do the coming week.
* Further reading of the books. Questions that I would like answered like:
  * It seems continuum percolation defines percolation as an unbounded connected
  component. How they able to tell if an unbounded connected component exists?
  * Are there other ways to define percolation. For discrete cases there seems
  to be the case when there's a pathway from top to bottom. Is there something
  as nice and finite as that for continuous cases?

* Investigate if the size is cause for the issues. This is because if we include
a very tiny tail we'll be forced to create a large number of points many of which
will be dropped.

* Work on attempt3 where we improve our \*.lp format and add entries for connections
  * We still define the initial collision in each row since the first collision
  is special
  * We actually don't need an attempt3 instead we extend our attempt2 to have
  `solvelp` produce additional output. After this we use python's set data
  structure, this should "technically" be linear.
  * Our `solvelp` should not delete the pairs but rather store them in an array
  to be ordered later.

* Maybe we could get a speed boost by calculating each pair using
multiprogramming. Store a global that is mutexed which finds the minimum.

## Long term
* Look into extending it to 3D (which shouldn't be that hard)

* Alternative shapes and distance metrics (manhattan distance)

##  Lingering questions.
* I'm currently unsure of where to take the project next.
  * Pursuing a proof is very challneging and something Thomas isn't exactly looking for
  * Finding ways to improve the algorithm because it's currently running at n^4 time
    * Thomas's suggestion seems like a nice way of doing things
    * Maybe analyse my new method to see it's big O performance
* An issue with the current model is that it's bounded by the dimensions we define.
  In reality this model should have an infinite number of circles. Is there any way
  we can approximate this behaviour? Some kind of limiting behaviour.
