# Journal entry for 20 December 2019
## What do I intend to-do this week.
- [x] Parsing seems to be a reoccurring theme in all my programs
perhaps it's time to start creating a module.

- [x] Email Christian about the method I've generated and if it's interesting.
"My professor/coordinator thinks there might be a paper in it"

- [ ] Take a look at dithered images because "ink bleed"

- [x] Explore boundary conditions
  - [x] Hit the boundary
  - [x] Loop back (periodic boundary conditions)
    * To do this maybe try finding if the looped coordinates 
    give a better distance. Since distance is fixed we should be fine to
    store that value.
      * Might be a good idea to define a macro which determined which distance
      algorithm to use.
  - [x] Cylinder geometry (one of the boundaries is solid)
  * With percolation you must have hard boundaries

- [x] Increasing the boundary does nothing. Just increase the intensity.
Thus ensure the boundaries are 1x1

- [ ] Just create a new program that `extends.py` the solved system

- [x] Generate a worse case system
  - You'll have to think long and hard about what would be our worst case
  - [x] Create a program that can generate one of these worst cases easily given
  the number of points desired.

- [x] Plot performance on a log graph to get gradients
  - [ ] ~Create a program and do the samples from more to less and less to more to check
  for memory optimisations~
  - [ ] ~Parallelise the approximation script~
  - [x] Look for what might be wrong in the Python script
    * Is it because I'm using a `set` data structure?
  - [x] Check that our C program is actually n^3 might actually be better.

- [x] Work on extending `attempt2` where we improve our \*.lp format and
add entries for connections
  * We still define the initial collision in each row since the first collision
  is special
  * Our `solvelp` should not delete the pairs but rather store them in an array
  to be ordered later.
  * We can redefine the initial collisions in the later section
  if we want but that's time and memory
  optimisations that we've lost.
    * Once implemented the delta increase run some experiments to numerically
    determine our curve or possible deltas to achieve percolation.

- [ ] Investigate better visualisers.
  * Also something that is interactive
  * You'll have to use the vector graphics since pdfs like vectors

- [x] Consider ways to turn the approximate algorithm into a matrix.

## What have I done so far.
* I'm not convinced the matrix idea has any legs. It seems as bad as what we've
been doing already. The approximation might benefit slightly from threading.

* So I've reorganised the python code. Only part that still sucks is the
discrete function since we're using a set.

* Reworking the `.lp` format was painful.

* Turns out the algorithm is definitely n^3 but also the python based approximation
was graphed around n^2.5 (probably because of the relation from time). The passing
happens around 1000 grains.

## What do I intend to do the coming week.

* Need to finish up the work on percolation.

* If we assume no extra births we can actually cull all collisions
that occur further away than closest grain. Not sure how important
this culling will be to performance (could it be magnitudes better?)
  * Would need to look up what the average "local group" is for a grain.

* Read Existence, Uniqueness, and Algorithmic Computation of General Lilypond
Systems (Heveling & Last, 2005)
  * They claim to have an algorithm (for the basic Lilypond model) that
  takes n^2

* Now that we've extended the `.lp` format we can now run this on moss
to determine percolation.
  * Consider the "standard lilypond model" with homogeneous intensity 1.
  We'll try different sized areas but for the most part we'll not consider
  dynamically born grains nor different velocities.

## Long term
* Look into extending it to 3D (which shouldn't be that hard)
