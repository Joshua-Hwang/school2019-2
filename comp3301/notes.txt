Define an OS-------------------------------------------------------------------
An OS is a program that acts as a middle man between the computer user and the
hardware.

The goals of an operating system are:
    Executing user programs
    Make the computer system convenient to use
    Use hardware in an efficient manner

The four components of on operating system. (Image at section 1.4)
1. At the top are the users
2. System application and programs (compilers, assemblers, text editors, database systems, etc.)
3. The operating system
4. The computer hardware

Where we see operating systems-----------------------------------------------
Shared computers like mainframes and minicomputer must keep all users happy.
Dedicated systemts like workstations have dedicated resources for each user BUT usually
    use shared resources from a server.
Handheld systems like phones have very few resources but are optimised for usability and battery life.
Embedded systems have little to no interface.

definitions of operating system-----------------------------------------------
The OS is a resource allocator,
    Manages ALL resources
    Divides the resources fairly (whatever that means) between conflicting requests.

The OS is a control program,
    Controls execution of programs to ensure programs are executed as expected with no errors.

As a matter of fact there isn't a single proper definition for an operating system.
"Everything a vendor ships with is a good approximation" (that varies wildly)

(unrelated) "The kernel is the one progam always running"

interrupts-------------------------------------------------------------------
An *interrupt* transfers control to the *interrupt service routine* through a
*interrupt vector* which contains the addresses of all the service routines.
(think of exceptions and events but more general so it encapsulates hardware)

The interrupt vector is a piece of code that gets executed when the interrupt
occurs.

A *trap* or *exception* is a "software generated" interrupt caused by errors
in the program or events.

An operating system is "interrupt driven"
When an interrupt occurs the operating system preserves the state of the CPU
by storing it in registers and the program counter

The operating system also determines what type of interrupt has occurred:
A polling [look it up]
A vectored interrupt system (see above)

storage device-------------------------------------------------------------
(Image at 1.10)
A pyramid with the top being:
registers,
cache,
main memory,
solid-state disk,
magnetic disk,
optical disk,
magnetic tapes

Computer system architecture---------------------------------------------
Most systems use a single general-purpose processor (a CPU)
or PDAs through a mainframe (I don't know what PDAs are)
They also use special-purpose processors (e.g. GPUs)

A new (not exactly new :P) wave of systems known as *multiprocessor systems* (aka parallel systems,
tightly-coupled systems). These systems CAN have increased throughput,
economy of scale, increased reliability (graceful degradation or fault tolerance)
[ZZZ expand these to defintions].

There are two types of multiprocess systems; asymmetric and symmetric.

In asymmetric we have cores of different types while in symmetric we have
many copies of the same, single type of core.

Note: multicore cpu is different to multiprocessor

Multiprogramming----------------------------------------------------
Multiprogramming organizes jobs (code and data) so the CPU always has one job to
execute. A subset of total jobs in the system is kept in memory.
One job is selected and run via *job scheduling*.
When a job has to wait for something (e.g. I/O) the OS switches jobs.

Timesharing (multitasking)-----------------------------------------------
Extends the idea of multiprogramming. The OS swaps jobs so quickly that
a user can interact seamlessly. Response time should be < 1 second.

A process is a program executing in memory.
CPU scheduling occurs if several jobs are ready to run at the same time.
Swapping occurs when a process can no longer fit in memory so it's sent to storage.
Virtual memory (explained later)

Interrupts are driven by hardware

Dual-mode splits operations into
User mode (=1),
kernel mode (=0).
This is done to protect the operating system and other system components.
Dual-mode is implemented by a *mode bit* provided by hardware.
There are certain *privileged* operations only executable in kernel mode.
A system call changes to kernel mode then resets to user mode when done.
Switching to kernel mode is called trap and returning is called return.

Many modern CPUs support multi-mode operations.
A virtual machine manager (VMM) mode for guest VMs.

An example of transitioning:
Construct a timer to prevent
infinite loops/process hogging resources.
This could be implemented thusly:
Create the following process before scheduling the process
Set interrupt after specific period
Decrement counter
When counter hits zero OS generates an interrupt.

A program is a passive entity while a process is an active entity.
A process needs resources (i.e. CPU, memory, I/O, files, initialisation data)
When a process terminates we need to take recycle those resouces.

Threading and multiple processes--------------------------------------------
A process has a *program counter* for each thread it has.
Each program counter specifies the location of the next instruction to execute.

Typically a system has many processes, maybe some users, with one or more
operating systems running concurrently on one or more CPUs.

Memory-----------------------------------------------------------------------
All data is in memory before and after the process.
All instructions in memory ready for execution.

Memory management determines what's in memory.
Memory management keeps track of which segments of memory are in use and by
whom, deciding which instructions and data to move into and out of memory,
allocating and deallocating memory space as needed.

Storage---------------------------------------------------------------
OS provides abstraction for storage (magnetic tape, hard drives, SSDs)
Each varying by properties like, access speed, capacity, data transfer rate,
access methods from sequential to random.

The OS usually contains a "file system" where files are organised into
directories. The OS will:
create and destory files and directories,
primitives to manipulate files and directores (explained later),
mapping files onto secondary storage (explained later),
backup files

IO subsystem------------------------------------------------------------------
A common activity for OSs is to abstract differences in hardware devices.
This includes:
buffering,
caching,
spooling (piping).

The OS constructs a general device-driver interface which all hardware can
work with.

Protection and Security-----------------------------------------------------
Protection is any mechanism used to ensure processes aren't bumping into each
other in dangerous ways.

Security is defense of the system against external or internal attacks.

Services---------------------------------------------------------------
The operating system provides an environment for programs and users.
Some examples include:
User interface (i.e. CLI, GUI),
Program execution (loading programs into memory and executing or handling errors),
I/O operations,
File-system manipulation,
Communication (between processes and also across networks),
Error detection (in the CPU, memory or I/O),
Resource allocation,
Accounting (keeping track of how much a user is using),
Protection and security

System call parameters------------------------------------------------
Sometimes simply asking for the kernel to do a thing isn't enough.
Additional information is required. Enter the system call parameter.

There are three general ways to implement these parameters:
Put the parameters in registers (there may not be enough registers),
Store in memory in it's own special place (using in linux and solaris),
Store on the stack (potential breach of security and protection)

Types of system calls--------------------------------------------------
Process control:
end, abort,
load, execute,
create or destory process,
get or set process attributes,
wait for time,
wait for event, signal event,
allocate and free memory,
dump memory on error,
debugging, single step execution,
locks (mutex),

File management:
create or delete file,
open or close file,
read or write or reposition,
get or set file attributes

Device management:
request and release devices,
read or write or reposition,
get or set device attributes,
logically (software) attach and detach devices

Information maintenance:
get or set time,
get or set system data,
get or set and number of different attributes

Communication:
create or delete connections,
send or receive messages external to own process,
shared-memory (multithreading),
transfer status information,
attach and detach remote devices

Protection:
control access to resources
get or set permissions,
allow or deny user access

As you can see these system calls provide a convenient
environment for program development and execution.

A system program is a program the makes system calls.
(e.g. printf MAKES a system call it isn't one)

Some system programs are simple (ls, cp) while others are very complex.
(vim, gcc, valgrind, ncat)

System programs can be divided into:
File manipulation (touch),
Status information (ls, whoami),
File modification (vim),
Programming language support (gcc),
Program loading and executing,
Communications (mechanisms for communication like mail and pipes),
Background services (run in user land) (aka services, subsystems, daemons),
Application programs (not considered part of OS)

Design----------------------------------------------------
The user and system have different goals for the OS
User: Convenient to use, easy to learn, reliable, safe and fast
System: easy to design, implement and mantain, flexible, reliable, error-free and efficient

A policy is what will be done.
A mechanism how we do it.

(e.g. Policy: I want this concept of piping in my OS)

Back in the day OSs were written in assembly then Algol, PL/1.
Many modern systems are written in C.

UNIX consists of two parts,
Systems programs,
The kernel (file system, CPU scheduling, memory management, etc.)

Note how many parts of the OS relies on the kernel (monolithic kernel)

The layered approach allows for modularity at each layer (like network stack).
Layer 0 is hardware. If a change will occur at later i then only that layer needs
to change.

loadable kernel modules (implemented in linux and solaris)
uses an object oriented approach
this approach is like the layered approach but more flexible

System boot---------------------------------------------------------
When the system powers on it looks for execution at a fixed, static point
in memory.
A small piece of code *bootstracp loader* stored in either
ROM or EEPROM locates the kernel, loads it up and executes it

Processes---------------------------------------------------------------
An intuitive definition of a process is "a program in execution, which forms
the basis of all computation"

A *Batch* Operating System executes *jobs*
*Time-shared* systems execute *user programs* or *tasks*
(time share means that it's more than a single process running and completing
cpu switches jobs on the fly)

"A process is program in execution; process execution must progress in
sequential fashion."

The anatomy of a process includes:
text section (the program code that is stored in memory)
current activity including program counter, processor registers
the stack
the heap
data section (separate from stack or heap) that contains global variables

placement of all these parts are seen on page 3.5

Note: one program can be made of several processes

A process can change into one of the following states:
new - process is created
running - instructions are being executed
waiting - process is waiting for an event to occur
ready - process is waiting to be assigned a processor
terminated - process has finished execution

A finitie state machine can be constructed naturally from this but for clarity
see 3.7

Multiprocess------------------------------------------------------------------
Process Control Block (aka task control block) contains all information
about a process. This includes:
Process state - see above
Program counter - location of instruction to next execute
CPU registers - contents of all process-centric registers
CPU scheduling information - priorities, scheduling queue pointers
Memory-management information - memory allocated to the process
Accounting information - CPU used, clock time elapsed since start, time limits
I/O status information - I/O devices allowed to process, list of open files

see 3.8 for a visual of how it's stored

A CPU can use these PCBs to save (freeze) the state of a process and load another.

As mentioned before a program may have more than one program counter if it
is multithreaded.

Scheduling---------------------------------------------------------------------
A process scheduler is what selects which process the cpu should run next.
It does with a bunch of scheduling queues including
job queue - set of all processes in the system
ready queue - processes that are in the ready state
device queues - processes waiting for I/O
processes migrate among the various queues

see 3.14 for an excellent queueing diagram which makes the whole thing really
clear

A long-term scheduler (aka the job scheduler) selects which processes should be
brought into the ready queue. It is invoked infrequenty (every second or so)
A short-term scheduler (aka CPU scheduler) selects which process should be
executed next. It is invoked frequently (millisecond)

The long-term scheduler controls the *degree of multiprogramming*.
Processes can be described as I/O-bound which spend more time doing I/O than
computation (short CPU burst) or CPU-bound which is the opposite (long CPU burst)
The long-term scheduler strives for a good mix of both.

We can add a new scheduler called the medium-term scheduler. Which stores
process states on disk rather than memory.

The act of swapping out a process (then saving it) and loading a saved state
is called a context switch.
It is called such because of the context that is stored in the PCB.
The larger the PCB and more complex the OS we end up with longer context switches.

Some hardware offers multiple sets of registers per CPU this allows for
multiple contexts to be loaded at once.

Creating processes-------------------------------------------------------------
A process can be created through the parent child paradigm. This results in
a tree of processes. Generally a process is identified and managed via a process identifier
(pid).
A parent and child have various sharing options:
Parent and child share all resources
Parents share a subset of all their resources
Parents share no resources

There are also different options for execution:
Parent and child execute concurrently and independently
Parent waits for child to terminate

Though you have done this in CSSE2310 take a look at 3.23 for fork exec code example.

A child process can be terminated in various ways. We can wait for the child to
exit via wait. We can force the child to terminate via abort.
pid t_pid; int status;
pid = wait(&status)

If no parent is waiting then the terminated process becomes a zombie.
If a parent dies then the child process is an orphan.

Processes can either be independent (no communication) or cooperating.
Cooperating processes would allow for information sharing, computation speed,
modularity, convenience.
To have this we need interprocess communication (IPC).
There are two models for IPC:
Shared memory
Message passing
see 3.26 for an example of each

On the other hand independent processes cannot affect or be affected by the
execution of another process; (deterministic). But screw that that's not as
cool as processes that can communicate.

A common paradigm between cooperating processes is the producer-consumer.
In this we could have an unbounded buffer or a bounded buffer.
(What both do should be obvious from the titles)

If we're going to implement the message passing IPC then we need:
a mechanism for process communication to synchronize their actions
we cannot resort to shared variables to know where the messages are

We normally have to operations:
send(message)
receive(message)
If process P and process Q wish to communicate they will also need to establish a
communication link which they can use to send a receive.

We can implement such a link physically (shared memory or hardware bus) or
logically (direct or indirect, synchronous or asynchronous, automatic or explicit buffering, we discuess these further down)

We must ask ourselves other implementation questions:
How are the links established?
Can a link be associated with more than two processes?
How many links can there be between every pair of communicating processes.
What is the capacity of a link?
Is the size of a message that the link can accomodate fixed or variable?
Is a link unidirectional or bidirectional?

Direct communication - processes must name each other explicitly
The send AND receive calls requires the pid.
This means to accept messages we need to know the pid
The benefit is that links are established automatically.
unidirectional and bidirectional are both possible with direct communication

Indirect Communication - messages are directed and received from mailboxes (aka ports)
Think more of a mailbox/messageboard in an apartment complex
Each mailbox has a unique id
Processes can communicate only if they share a mailbox
send and receive requires the id of the mailbox
Maybe even create and destory mailboxes
This means a link may be associated with many processes all using the same mailbox
Links will only be made for processes with the same mailboxes.
unidirectional and bidirectional are both possible with indirect communication

Sychronization - blocking and non-blocking I/O
Blocking is synchronous and vice-versa
If both send a receive are synchronised (blocking) then we have a rendezvous.

Buffer - the amount of messages that can build up
zero capacity - sender must wait for receiver to accept the single message (rendezvous)
bounded capacity - finite length of n messages. The sender must wait if the buffer is full
unbounded capacity - infinite buffer. Sender never waits.

Some examples of IPC systems on 3.37 onward
The examples are briefly listed below
Sockets
Remote Procedure Calls (RPC)
Pipes (aka in Windows as anonymous pipes)
Named pipes

Threads------------------------------------------------------------------------
Why threads instead of multiprocess?
Thread creation is light-weight in comparison to process creation.
Threads are a great way for interactability in programs; responsive.
Kernels are generally multithreaded themselves.
Threads share resources in an easier way than shared memory or message parsing.
Thread switching has lower overhead than context switching.
Threads can take advantage of multiprocess architecture.

*Multicore programming* (aka mutliprocessor) makes use of all cores to run a
program. The challenges with multiprocessor systems include:
Dividing activities
balance
data splitting
data dependency
testing and debugging (non-deterministic)

Parallelism - a system can perform more than one task simultaneously
Concurrency - more than one task is making progress (time sharing and switching)

Data parallelism - distributes subsets of data across multiple cores, same operation on each.
Task parallelism - distribute threads across cores, each thread is performing unique operations.

Throughout the years as the number of threads a program runs increases so does
the architectural support for threading. A CPU can have *hardware threads*.
e.g. the Oracle SPARC T4 with 8 cores and 8 hardware threads per core.

4.9 shows a clear diagram of the difference between single and multithreaded.
Each thread requires its own registers and stack but they all share the same
code, data and files.

Amdahl's law
$$$
speedup \leq \frac{1}{S+\frac{1-S}{N}}
$$$
Where S is the serial portion (percentage of the program that is in serial)
N is the number of cores

We can see as the number of cores approaches infinity the speedup depends on
1/S.

*User threads* where management (creating, destroying, etc.) is done by
user-level thread libraries. Such
threads are POSIX Pthreads, Windows threads and Java threads.

*Kernel threads* where the kernel handles it. e.g. Windows, Solaris, Linux,
MACOSX

There are three models for multithreading
Many-to-one where many userland threads are mapped to a single kernel thread.
If one of the threads blocks then all the threads have to wait.
This is a system that is no longer used.
e.g. GNU Portable threads, Solaris Green Threads

One-to-one where each userland thread is mapped to a single kernel thread.
The number of threads per process sometimes gets restricted due to overhead.
e.g. Windows, Linux, Solaris 9 and later

Many-to-many where many user threads can be mapped to many kernel threads.
Sometimes sharing a kernel thread and sometimes not. This allows the OS
to create a sufficient number of threads then, when needed, make the userland
threads share kernel threads.
e.g. Solaris prior to version 9, windows with ThreadFiber package

User-level threading libraries-------------------------------------------------
Pthreads may be provided either as user-level or kernel-level (pthreads work
in windows)
POSIX is a standard (IEEE 1003.1c) API for thread creation and synchronization.
NOTE it is a specification (also a standard) NOT an implementation.
Many UNIX systems have Pthreads by default.

Java threads are managed by the JVM. Typically implemented with the underlying
OS's threading model.
4.18 for how to create java threads

Implicit threading. Since the number of threads a program can create is
becoming unmanageable and becoming difficult to ensure correctness.
Implicit threading lets the compiler and runtime libraries manage threading.
We explore the following methods:
Thread Pools
OpenMP
Grand Central Dispatch
Microsoft Threading Building Blocks (not explored)
java.util.concurrent (not explored)

Thread pools-------------------------------------------------------------------
Create a bunch of threads where they await work.
This is a good idea because requesting an existing thread is usually faster
than creating a new one.
The thread limit is determined by what's in the pool.
Paradigm is easier to work with (apparently).

OpenMP-------------------------------------------------------------------------
Set of compiler directives (the #pragma thingy) with an API for
C, C++ and FORTRAN.
It identifies parallel regions in the code where it can parallelise.
see 4.21 for an example of using the compiler directives.

Grand Central Dispatch---------------------------------------------------------
A technology made by Apple for Mac OSX and iOS.
It identifies parallel sections like OpenMP
Each block is placed in a dispatch queue. The block then leaves the queue and
gets assigned to a thread in the thread pool.

The dispatch queue could be:
serial (aka main queue) there is a single queue per process
concurrent where several blocks can be popped off at once (there are three queues labelled "high priority" "default" and "high")

Tough decisions implementing threads-------------------------------------------
Does fork() duplicate only the calling thread of all threads?
Does exec() replace all running threads in the process? (usually yes)

Signals are defined below (somewhere)
Should a signal be sent to the thread which the signal applies?
Send to every thread in the process?
Send to specific threads?
Assign an signal handling thread?

Thread-Local Storage (TLS)-----------------------------------------------------
is where a thread has its own copy of data.
This is useful when thread creation process is implicit (OpenMP etc.)

This is different to local variables since local variables are only
visible during single function invocation. TLS is visible across functions
invocations. TLS is like global and static variables but only for that thread.

Signals------------------------------------------------------------------------
Signals are used in UNIX systems to notify a process of an event.
A signal handler is used to process signals. A programmer can override the default signal handle.

Linux threads------------------------------------------------------------------
Linux refers to threads as tasks. See 4.27 for an details on creating threads for Linux.

[ZZZ Stopped at Chapter 4 have not seen lecture 30 July]
