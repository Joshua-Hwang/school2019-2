Define an OS-------------------------------------------------------------------
An OS is a program that acts as a middle man between the computer user and the
hardware.

The goals of an operating system are:
    Executing user programs
    Make the computer system convenient to use
    Use hardware in an efficient manner

The four components of on operating system. (Image at section 1.4)
1. At the top are the users
2. System application and programs (compilers, assemblers, text editors, database systems, etc.)
3. The operating system
4. The computer hardware

Where we see operating systems-----------------------------------------------
Shared computers like mainframes and minicomputer must keep all users happy.
Dedicated systemts like workstations have dedicated resources for each user BUT usually
    use shared resources from a server.
Handheld systems like phones have very few resources but are optimised for usability and battery life.
Embedded systems have little to no interface.

definitions of operating system-----------------------------------------------
The OS is a resource allocator,
    Manages ALL resources
    Divides the resources fairly (whatever that means) between conflicting requests.

The OS is a control program,
    Controls execution of programs to ensure programs are executed as expected with no errors.

As a matter of fact there isn't a single proper definition for an operating system.
"Everything a vendor ships with is a good approximation" (that varies wildly)

(unrelated) "The kernel is the one progam always running"

interrupts-------------------------------------------------------------------
An *interrupt* transfers control to the *interrupt service routine* through a
*interrupt vector* which contains the addresses of all the service routines.
(think of exceptions and events but more general so it encapsulates hardware)

The interrupt vector is a piece of code that gets executed when the interrupt
occurs.

A *trap* or *exception* is a "software generated" interrupt caused by errors
in the program or events.

An operating system is "interrupt driven"
When an interrupt occurs the operating system preserves the state of the CPU
by storing it in registers and the program counter

The operating system also determines what type of interrupt has occurred:
A polling [look it up]
A vectored interrupt system (see above)

storage device-------------------------------------------------------------
(Image at 1.10)
A pyramid with the top being:
registers,
cache,
main memory,
solid-state disk,
magnetic disk,
optical disk,
magnetic tapes

Computer system architecture---------------------------------------------
Most systems use a single general-purpose processor (a CPU)
or PDAs through a mainframe (I don't know what PDAs are)
They also use special-purpose processors (e.g. GPUs)

A new (not exactly new :P) wave of systems known as *multiprocessor systems* (aka parallel systems,
tightly-coupled systems). These systems CAN have increased throughput,
economy of scale, increased reliability (graceful degradation or fault tolerance)
[ZZZ expand these to defintions].

There are two types of multiprocess systems; asymmetric and symmetric.

In asymmetric we have cores of different types while in symmetric we have
many copies of the same, single type of core.

Note: multicore cpu is different to multiprocessor

Multiprogramming----------------------------------------------------
Multiprogramming organizes jobs (code and data) so the CPU always has one job to
execute. A subset of total jobs in the system is kept in memory.
One job is selected and run via *job scheduling*.
When a job has to wait for something (e.g. I/O) the OS switches jobs.

Timesharing (multitasking)-----------------------------------------------
Extends the idea of multiprogramming. The OS swaps jobs so quickly that
a user can interact seamlessly. Response time should be < 1 second.

A process is a program executing in memory.
CPU scheduling occurs if several jobs are ready to run at the same time.
Swapping occurs when a process can no longer fit in memory so it's sent to storage.
Virtual memory (explained later)

Interrupts are driven by hardware

Dual-mode splits operations into
User mode (=1),
kernel mode (=0).
This is done to protect the operating system and other system components.
Dual-mode is implemented by a *mode bit* provided by hardware.
There are certain *privileged* operations only executable in kernel mode.
A system call changes to kernel mode then resets to user mode when done.
Switching to kernel mode is called trap and returning is called return.

Many modern CPUs support multi-mode operations.
A virtual machine manager (VMM) mode for guest VMs.

An example of transitioning:
Construct a timer to prevent
infinite loops/process hogging resources.
This could be implemented thusly:
Create the following process before scheduling the process
Set interrupt after specific period
Decrement counter
When counter hits zero OS generates an interrupt.

A program is a passive entity while a process is an active entity.
A process needs resources (i.e. CPU, memory, I/O, files, initialisation data)
When a process terminates we need to take recycle those resouces.

Threading and multiple processes--------------------------------------------
A process has a *program counter* for each thread it has.
Each program counter specifies the location of the next instruction to execute.

Typically a system has many processes, maybe some users, with one or more
operating systems running concurrently on one or more CPUs.

Memory-----------------------------------------------------------------------
All data is in memory before and after the process.
All instructions in memory ready for execution.

Memory management determines what's in memory.
Memory management keeps track of which segments of memory are in use and by
whom, deciding which instructions and data to move into and out of memory,
allocating and deallocating memory space as needed.

Storage---------------------------------------------------------------
OS provides abstraction for storage (magnetic tape, hard drives, SSDs)
Each varying by properties like, access speed, capacity, data transfer rate,
access methods from sequential to random.

The OS usually contains a "file system" where files are organised into
directories. The OS will:
create and destory files and directories,
primitives to manipulate files and directores (explained later),
mapping files onto secondary storage (explained later),
backup files

IO subsystem------------------------------------------------------------------
A common activity for OSs is to abstract differences in hardware devices.
This includes:
buffering,
caching,
spooling (piping).

The OS constructs a general device-driver interface which all hardware can
work with.

Protection and Security-----------------------------------------------------
Protection is any mechanism used to ensure processes aren't bumping into each
other in dangerous ways.

Security is defense of the system against external or internal attacks.

Services---------------------------------------------------------------
The operating system provides an environment for programs and users.
Some examples include:
User interface (i.e. CLI, GUI),
Program execution (loading programs into memory and executing or handling errors),
I/O operations,
File-system manipulation,
Communication (between processes and also across networks),
Error detection (in the CPU, memory or I/O),
Resource allocation,
Accounting (keeping track of how much a user is using),
Protection and security

System call parameters------------------------------------------------
Sometimes simply asking for the kernel to do a thing isn't enough.
Additional information is required. Enter the system call parameter.

There are three general ways to implement these parameters:
Put the parameters in registers (there may not be enough registers),
Store in memory in it's own special place (using in linux and solaris),
Store on the stack (potential breach of security and protection)

Types of system calls--------------------------------------------------
Process control:
end, abort,
load, execute,
create or destory process,
get or set process attributes,
wait for time,
wait for event, signal event,
allocate and free memory,
dump memory on error,
debugging, single step execution,
locks (mutex),

File management:
create or delete file,
open or close file,
read or write or reposition,
get or set file attributes

Device management:
request and release devices,
read or write or reposition,
get or set device attributes,
logically (software) attach and detach devices

Information maintenance:
get or set time,
get or set system data,
get or set and number of different attributes

Communication:
create or delete connections,
send or receive messages external to own process,
shared-memory (multithreading),
transfer status information,
attach and detach remote devices

Protection:
control access to resources
get or set permissions,
allow or deny user access

As you can see these system calls provide a convenient
environment for program development and execution.

A system program is a program the makes system calls.
(e.g. printf MAKES a system call it isn't one)

Some system programs are simple (ls, cp) while others are very complex.
(vim, gcc, valgrind, ncat)

System programs can be divided into:
File manipulation (touch),
Status information (ls, whoami),
File modification (vim),
Programming language support (gcc),
Program loading and executing,
Communications (mechanisms for communication like mail and pipes),
Background services (run in user land) (aka services, subsystems, daemons),
Application programs (not considered part of OS)

Design----------------------------------------------------
The user and system have different goals for the OS
User: Convenient to use, easy to learn, reliable, safe and fast
System: easy to design, implement and mantain, flexible, reliable, error-free and efficient

A policy is what will be done.
A mechanism how we do it.

(e.g. Policy: I want this concept of piping in my OS)

Back in the day OSs were written in assembly then Algol, PL/1.
Many modern systems are written in C.

UNIX consists of two parts,
Systems programs,
The kernel (file system, CPU scheduling, memory management, etc.)

Note how many parts of the OS relies on the kernel (monolithic kernel)

The layered approach allows for modularity at each layer (like network stack).
Layer 0 is hardware. If a change will occur at later i then only that layer needs
to change.

loadable kernel modules (implemented in linux and solaris)
uses an object oriented approach
this approach is like the layered approach but more flexible

System boot---------------------------------------------------------
When the system powers on it looks for execution at a fixed, static point
in memory.
A small piece of code *bootstracp loader* stored in either
ROM or EEPROM locates the kernel, loads it up and executes it

Processes---------------------------------------------------------------
An intuitive definition of a process is "a program in execution, which forms
the basis of all computation"

A *Batch* Operating System executes *jobs*
*Time-shared* systems execute *user programs* or *tasks*
(time share means that it's more than a single process running and completing
cpu switches jobs on the fly)

"A process is program in execution; process execution must progress in
sequential fashion."

The anatomy of a process includes:
text section (the program code that is stored in memory)
current activity including program counter, processor registers
the stack
the heap
data section (separate from stack or heap) that contains global variables

placement of all these parts are seen on page 3.5

Note: one program can be made of several processes

A process can change into one of the following states:
new - process is created
running - instructions are being executed
waiting - process is waiting for an event to occur
ready - process is waiting to be assigned a processor
terminated - process has finished execution

A finitie state machine can be constructed naturally from this but for clarity
see 3.7

Multiprocess------------------------------------------------------------------
Process Control Block (aka task control block) contains all information
about a process. This includes:
Process state - see above
Program counter - location of instruction to next execute
CPU registers - contents of all process-centric registers
CPU scheduling information - priorities, scheduling queue pointers
Memory-management information - memory allocated to the process
Accounting information - CPU used, clock time elapsed since start, time limits
I/O status information - I/O devices allowed to process, list of open files

see 3.8 for a visual of how it's stored

A CPU can use these PCBs to save (freeze) the state of a process and load another.

As mentioned before a program may have more than one program counter if it
is multithreaded.

Scheduling---------------------------------------------------------------------
A process scheduler is what selects which process the cpu should run next.
It does with a bunch of scheduling queues including
job queue - set of all processes in the system
ready queue - processes that are in the ready state
device queues - processes waiting for I/O
processes migrate among the various queues

see 3.14 for an excellent queueing diagram which makes the whole thing really
clear

A long-term scheduler (aka the job scheduler) selects which processes should be
brought into the ready queue. It is invoked infrequenty (every second or so)
A short-term scheduler (aka CPU scheduler) selects which process should be
executed next. It is invoked frequently (millisecond)

The long-term scheduler controls the *degree of multiprogramming*.
Processes can be described as I/O-bound which spend more time doing I/O than
computation (short CPU burst) or CPU-bound which is the opposite (long CPU burst)
The long-term scheduler strives for a good mix of both.

We can add a new scheduler called the medium-term scheduler. Which stores
process states on disk rather than memory.

The act of swapping out a process (then saving it) and loading a saved state
is called a context switch.
It is called such because of the context that is stored in the PCB.
The larger the PCB and more complex the OS we end up with longer context switches.

Some hardware offers multiple sets of registers per CPU this allows for
multiple contexts to be loaded at once.

Creating processes-------------------------------------------------------------
A process can be created through the parent child paradigm. This results in
a tree of processes. Generally a process is identified and managed via a process identifier
(pid).
A parent and child have various sharing options:
Parent and child share all resources
Parents share a subset of all their resources
Parents share no resources

There are also different options for execution:
Parent and child execute concurrently and independently
Parent waits for child to terminate

Though you have done this in CSSE2310 take a look at 3.23 for fork exec code example.

A child process can be terminated in various ways. We can wait for the child to
exit via wait. We can force the child to terminate via abort.
pid t_pid; int status;
pid = wait(&status)

If no parent is waiting then the terminated process becomes a zombie.
If a parent dies then the child process is an orphan.

Processes can either be independent (no communication) or cooperating.
Cooperating processes would allow for information sharing, computation speed,
modularity, convenience.
To have this we need interprocess communication (IPC).
There are two models for IPC:
Shared memory
Message passing
see 3.26 for an example of each

On the other hand independent processes cannot affect or be affected by the
execution of another process; (deterministic). But screw that that's not as
cool as processes that can communicate.

A common paradigm between cooperating processes is the producer-consumer.
In this we could have an unbounded buffer or a bounded buffer.
(What both do should be obvious from the titles)

If we're going to implement the message passing IPC then we need:
a mechanism for process communication to synchronize their actions
we cannot resort to shared variables to know where the messages are

We normally have to operations:
send(message)
receive(message)
If process P and process Q wish to communicate they will also need to establish a
communication link which they can use to send a receive.

We can implement such a link physically (shared memory or hardware bus) or
logically (direct or indirect, synchronous or asynchronous, automatic or explicit buffering, we discuess these further down)

We must ask ourselves other implementation questions:
How are the links established?
Can a link be associated with more than two processes?
How many links can there be between every pair of communicating processes.
What is the capacity of a link?
Is the size of a message that the link can accomodate fixed or variable?
Is a link unidirectional or bidirectional?

Direct communication - processes must name each other explicitly
The send AND receive calls requires the pid.
This means to accept messages we need to know the pid
The benefit is that links are established automatically.
unidirectional and bidirectional are both possible with direct communication

Indirect Communication - messages are directed and received from mailboxes (aka ports)
Think more of a mailbox/messageboard in an apartment complex
Each mailbox has a unique id
Processes can communicate only if they share a mailbox
send and receive requires the id of the mailbox
Maybe even create and destory mailboxes
This means a link may be associated with many processes all using the same mailbox
Links will only be made for processes with the same mailboxes.
unidirectional and bidirectional are both possible with indirect communication

Sychronization - blocking and non-blocking I/O
Blocking is synchronous and vice-versa
If both send a receive are synchronised (blocking) then we have a rendezvous.

Buffer - the amount of messages that can build up
zero capacity - sender must wait for receiver to accept the single message (rendezvous)
bounded capacity - finite length of n messages. The sender must wait if the buffer is full
unbounded capacity - infinite buffer. Sender never waits.

Some examples of IPC systems on 3.37 onward
The examples are briefly listed below
Sockets
Remote Procedure Calls (RPC)
Pipes (aka in Windows as anonymous pipes)
Named pipes

Threads------------------------------------------------------------------------
Why threads instead of multiprocess?
Thread creation is light-weight in comparison to process creation.
Threads are a great way for interactability in programs; responsive.
Kernels are generally multithreaded themselves.
Threads share resources in an easier way than shared memory or message parsing.
Thread switching has lower overhead than context switching.
Threads can take advantage of multiprocess architecture.

*Multicore programming* (aka mutliprocessor) makes use of all cores to run a
program. The challenges with multiprocessor systems include:
Dividing activities
balance
data splitting
data dependency
testing and debugging (non-deterministic)

Parallelism - a system can perform more than one task simultaneously
Concurrency - more than one task is making progress (time sharing and switching)

Data parallelism - distributes subsets of data across multiple cores, same operation on each.
Task parallelism - distribute threads across cores, each thread is performing unique operations.

Throughout the years as the number of threads a program runs increases so does
the architectural support for threading. A CPU can have *hardware threads*.
e.g. the Oracle SPARC T4 with 8 cores and 8 hardware threads per core.

4.9 shows a clear diagram of the difference between single and multithreaded.
Each thread requires its own registers and stack but they all share the same
code, data and files.

Amdahl's law
$$$
speedup \leq \frac{1}{S+\frac{1-S}{N}}
$$$
Where S is the serial portion (percentage of the program that is in serial)
N is the number of cores

We can see as the number of cores approaches infinity the speedup depends on
1/S.

*User threads* where management (creating, destroying, etc.) is done by
user-level thread libraries. Such
threads are POSIX Pthreads, Windows threads and Java threads.

*Kernel threads* where the kernel handles it. e.g. Windows, Solaris, Linux,
MACOSX

There are three models for multithreading
Many-to-one where many userland threads are mapped to a single kernel thread.
If one of the threads blocks then all the threads have to wait.
This is a system that is no longer used.
e.g. GNU Portable threads, Solaris Green Threads

One-to-one where each userland thread is mapped to a single kernel thread.
The number of threads per process sometimes gets restricted due to overhead.
e.g. Windows, Linux, Solaris 9 and later

Many-to-many where many user threads can be mapped to many kernel threads.
Sometimes sharing a kernel thread and sometimes not. This allows the OS
to create a sufficient number of threads then, when needed, make the userland
threads share kernel threads.
e.g. Solaris prior to version 9, windows with ThreadFiber package

User-level threading libraries-------------------------------------------------
Pthreads may be provided either as user-level or kernel-level (pthreads work
in windows)
POSIX is a standard (IEEE 1003.1c) API for thread creation and synchronization.
NOTE it is a specification (also a standard) NOT an implementation.
Many UNIX systems have Pthreads by default.

Java threads are managed by the JVM. Typically implemented with the underlying
OS's threading model.
4.18 for how to create java threads

Implicit threading. Since the number of threads a program can create is
becoming unmanageable and becoming difficult to ensure correctness.
Implicit threading lets the compiler and runtime libraries manage threading.
We explore the following methods:
Thread Pools
OpenMP
Grand Central Dispatch
Microsoft Threading Building Blocks (not explored)
java.util.concurrent (not explored)

Thread pools-------------------------------------------------------------------
Create a bunch of threads where they await work.
This is a good idea because requesting an existing thread is usually faster
than creating a new one.
The thread limit is determined by what's in the pool.
Paradigm is easier to work with (apparently).

OpenMP-------------------------------------------------------------------------
Set of compiler directives (the #pragma thingy) with an API for
C, C++ and FORTRAN.
It identifies parallel regions in the code where it can parallelise.
see 4.21 for an example of using the compiler directives.

Grand Central Dispatch---------------------------------------------------------
A technology made by Apple for Mac OSX and iOS.
It identifies parallel sections like OpenMP
Each block is placed in a dispatch queue. The block then leaves the queue and
gets assigned to a thread in the thread pool.

The dispatch queue could be:
serial (aka main queue) there is a single queue per process
concurrent where several blocks can be popped off at once (there are three queues labelled "high priority" "default" and "high")

Tough decisions implementing threads-------------------------------------------
Does fork() duplicate only the calling thread of all threads?
Does exec() replace all running threads in the process? (usually yes)

Signals are defined below (somewhere)
Should a signal be sent to the thread which the signal applies?
Send to every thread in the process?
Send to specific threads?
Assign an signal handling thread?

Thread-Local Storage (TLS)-----------------------------------------------------
is where a thread has its own copy of data.
This is useful when thread creation process is implicit (OpenMP etc.)

This is different to local variables since local variables are only
visible during single function invocation. TLS is visible across functions
invocations. TLS is like global and static variables but only for that thread.

Signals------------------------------------------------------------------------
Signals are used in UNIX systems to notify a process of an event.
A signal handler is used to process signals. A programmer can override the default signal handle.

Linux threads------------------------------------------------------------------
Linux refers to threads as tasks. See 4.27 for an details on creating threads for Linux.

CPU scheduling---------------------------------------------------------
(aka short-term scheduler)
Maximum CPU utilization is achieved via multiprogramming.
In this paradigm we have a CPU-I/O Burst cycle. Where we have a burst of CPU
time then we wait for I/O, then we repeat.

CPU scheduling decisions may take place when a process:
Switches from running to waiting
Terminates
Switches from running to ready
Switches from waiting to ready,

Switching from the first two cases is nonpreemptive this means it waits till
the process has done what it wants to. In comparison, with preemptive
we cut the process off whenever we want. Consider when an important OS task
must be completed then we perform preemptive scheduling and kick our current
process back on the ready queue.

Dispatcher-----------------------------------------------------------------
The dispatcher module gives control of the CPU to the process selected
by the short term scheduler.
This involves:
switching context
switching to user land
jumping to the proper location in the user program to restart the program

Dispatch latency is the time it takes for the dispatcher to stop one process
and start another running

Scheduling Criteria-------------------------------------------------------
Scheduling can be measured by
CPU utilization is maximal
Throughput (num of processes that complete execution per time unit)
Turnaround time (amount of time to execute a process)
Waiting time (amount of time a process has been in the waiting queue)
Response time (amount of time it takes from request to first response)

Optimally, we want to:
Maximise CPU utilization and throughput
Minimise turnaround time, waiting time and response time.

Some scheduling methods----------------------------------------------------
First come first served (FCFS) [see fcfs.png for example]
Notice that the waiting times and process time change when we reorder the
example.

This is the convoy effect that occurs when short processes are put before
long processes.

Shortest job first (SJF) makes use of this convoy rule. SJF is acutally
optimal the only problem is it's hard to know the size of the request
beforehand.
No examples is shown for SJF since it's just like the above but you
reorder beforehand.

You could estimate the length of the next CPU burst by assuming that the
next burst will be like the previous one. Given
t_n = actual length of the nth CPU burst
\tau_n = the length that was predicted
\tau_{n+1} = the prediction for the next length
\alpha = some arbitrary number for weighing prediction against data (usually 0.5)
\tau_{n+1} = \alpha t_n + (1-\alpha)\tau_n

This method is called exponential averaging [see expavg.png]

Shortest remaining time accounts for variable arrival time and uses
preemption to cut processes short. i.e. P_1 is run for 1 second then P_2
arrives and requires less time so we move over to that one. After completing
P_2 we swap back to P_1 to finish.

Priority scheduling defines priority numbers to each process which determines
how much sequential CPU time a process gets. A smaller intger means
a higher priority. SJF is essentially priority scheduling but where the
priorities are determined by the inverse of the predicted CPU burst time.

Starvation (low priority processes always get superseded)
may occur in these cases we make it that as time progresses the priority
increases over time.

Round robin (RR) gives each process an equal and small amount of CPU time
called time quantum q; q is usually 10-100 milliseconds. After this the
process is preempted and requeued.

Round robin sucks if the overhead for context switching is higher than
the benefits of such a simple scheduling system.

RR typically has a higher average turnaround but better response time.
A context switch is usually 10 microseconds.
The time quantum should be greater than 80% of all CPU bursts.

Multilevel queue-------------------------------------------------------
Multilevel queue involves creating several ready queues. e.g. A foreground
queue (interactive) and a background queue (batch).
Processes permanently live in one of these queues. Each queue uses its own
scheduling algorithm. foreground uses RR while background uses FCFS.

The issue now becomes how we can schedule the different queues.
Fixed priority scheduling attempts to evaluate all in foreground then
from background. This causes issues with starvation again.

Time slice where each queue is treated round robin style (maybe giving
each queue a different weighting 80% forground 20% background).

Multilevel Feedback Queue--------------------------------------------
Modifies multilevel queues. Processes are now allowed to move between the
queues. We implement aging with this new ability.
Now we need to know how to upgrade a process to a new queue, how to demote
a process, and which queue to put a new process into. [see 6.27 for an
example of a multilevel feedback queue]

Thread scheduling------------------------------------------------------
When threads are supported in the OS threads are what are scheduled and not
processes.

Thread libraries that implement many-to-one and many-to-many user-level threads
they use lightweight processes (LWP).
Since scheduling is now a competition within a process this is called
process-contention scope (PCS). This is typically handled by the programmer.
On the other hand system-contention scope (SCS) occurs when competition
occurs between all threas on the system regardless of process.

Multi-processor scheduling----------------------------------------------
CPU scheduling becomes far more complex when multiple CPUs are available.
Various algorithms can alleviate this:

Homogeneous processors which all do the same tasks

Asymmetric multiprocessing which assigns one processor to accessing system
data structures which alleviates the need for data sharing

Symmetric multiprocessing (SMP) each processor is self-scheduling and
may or may not share a common ready queue. (This method is currently the
most common among real systems)

Processor affinity where processes has a bias to be run on a certain processor
either soft affinity, hard affinity and other variations.

Load balancing-------------------------------------------------------------
If we're using SMP then we must ensure the workload is evenly distributed
across all the cores.
Push migration is a periodic task that checks the load of various CPUs to move
tasks around to other processes.
Pull migration on the other hand lets idle processors steal waiting tasks from
busy processors.

Another problem with multi-processor is retrieving memory that the current
CPU is not privy to or does not have fast access for. This causes obvious
overhead cost.

The current trend is having multiple processors on the same chip which
consumes less power than mutliple chips. There is also a desire to increase
the number of threads per core, this allows the system to take advantage of
memory stalls and work on another thread.

Virtualisation and scheduling----------------------------------------------
Virtualisation software does the scheduling for multiple guests on the CPU.
Each guest does its own scheduling completely oblivious to the fact that
they don't own the CPU.
This can cause problems with poor response time and even can effect
the clocks of the guest.

Real-time CPU scheduling---------------------------------------------------
Real time systems can be categorised as either:
Soft real-time systems where there is not guarantee to when critical time-based
processes will be scheduled.
Hard real-time systems where there is a stronger guarantee to meeting deadlines.

Hard real-time systems have to deal with two types of latencies:
Interrupt latency - time from input of interrupt to start of interrupt process
Dispatch latency - essentially context switch overhead

As shown in diagram conflict.png
The dispatch latency contains a conflict phase. This is due to either
preemption (context switching) of processes running in kernel mode or
releasing low-priority processes for high-priority processes.

It's a good idea to implement real-time systems using priority based
scheduling to ensure certain tasks get completed quicker than others.
Unfortunately this only gets us to soft real-time.

To implement hard real-time we add a new characteristic called periodics.
Processing time (t), deadline (d), period (p).
0 \leq t \leq d \leq p
The rate of a period task is 1/p
The idea is that in a section of time (the period) we MUST complete t
processing time before the deadline (d). This gets repeated every period.

One way of implementing priority now is to make it the inverse of the period.
Shorter period = higher priority.
Note: it's still possible to miss the deadline.
To guarantee scheduling we must have a CPU utilization that is at least
N(2^(1/N) - 1)

Now instead of priority based on period we base it on deadlines. Earlier
deadline gets top priority.

Proportional Share Scheduling (ZZZ I don't know where this is meant to go)

Real world examples----------------------------------------------------
Linux scheduling works by a system known as Completely Fair Scheduler (CFS).
There are scheduling classes which are ordered in priority. The scheduler
will pick the highest priority task in the highest scheduling class.
Instead of fixed time, quantum time based allotment we have CPU time
proportions.
By default there are 2 scheduling classes but more can be added;
default and real-time.

A *target latency* is calculated which gives and interval of time during
which a task should run at least once. The target latency can be increased
if maintaining that target is too hard.

The predicted run time is stored in a variable *vruntime*. This runtime also
considers the priority of the process. The scheduler chooses the lowest
virtual time.

The linux scheduler has a *global priority scheme* which maps 100 to 139 as
normal priority (nice value -20 to +19). The range 0 to 99 is for the
realtime class.

Windows scheduling uses priority-based preemptive scheduling. It's simply the
highest priority thread that runs next.
The dispatcher is the scheduler.
Threads run until a block happens, using time-slice or preempted by a higher
priority process.
Real-time threads can preempt non-real-time.
There are 32 levels in the windows priority scheme. 1-15 is variable and
16-31 is real-time.
Priority 0 is memory-management thread. Each priority has it's own queue.
If no thread is ready ye tthen run idle thread.

Solaris uses priority based scheduling. Six classes are used:
Time sharing (default) (TS)
Interactive (IA)
Real time (RT)
System (SYS)
Fair Share (FSS)
Fixed priority (FP)

Each thread is associated with a single class. Each class has its own
scheduling algorithms. See multi-level feedback queue.

Evaluating algorithms----------------------------------------------------
We can either do it deterministically via deterministic modelling where
we consider an example case and see how our model performs in it.

We could use queue models where we consider the average number of arrivals,
I/O bursts etc.

Little's Formula assumes the system is in a steady state which means the
number of processes arriving is equal to the number of processes leaving.
n = \lambda \times W
Where n is the average queue length
W is the average waiting time
\lambda is the average arrival rate
Little's law states \lambda \times W = n

Another way to evaluate algorithms is to run them in a simulation which has
many variables but perfectly describes what will happen in a real-world
setting. But even the simulations can't to everything, high cost and
environments vary.

Data races---------------------------------------------------------------
A common issue is two processes taking a common variable into their
registers, modifying it, then putting it back into the variable.
If two processes take the same value into their registers at the same time
then each process will incorrectly change the variable based on old data.

Critical Section Problem
There are multiple processes with a *critical
section* where the process changes common variables or updates tables or
writes to a file, etc.

A solution to this problem would be for each process to ask permission to
enter critical section in something called an entry section,
which may then follow with an
exit section after doing the critical work. The stuff after the exit section
is called the remainder section.

We mark solutions to the critical selection problem against three properties.
If these properties don't hold then it doesn't work.

Mutual exclusion ensures if process P is doing critical work then no other
processes can be doing critical work.

Progress ensures that if no process is in their critical section and a process
would like to be in it that they are not forced to wait indefinitely for
the opportunity to do their work. Note, a process is allowed to hog
critical mode.

Bounded Waiting ensures a bound exists for the number of times a process
is allowed to enter their critical section in a given time frame.

We have seen this kind of problem before when processes require kernel mode
or userland mode.
In a preemptive kernel, it's easy to pause the process who's in kernel mode
and let another process do someting in kernel mode alongside other kernel mode
processes.
In a non-preemptive kernel we have to wait for the process in kernel mode
to get back to userland.

Peterson's solution
An example of how it works uses two processes. The processes share two
variables,
int turn
Boolean flag[2]
turn indicates whose turn it is to enter their critical section
flag[i] == true implies that the process i is ready.

Both processes follow this algorithm
do {
    flag[i] = true;
    turn = j;
    while (flag[j] && turn == j);
    // critical section
    flag[i] = false;
    // remainder section
} while (true);

[Get comfortable with the fact that this algorithm satisfies the three
requirements for a solution. Past you doesn't know how but is hoping you do.]

Many systems provide a solution by the idea of locking.
[5.18 onwards shows some nice imlementations for locks using some atomic
operations]

Mutex locks
The solutions shown above require work on the programmers part. This kinda
sucks especially for application programmers. Instead OS designers have
created a simple mutex lock. You acquire() the lock then release() it.
acquire() doesn't wait for the lock but instead results in a busy wait
scenario. This kind of lock is called a spinlock.

Semaphores provide a more sophisticated way to synchronise.
Access of a mutex is done via two atomic operations; wait() and signal()
[aka P() and V()]

wait(S) {
    while (S <= 0); // busy wait
    s--;
}

signal(S) { S++; }

You have likely seen these in CSSE2310. A clear use case for these is queues.
You add to the queue and signal the semaphore (acting like a counter).
Once a process/thread is willing to work on the next element on the queue
they'll dequeue it and drop the semaphore by one.

You can also use semaphores by jumping between 0 and 1 which essentially
becomes mutex lock; aka binary semaphore.

The implementation for wait you saw above makes use of busy waiting.
This is certainly not a good way of doing things. We reimplement semaphores
using a waiting queue for each semaphore and a ready queue.
We now havw two new operations for the queues; block() puts the process that
called block onto the waiting queue, wakeup(P) removes a process P from the
waiting queue and puts it into the ready queue.

The semaphore is now a struct containing the int value for counting and
a pointer to a linked list of processes.

wait(semaphorek *S) {
    S->value--;
    if (S->value < 0) {
        add this process to S->list;
        block();
    }
}

signal(semaphore *S) {
    S->value++;
    if (S->value <= 0) {
        remove a process P from S->list;
        wakeup(P);
    }
}

In this way each process waiting for a signal is put on a waiting list.
The OS doesn't even bother running this process until the semaphore let's us
know that it's ready for a new process. From here we dequeue some process
(unimportant which) and get it ready for running.

We introduce some new terms needed for describing new problems.
Deadlock is exactly as you've seen in CSSE2310. Requiring two deadlocks
means that a deadlock state can occur.

Deadlocks can occur if four conditions hold simulataneously
1. Mutual exclusion means a single process can use a resource at a time.
2. A process is already holding a single resource and is waiting to hold another aswell
3. No preemption, it cannot be released when the process wants to
4. Circular wait, everyone wants what everyone else has

Starvation is the event when a semaphore will never be removed from a semaphores
waiting list. Starvation occurs in a deadlock.

Priority Inversion is where a lower priority process holds a lock needed
for a higher-priority process.

The new synch schemes we'll explore are first explained in the context of these
classic problems:
Bounded-buffer problem. This is where you have a consumer and producer process
who are adding and taking from a queue. The problem is, can you construct such
a pattern with your synching tools. [5.32 shows an implementation with
semaphores]

Reader-Writers problem. This is where we have processes that are readers and
those that are writers (can read and write). We should allow multiple readers
but only one writer.
There are two variations to this problem.
1. No reader is kept waiting unless the writer is doing something
2. Writer performs the modification ASAP
In the first case the writer may be forced to wait for readers to finish
reading while the second case may kick readers out.
[5.35 shows an implementation with semaphores]

Dining-Philosophers problem. Imagine a round table with a chopstick on either
side of each chair. The problem is that two neighbouring chairs are sharing
a single chopstick. Thus given 5 philosophers there are only 5 chopsticks.
Each philosopher must use both chopsticks to eat and there is no way for the
philosophers to communicate.

This is obviously difficult since deadlocks can occur. A potential solution
is to somehow allow a philosopher to only pick up both chopsticks when
both are available. But even then, what happens if two philosophers
attempt to reach for their chopsticks?
You make use of an asymmetric solution. An odd numbered philosopher picks up
their left chopstick first while an even numbered philosopher picks up their
right chopstick first.

Higher level sync constructs-------------------------------------------------
A monitor is a higher level construct that can be used for shared variables.
The monitor object contains variables and methods in it. Only one process is
allowed to call the procedures and access the variables within it. It can
be implemented with semaphores (semaphores can be implemented with it as well).

Condition variable. Condition variables are like events.
Think of a condition variable x like a gate.
Processes call x.wait() and are stuck until someone calls x.signal().
Once x.signal() is called then all processes that are waiting in x.wait()
are woken up.
A choice that must be made is which process should x.signal() release. FCFS
is not adequate for the most part. Instead we should release the process
with the highest priority. It could be implemented with a x.wait(c) where
c is the priority of the process.
[ZZZ yeah don't understand the problems with condition variables as shown in
5.47 so I'm gonna let you figure that out]

[Implementation for the above are show from 5.48 onward]

Single resource allocation uses same idea as the condition variable.
Consider ResourceAllocator R such that R.acquire(t) specifies the amount of
time t we will use the resource for. From here the ResourceAllocator determines
when you're allowed to access the resource. Afterwards you call R.release
[ZZZ I don't see the difference]

Alternative sync ideas-------------------------------------------------------
Transactional memory
Make the read-write operations to memory atomic.

OpenMP and Function Programming Languages both make use of fancy compiler
tricks to parallelise programs.

Real world sync examples------------------------------------------------------
Solaris synchronisation
Uses adaptive mutexes, condition variables, readers-writers and turnstiles.
[slides don't say anything about turnstiles]

Windows synchronisation
Uses spinlocks, dispatcher objects which act like mutexes, semaphores
events and timers.

Linux synchronisation
Uses semaphores, atomic integers, spinlocks, reader-writer both versions

Pthreads synchronisation
Pthreads is an OS-independent API
It provides mutex locks and condition variables
It also sometimes provides read-write locks and spinlocks

System model-----------------------------------------------------------------
We construct a graph to visualise the system.
[7.8 shows the symbols we'll be using]

From the graph we observe that if the graph doens't contain a cycle then
there aren't any deadlocks. If there are cycles then there MIGHT be a deadlock.

Deadlock prevention and avoidance---------------------------------------
We intend to find a way to deal with deadlocks either we
prevent deadlocks (deadlock prevention) or we avoid deadlocks
(deadlock avoidance).
We could also find a way to handle the deadlock state and recover. You could
also ignore the problem and pretend deadlocks never occur like most modern
operating systems do.

In deadlock prevention we solve the following:
Mutual exclusion is not required for sharable resources.
Hold and wait, processes aren't allowed to hold multiple resources. Another
option could be to allocate all resources when the process first begins.
This could obviously cause starvation.

No preemption. If a process is holding a few resources and would like another
yet it can't get it then all resources it was holding will get released.
The process then has to rewait for all of the resources. The process will
restart when it can get all its resources (and the one it wants) back.

Circular wait. We impose a total ordering of resources. From here when
processes request resources they have to do it in the defined order.

In deadlock avoidance, the processes must tell the OS a priori,
what resources they would like to use beforehand.
This allows a deadlock-avoidance algorithm
to dynamically examine the resource state to ensure a deadlock condition
may never occur. The algorithm determines if allocating the resources to a
particular process will keep the allocation in a *safe state*.

A system is in a safe state if there exists a sequence <P_1, P_2, ..., P_n>
of ALL the processes in the systems such that for each P_i, the resources
that P_i can still request can be satisfied by currently available resources
+ resources held by all the P_j, with j < i. That is:
* If P_i resource needs are not immediately available,
then P_i can wait until all P_j have finished
* When P_j is finished, P_i can obtain resources, execute, return allocated
resources, and terminate
* When P_i terminates, P_{i+1} can obtain its needed resources, and so on
In this safe state no deadlocks can occur.

[It's likely a resource allocation graph will be on the exam along with
determining if it's in a safe state or not. 7.23 shows some examples of
safe and unsafe graphs.]

Banker's algorithm attempts to implement deadlock avoidance. In Banker's
algorithm each process must make a priori claim of maximum use. A process
may have to wait for a resource and when it gets the resource it must return
it in finite time.
[See 7.28 for details] Essentially it groups the resources
into groups and calls each element of the group an instance. The process
becomes calculating matrices where the rows are the processes and the columns
are the resource types. Safety is also checked.

Deadlock recovery-------------------------------------------------------------
The alternative solution is to somehow recover from a deadlock state.
If we create and maintain a wait-for graph (different from a resource
allocation graph) then we can simply check this graph for cycles periodically.
If a cycle exists then we've detected a deadlock. Unfortunately all cycle
detection algorithms require n^2 operations.

A wait-for graph is essentially a resource allocation graph but with the
resource nodes removed. Processes simply pointing to other processes.
The graph is represented as a vector and stuff is done to it.
[see 7.37 for details]

The question now becomes how do you recover? We have a few options.
* Abort all deadlocked processes
* Abort one process at a time until the deadlock cycle is eliminated.
In the second case we need to determine which process should be eliminated
first.

From there we select the victim, then we rollback this victim to some previous
safe state. It's likely that our selection algorithm will choose the same
victim over and over again so we should also change the selection to make
that process less likely.

Memory-------------------------------------------------------------------------
In our logical address apce the operating system takes the first few addresses
(e.g. 0 to 25600). For other processes we partition a portion of memory.
This segment is defined by a base (the smaller address) to a limit. On every
memory access the CPU needs to check that the access is within the limit and the
base.

Programs are stored on disk in storage. Those programs need to be loaded into
memory for the CPU to run them as processes. The programs that are waiting to
enter memory are put into the input queue. Processes are free to be placed
anywhere in memory. The beginning address 0000 is not the only option.

Imagine you're writing a piece of code, how does the OS determine the memory
for this program? When the compiler has read the program it then needs to decide
how to order the memory in your program in an preferable way. A compiler
typically binds these symbolic addresses (like a stack variable called count)
to relocatable (relative) addresses such as "14 bytes from the beginning of
the module". The linkage editor or loader (I call linker) binds the relocatable
addresses to absolute addresses (such as 17014).

Address binding can happen at any of the three stages:
Compile time
When the program compiles the variables and memory are given
absolute addresses. If the memory location is required to change then the
program would be forced to recompile. MS-DOS.COM programs did this.
Load time
Final binding to absolute addresses is done just before loading
the program into memory.
Execution time
The program can be moved even during execution of the process.
To do this we need special hardware. Most general-purpose OSs use this method.

Like taught in CSSE2310 the memory addresses generated by the CPU
are called logical or virtual addresses. While the memory unit talks in
physical addresses.

Logical and physical addresses are the same in compile-time and load-time
address-binding schemes but not in execution time address binding schemes.

A memory management unit (MMU) is a piece of hardware that maps the logical
address space onto the physical address space. There are many different methods
for the MMU to operate.

Perhaps the simplest form is to map each process to a new base; in this method
the base register is now called to relocation register. For example
say the memory for a user process is offset by 14000 then an access to
logical address 0 will lead to physical addres 14000 and logical address 346
will map to physical address 14346.

Dynamic loading---------------------------------------------------------
An optimisation trick is to only load the necessary routines on demand.
The unloaded routines are kept on disk in a relocatable load format. This
method is preferable for low memory and also for routines that are rarely
called but have massive code bases. The OS does not need to add any new
features for this optimsation. It is up to the user to make use of this
optimisation.

Linking---------------------------------------------------------------------
Static linking adds all the libraries to the final binary.
Instead we have a binary that only gets linked at execution time much
like in dynamic loading. This dynamic linking is also known as shared libraries.
With dynamic linking a stub is placed in the binary which is used to locate
the appropriate memory-resident library routine.
The OS checks if the routine is in memory. If it is then the stub gets
replaced with the routine address otherwise the routine gets loaded in then
linked.
It's important to consider the specific version of the system library we're
linking to. Perhaps incompatible version may cause surprises.

Standard swapping------------------------------------------------------------
A process can be swapped out of memory and stored on storage and be brought
back for continued execution. The place where the process gets stored on disk
is called a backing store. o

It's important to only swap the lower priority process so higher priority
processes can take valuable memory space. The system maintains a ready
queue with ready-to-run processes which have memory images on the disk.

Now a question arises, does the swapped out process need to swap back
in to the same physical addresses? This is determined by the binding
method we have chosen.

Swapping out costs time; proportional to the size of the process that
needs to get swapped out. This time is called the transfer time.
When the CPU intends to switch to a process that isn't available in memory
it must grab it from storage. This could make the context switch time
very long. It can be reduced slightly by knowing the *exact* amount
of memory the process is using.

When swapping out processes we have to keep in mind that we're swapping out
chunks of memory; not just CPU attention. Consider a pending I/O operation.
If the process that's requesting I/O gets swapped out then the physical
memory location will be replaced with a different process. If the I/O operation
comes back to this new process then this could cause issues in memory.
There are two main ways to avoid this scenario:
Just prevent swapping when I/O is being performed.
Let the kernel handle all I/O. This is known as double buffering and it
comes with additional overhead.

This standard swapping is rarely implemented in modern OSs since it is too
slow. A modified version of standard swapping is implemented instead and
is used in the most extreme cases.

In the mobile world swapping is not done at all. This is because the tiny
flash memory on mobile devices is precious storage space. Flash has a small
amount of space, limited number of write cycles and has poor throughput.

iOS asks apps to volutarily relinquish allocated memory. Read-only data
get thrown out of memory and reloaded from flash on demand.
Android terminates apps if there is little free memory but first writes
the application state to flash for fast restart. [This is kinda like swapping
isn't it?]

Contiguous allocation----------------------------------------------------
As we've seen memory is in short supply and should be treated as such.
One possible way of maximising memory is to find a way to fit all the
processes together as tightly as possible in memory. Contiguous allocation
is a primitive way to do just that.
Memory is usually split into two sections.
The resident operating system usually held in the first few addresses of
memory. After that we have the user processes which are held in high memory.
Each process is contained in a single contiguous section of memory.

The relocation registers mentioned before are used as boundaries for
each process.

If the bounds are dynamic and we keep some of the kernel code saved
in storage rather than running in memory then we call it transient
operating system code and this allows the kernel to change sizes.

Multiple partition allocation----------------------------------------------
The memory is split into fixed size partitions. When a partition is free
it is available for another process to take it. The degree of multiprogramming
is such a system is limited by the number of partitions.

Another version is variable partition where a *hole* is empty space for a
process to fill. From INFS2200 you know there's some issues with this
kind of method because of fragmentation.

There are a few methods on how to fill holes with processes.
First-fit. Allocate the first hole big enough
Best-fit. Allocate the smallest hold big enough
Worst-fit Allocate the largest hole.

There are two measures for fragmentation.
External fragmentation is when there's technically enough space to fulfill
a process but the space is fragmented such that it can't be used.
Internal fragmentation is when the space given to the process is more than
required.

First fit analysis reveals that that 50% of blocks get lost to fragmentation.
This means a third of memory may be unusable aka the 50 percent rule.
[ZZZ No idea where the 1/3 bit came from]

Another options is to just move the processes around in memory. This is
known as compaction. Of course we can only do this if relocation is
dynamic on execution. Moving processes also hits the problem of swapping
and I/O requests. Thus doiuble buffering is also required.

Segmentation----------------------------------------------------------------
Segmentation of memory is a scheme that intends to marry the programmer's
view of memory with the physical hardware.

In this paradigm memory is viewed as a space where separate segments exist.
In the physical space we then split our memory into segments.

The logical address consists of <segment-number, offset>
The sgement table maps this two dimensional logical address to
a two dimensional physical address. It contains the base (where
the segment resides in memory) and the limit which specifies the
length of the segment.

Additional tools for this setup include:
Segment-table base register (STBR) points to the segment table's location
in memory
Segment-table length register (STLR) indicates the number of segments
used by a program (segment s is legal if s < STLR)
[8.32 of the book gives a great diagram]

Segmenting the memory offers protection from invalid and illegal read, write
and execute privileges.

Paging----------------------------------------------------------------------
Previously we have required the processes are maintained in a single contiguous
space in memory. Instead we allow for noncontiguous memory. This helps
with fragmentation and avoid problems with varying size memory chunks.

We divide our physical memory into fixed size blocks called frames and
divide our logical memory into blocks of the same size called pages.
Internal fragmentation is still a problem however as we'll see.

Internal fragmentation is on average 1/2 frame size. Thus it makes sense
to reduce the frame size. This is a tradeoff since the number of frames
we need to keep a track of increases.

This process requires that we keep a track of all free frames as to know
where we may map new pages. A page table is used to keep track of the mappings.

Every access requires two access operations; one for the page table and the other
for the frame. We can solve this issue with a special fast-lookup hardware
cache called associative memory or translation look-aside buffers (TLBs).
The TLB works by acting like a dictionary with keys and value pairs.
The hardware is made in such a way that the key gets checked in constant time.
The value in the TLB is the cached value of the memory so that gets returned
without checking the memory.
Some TLBs store address space identifiers (ASIDs) in each TLB entry. This
uniquely identifies each process to provide address space protection for that
process.
On a TLB miss, the value is loaded into the TLB for faster access next time.
But TLBs are typically small only around 64 to 1024 entries so there must
be a policy for removal and replacement.
[8.43 shows a diagram using TLB]

The page table itself is stored in memory and is defined by two registers.
Page-table base register (PTBR) points to the page table
Page-table length register (PTLR) indicates size of the page

Effective access time (EAT) is the calculation to find the average memory access
considering we have a TLB. The calculation requires the hit ratio for
the TLB, the access time for the TLB and the access time for the memory.

Each frame has a corresponding bit for read or read-write. Maybe even an
execute only if we'd want to implement that. We can also add a valid-invalid
bit attached to the page table to check if the process is allowed to access
those parts of memory. Another option is to use a page-table length register
(PTLR) which defines the size of the page table, any page table entries beyond
this register are illegal.

Shared pages-------------------------------------------------------------
Another option is shared code. This is where we have a single copy of read only
(reentrant) code which multiple processes can share. (text editor, compilers,
window systems).
This is the same idea as shared resources with multiple threads.
[8.48 for a diagram of this shared resource]

In this form we need to reorganise our pages such that each process has their
own page table.

Structure of page table---------------------------------------------------
There are a few options for structures. These options help to reduce the
size of the page table in memory.

Hierarchical page tables
This is the two level page table we've seen in CSSE2310. [See 8.51 for
a visual]

The address requires to logical address offsets. One for the outer page table
then another for the second page table.

This structure may not be the most ideal. Instead try a hashed page table.
We hash the logical address (with the possibility of collisions). To handle
the collisions each hash table is attached to a linked list which we trudge
for the correct mapping.

The third structure is called inverted page tables. This is very much like
the original page table except the PID is part of the address. This PID along
with the page number is then searched in the page table
(this is different to providing the offset in the standard address).
A hash table can be combined with this structure to reduce space even
further.

Oracle SPARC solaris intends to be very efficient with low overhead. It
makes use of hashing but in a more complex way.
It uses two hash tables. One for the kernel and one for the user processes.
It uses contiguous areas of memory which is more efficient than mapping
individual pages.
[Other insane things I don't understand. Look at 8.61 for information
and other examples]

Optimisations with virtual memory----------------------------------------------
Using a virtual memory mapping we can do some nice optimisations. For one we
can map different parts of virtual memory to the same page/frame. This allows
us to use shared libraries which are sometimes stored in between the heap and
the stack. Keep in mind the stack grows from higher addresses downward while
the heap grows upward.

Deamnd paging---------------------------------------------------------------
Another neat trick is to partially bring a process into memory that had been
swapped out. We simply bring in the pages of memory that are needed instead of
bringing the entire process. This is called demand paging.

To figure out whether a virtual address is mapped to an actual location in
memory, the page table is given a valid or invalid bit. When it's set to
invalid we know we need to retrieve from storage otherwise it's the same
paging as before.

If there's a reference to a page but the page table
has the invalid bit set then the operating system will handle it.
It will first look at the other tables, if it finds the address is invalid
in any way then it aborts otherwise we assume it's in storage.
We find some free space in memory and schedule a disk operation to retrieve our
desired page. We then indicate in the table that our validation bit is set.

This demand paging requires hardware support since we need to add and modify a
bit in each entry to notify ourselves of valid and invalid.

There are three major activities for deamnd paging; servicing the interrupt
(aka the page fault), read the page from storage (this takes the
longest), restart the process.

Obviously this entire process takes a long time. To measure the Effective
Access Time (EAT) we need to know the page fault rate which is the chance
we hit a page fault. From there the equation should be simple.

In the real world we find that 1/1000 page fault rate incurs a slowdown by a
factor of 40.

There are certain optimisations that can be made to this process. We can
attempt to make the swap space I/O faster than the file system even if its
own the same device. We can do this by letting creating larger chunks for swaps
but also requiring less storage management than a long-term organised file
system.

We could also a make a copy of the process at load time making the move
faster [ZZZ don't know how]. This system is used in older BSD Unix.
Of the same ilk we could drop the page frames we know aren't important. This
method is used in Solaris and current BSD.

We use the same idea as swap spaces. We create a modify (dirty) bit to denote
if the page has been modified.

We optimise even further by introducing another bit into the page table called
the modify (aka dirty) bit. This bit tells us if it was modified since
conception. If not then we can freely destory it and retrieve from the
file system. Otherwise we must hold onto the entry and save it on disk.

A naive approach to page replacement is as such.
Find the frame on disk we want to bring to memory. Find a free frame. If there
isn't one then we find a victim frame. We check if the victim frame is dirty
then we must write it to the swap space. This algorithm has worst case two
frame transfers.

There are numerous other algorithms for handling page replacements.
We should expect a tradeoff [see 9.31] between the number of page faults
and the number of frames. This is not always the case however, Belady's anomaly
means that have more frames might even make things worse in some cases.

The optimal strategy is to see into the future and know which pages will
be retrieved. Of course this isn't possible but is the most useable upper
bound to measure from. This can suffer from Belady's Anomaly.

First-in-first-out (FIFO) algorithm. We consider memory as a queue where we
enqueue and dequeue frames. This doesn't suffer from Belady's Anomaly.

Least Recently Used (LRU) self explanatory. In general slightly better than
FIFO. We could measure usage in many ways. Using a counter for each page
entry, or use a stack that's ordered by recently used.
This doesn't suffer from Belady's Anomaly.

LRU still needs special hardware and yet is still slow. We could employ
LRU approximation algorithms intead.
We could use a reference bit that splits our pages into those that are
"recent" and those that aren't. Whenever a replacement needs to happen, we
only replace those that don't have the "recent" bit set and reset all
"recent" bits to 0. Another option is to build on this idea and augment it by
using the FIFO method this is called the second-chance algorithm.

We further enhance the second-chance algorithm by creating ordered pairs with
the modify bit (reference, modify). We now have 4 cases;
(0,0) not modified or used recently. Good choice for replacing.
(0,1) not recently used by modified. Second best choice. Still need to write
changes to swap.
(1,0) recently used but clean. In this case it's likely we'll have to use again.
(1,1) worst choice for replacement.

Based on the same idea as counting frequency we now offer
Most Frequently Used (MFU) as contradiction of LFU. The argument for MFU is that
a process that has not been used is likely to be used very soon.

Another algorithm is to have a pool of free frames. When we "replace" a frame
we're actually storing this frame in the pool for a little while much like a
buffer. This is helpful if we evict the wrong victim frame since we can
quickly revert our choice much more quickly. When it's convenient for us we
actually write the frame to disk.

Copy on write-----------------------------------------------------------------
Copy On Write (aka COW) allows a fork to initially share the same pages in
memory. Only the pages that are modified are copied.

When a page needs to be copied we allocate from a pool of zero-fill-on-demand
pages. [ZZZ I think this is like a promise of available pages] [ZZZ also don't
know why they have to be santised via zeroed]
vfork() is a variation of fork the does this COW optimisation. This is commonly
used for children that will call exec.

The issue comes if the pool of pages has dried up. In this case we swap out
some other process.

Allocating frames-------------------------------------------------------------
How do we figure out how many frames each process needs? There are two major
schools of thought; fixed allocation and priority allocation.
In fixed allocation we give every process an equal number of frames. e.g.
100 frames would share between 5 processes so that each process got 20 frames.

In proportional allocation we make use of the initial size of each process to
determine which frames to allocate. The equation is simple [9.45]. This isn't
the only measure for proportions though.

Priority allocation uses the same proportional allocation method but instead
is based on priority rather than size.

Global replacement, if a process needs additional frames and our initial
allocation was incorrect then we take from the set of all frame or even steal
frames from another process.
This can cause problems since now you've just pushed the problem to another
process; execution times can vary greatly. There is more throughput now though
[Don't know what throughput means in this context] This has lead to global
replacement being the commonly used method.

In local a process cannot steal from different processes nor can it steal from
the global set of frames. In comparison to global, we have more consistent
per-process performance but that does mean some proesses will be using more
frames than necessary.

Many systems are Non-Uniform Memory Access (NUMA) which means the different
memory devices have different speeds. Thus we now have a new factor to consider
when allocating memory. Processes of high priority would preferably be put into
high speed memory.
Solaris solved this issue by creating lgroups.
Lgroups are structures that keep track of CPU/Memory low latency groups.
The scheduler makes us of the information in lgroups and aims to schedule the
threads and allocate the memory of a single process to a single lgroup.

If there not enough pages then the computer is *thrashing*.
Thrashing is where the we repeatedly get page faults when looking for pages.
Thus we find ourselves constantly replacing frames and then replacing the ones
we replaced continuously.
Thrashing leads to low CPU utilization, which makes the operating system think
that it needs to increase the degree of multiprogramming which causes more
processes to come up and make things worse.

As multiprogramming gets larger and larger the CPU utilization rises then falls
after some maximal point.

[stopped at 9.51]
