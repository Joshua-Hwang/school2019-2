Define an OS-------------------------------------------------------------------
An OS is a program that acts as a middle man between the computer user and the
hardware.

The goals of an operating system are:
    Executing user programs
    Make the computer system convenient to use
    Use hardware in an efficient manner

The four components of on operating system. (Image at section 1.4)
1. At the top are the users
2. System application and programs (compilers, assemblers, text editors, database systems, etc.)
3. The operating system
4. The computer hardware

Where we see operating systems-----------------------------------------------
Shared computers like mainframes and minicomputer must keep all users happy.
Dedicated systemts like workstations have dedicated resources for each user BUT usually
    use shared resources from a server.
Handheld systems like phones have very few resources but are optimised for usability and battery life.
Embedded systems have little to no interface.

definitions of operating system-----------------------------------------------
The OS is a resource allocator,
    Manages ALL resources
    Divides the resources fairly (whatever that means) between conflicting requests.

The OS is a control program,
    Controls execution of programs to ensure programs are executed as expected with no errors.

As a matter of fact there isn't a single proper definition for an operating system.
"Everything a vendor ships with is a good approximation" (that varies wildly)

(unrelated) "The kernel is the one progam always running"

interrupts-------------------------------------------------------------------
An *interrupt* transfers control to the *interrupt service routine* through a
*interrupt vector* which contains the addresses of all the service routines.
(think of exceptions and events but more general so it encapsulates hardware)

The interrupt vector is a piece of code that gets executed when the interrupt
occurs.

A *trap* or *exception* is a "software generated" interrupt caused by errors
in the program or events.

An operating system is "interrupt driven"
When an interrupt occurs the operating system preserves the state of the CPU
by storing it in registers and the program counter

The operating system also determines what type of interrupt has occurred:
A polling [look it up]
A vectored interrupt system (see above)

storage device-------------------------------------------------------------
(Image at 1.10)
A pyramid with the top being:
registers,
cache,
main memory,
solid-state disk,
magnetic disk,
optical disk,
magnetic tapes

Computer system architecture---------------------------------------------
Most systems use a single general-purpose processor (a CPU)
or PDAs through a mainframe (I don't know what PDAs are)
They also use special-purpose processors (e.g. GPUs)

A new (not exactly new :P) wave of systems known as *multiprocessor systems* (aka parallel systems,
tightly-coupled systems). These systems CAN have increased throughput,
economy of scale, increased reliability (graceful degradation or fault tolerance)
[ZZZ expand these to defintions].

There are two types of multiprocess systems; asymmetric and symmetric.

In asymmetric we have cores of different types while in symmetric we have
many copies of the same, single type of core.

Note: multicore cpu is different to multiprocessor

Multiprogramming----------------------------------------------------
Multiprogramming organizes jobs (code and data) so the CPU always has one job to
execute. A subset of total jobs in the system is kept in memory.
One job is selected and run via *job scheduling*.
When a job has to wait for something (e.g. I/O) the OS switches jobs.

Timesharing (multitasking)-----------------------------------------------
Extends the idea of multiprogramming. The OS swaps jobs so quickly that
a user can interact seamlessly. Response time should be < 1 second.

A process is a program executing in memory.
CPU scheduling occurs if several jobs are ready to run at the same time.
Swapping occurs when a process can no longer fit in memory so it's sent to storage.
Virtual memory (explained later)

Interrupts are driven by hardware

Dual-mode splits operations into
User mode (=1),
kernel mode (=0).
This is done to protect the operating system and other system components.
Dual-mode is implemented by a *mode bit* provided by hardware.
There are certain *privileged* operations only executable in kernel mode.
A system call changes to kernel mode then resets to user mode when done.
Switching to kernel mode is called trap and returning is called return.

Many modern CPUs support multi-mode operations.
A virtual machine manager (VMM) mode for guest VMs.

An example of transitioning:
Construct a timer to prevent
infinite loops/process hogging resources.
This could be implemented thusly:
Create the following process before scheduling the process
Set interrupt after specific period
Decrement counter
When counter hits zero OS generates an interrupt.

A program is a passive entity while a process is an active entity.
A process needs resources (i.e. CPU, memory, I/O, files, initialisation data)
When a process terminates we need to take recycle those resouces.

Threading and multiple processes--------------------------------------------
A process has a *program counter* for each thread it has.
Each program counter specifies the location of the next instruction to execute.

Typically a system has many processes, maybe some users, with one or more
operating systems running concurrently on one or more CPUs.

Memory-----------------------------------------------------------------------
All data is in memory before and after the process.
All instructions in memory ready for execution.

Memory management determines what's in memory.
Memory management keeps track of which segments of memory are in use and by
whom, deciding which instructions and data to move into and out of memory,
allocating and deallocating memory space as needed.

Storage---------------------------------------------------------------
OS provides abstraction for storage (magnetic tape, hard drives, SSDs)
Each varying by properties like, access speed, capacity, data transfer rate,
access methods from sequential to random.

The OS usually contains a "file system" where files are organised into
directories. The OS will:
create and destory files and directories,
primitives to manipulate files and directores (explained later),
mapping files onto secondary storage (explained later),
backup files

IO subsystem------------------------------------------------------------------
A common activity for OSs is to abstract differences in hardware devices.
This includes:
buffering,
caching,
spooling (piping).

The OS constructs a general device-driver interface which all hardware can
work with.

Protection and Security-----------------------------------------------------
Protection is any mechanism used to ensure processes aren't bumping into each
other in dangerous ways.

Security is defense of the system against external or internal attacks.

Services---------------------------------------------------------------
The operating system provides an environment for programs and users.
Some examples include:
User interface (i.e. CLI, GUI),
Program execution (loading programs into memory and executing or handling errors),
I/O operations,
File-system manipulation,
Communication (between processes and also across networks),
Error detection (in the CPU, memory or I/O),
Resource allocation,
Accounting (keeping track of how much a user is using),
Protection and security

System call parameters------------------------------------------------
Sometimes simply asking for the kernel to do a thing isn't enough.
Additional information is required. Enter the system call parameter.

There are three general ways to implement these parameters:
Put the parameters in registers (there may not be enough registers),
Store in memory in it's own special place (using in linux and solaris),
Store on the stack (potential breach of security and protection)

Types of system calls--------------------------------------------------
Process control:
end, abort,
load, execute,
create or destory process,
get or set process attributes,
wait for time,
wait for event, signal event,
allocate and free memory,
dump memory on error,
debugging, single step execution,
locks (mutex),

File management:
create or delete file,
open or close file,
read or write or reposition,
get or set file attributes

Device management:
request and release devices,
read or write or reposition,
get or set device attributes,
logically (software) attach and detach devices

Information maintenance:
get or set time,
get or set system data,
get or set and number of different attributes

Communication:
create or delete connections,
send or receive messages external to own process,
shared-memory (multithreading),
transfer status information,
attach and detach remote devices

Protection:
control access to resources
get or set permissions,
allow or deny user access

As you can see these system calls provide a convenient
environment for program development and execution.

A system program is a program the makes system calls.
(e.g. printf MAKES a system call it isn't one)

Some system programs are simple (ls, cp) while others are very complex.
(vim, gcc, valgrind, ncat)

System programs can be divided into:
File manipulation (touch),
Status information (ls, whoami),
File modification (vim),
Programming language support (gcc),
Program loading and executing,
Communications (mechanisms for communication like mail and pipes),
Background services (run in user land) (aka services, subsystems, daemons),
Application programs (not considered part of OS)

Design----------------------------------------------------
The user and system have different goals for the OS
User: Convenient to use, easy to learn, reliable, safe and fast
System: easy to design, implement and mantain, flexible, reliable, error-free and efficient

A policy is what will be done.
A mechanism how we do it.

(e.g. Policy: I want this concept of piping in my OS)

Back in the day OSs were written in assembly then Algol, PL/1.
Many modern systems are written in C.

UNIX consists of two parts,
Systems programs,
The kernel (file system, CPU scheduling, memory management, etc.)

Note how many parts of the OS relies on the kernel (monolithic kernel)

The layered approach allows for modularity at each layer (like network stack).
Layer 0 is hardware. If a change will occur at later i then only that layer needs
to change.

loadable kernel modules (implemented in linux and solaris)
uses an object oriented approach
this approach is like the layered approach but more flexible

System boot---------------------------------------------------------
When the system powers on it looks for execution at a fixed, static point
in memory.
A small piece of code *bootstracp loader* stored in either
ROM or EEPROM locates the kernel, loads it up and executes it

Processes---------------------------------------------------------------
An intuitive definition of a process is "a program in execution, which forms
the basis of all computation"

A *Batch* Operating System executes *jobs*
*Time-shared* systems execute *user programs* or *tasks*
(time share means that it's more than a single process running and completing
cpu switches jobs on the fly)

"A process is program in execution; process execution must progress in
sequential fashion."

The anatomy of a process includes:
text section (the program code that is stored in memory)
current activity including program counter, processor registers
the stack
the heap
data section (separate from stack or heap) that contains global variables

placement of all these parts are seen on page 3.5

Note: one program can be made of several processes

A process can change into one of the following states:
new - process is created
running - instructions are being executed
waiting - process is waiting for an event to occur
ready - process is waiting to be assigned a processor
terminated - process has finished execution

A finitie state machine can be constructed naturally from this but for clarity
see 3.7

Multiprocess------------------------------------------------------------------
Process Control Block (aka task control block) contains all information
about a process. This includes:
Process state - see above
Program counter - location of instruction to next execute
CPU registers - contents of all process-centric registers
CPU scheduling information - priorities, scheduling queue pointers
Memory-management information - memory allocated to the process
Accounting information - CPU used, clock time elapsed since start, time limits
I/O status information - I/O devices allowed to process, list of open files

see 3.8 for a visual of how it's stored

A CPU can use these PCBs to save (freeze) the state of a process and load another.

As mentioned before a program may have more than one program counter if it
is multithreaded.

Scheduling---------------------------------------------------------------------
A process scheduler is what selects which process the cpu should run next.
It does with a bunch of scheduling queues including
job queue - set of all processes in the system
ready queue - processes that are in the ready state
device queues - processes waiting for I/O
processes migrate among the various queues

see 3.14 for an excellent queueing diagram which makes the whole thing really
clear

A long-term scheduler (aka the job scheduler) selects which processes should be
brought into the ready queue. It is invoked infrequenty (every second or so)
A short-term scheduler (aka CPU scheduler) selects which process should be
executed next. It is invoked frequently (millisecond)

The long-term scheduler controls the *degree of multiprogramming*.
Processes can be described as I/O-bound which spend more time doing I/O than
computation (short CPU burst) or CPU-bound which is the opposite (long CPU burst)
The long-term scheduler strives for a good mix of both.

We can add a new scheduler called the medium-term scheduler. Which stores
process states on disk rather than memory.

The act of swapping out a process (then saving it) and loading a saved state
is called a context switch.
It is called such because of the context that is stored in the PCB.
The larger the PCB and more complex the OS we end up with longer context switches.

Some hardware offers multiple sets of registers per CPU this allows for
multiple contexts to be loaded at once.

Creating processes-------------------------------------------------------------
A process can be created through the parent child paradigm. This results in
a tree of processes. Generally a process is identified and managed via a process identifier
(pid).
A parent and child have various sharing options:
Parent and child share all resources
Parents share a subset of all their resources
Parents share no resources

There are also different options for execution:
Parent and child execute concurrently and independently
Parent waits for child to terminate

Though you have done this in CSSE2310 take a look at 3.23 for fork exec code example.

A child process can be terminated in various ways. We can wait for the child to
exit via wait. We can force the child to terminate via abort.
pid t_pid; int status;
pid = wait(&status)

If no parent is waiting then the terminated process becomes a zombie.
If a parent dies then the child process is an orphan.

Processes can either be independent (no communication) or cooperating.
Cooperating processes would allow for information sharing, computation speed,
modularity, convenience.
To have this we need interprocess communication (IPC).
There are two models for IPC:
Shared memory
Message passing
see 3.26 for an example of each

On the other hand independent processes cannot affect or be affected by the
execution of another process; (deterministic). But screw that that's not as
cool as processes that can communicate.

A common paradigm between cooperating processes is the producer-consumer.
In this we could have an unbounded buffer or a bounded buffer.
(What both do should be obvious from the titles)

If we're going to implement the message passing IPC then we need:
a mechanism for process communication to synchronize their actions
we cannot resort to shared variables to know where the messages are

We normally have to operations:
send(message)
receive(message)
If process P and process Q wish to communicate they will also need to establish a
communication link which they can use to send a receive.

We can implement such a link physically (shared memory or hardware bus) or
logically (direct or indirect, synchronous or asynchronous, automatic or explicit buffering, we discuess these further down)

We must ask ourselves other implementation questions:
How are the links established?
Can a link be associated with more than two processes?
How many links can there be between every pair of communicating processes.
What is the capacity of a link?
Is the size of a message that the link can accomodate fixed or variable?
Is a link unidirectional or bidirectional?

Direct communication - processes must name each other explicitly
The send AND receive calls requires the pid.
This means to accept messages we need to know the pid
The benefit is that links are established automatically.
unidirectional and bidirectional are both possible with direct communication

Indirect Communication - messages are directed and received from mailboxes (aka ports)
Think more of a mailbox/messageboard in an apartment complex
Each mailbox has a unique id
Processes can communicate only if they share a mailbox
send and receive requires the id of the mailbox
Maybe even create and destory mailboxes
This means a link may be associated with many processes all using the same mailbox
Links will only be made for processes with the same mailboxes.
unidirectional and bidirectional are both possible with indirect communication

Sychronization - blocking and non-blocking I/O
Blocking is synchronous and vice-versa
If both send a receive are synchronised (blocking) then we have a rendezvous.

Buffer - the amount of messages that can build up
zero capacity - sender must wait for receiver to accept the single message (rendezvous)
bounded capacity - finite length of n messages. The sender must wait if the buffer is full
unbounded capacity - infinite buffer. Sender never waits.

Some examples of IPC systems on 3.37 onward
The examples are briefly listed below
Sockets
Remote Procedure Calls (RPC)
Pipes (aka in Windows as anonymous pipes)
Named pipes

Threads------------------------------------------------------------------------
Why threads instead of multiprocess?
Thread creation is light-weight in comparison to process creation.
Threads are a great way for interactability in programs; responsive.
Kernels are generally multithreaded themselves.
Threads share resources in an easier way than shared memory or message parsing.
Thread switching has lower overhead than context switching.
Threads can take advantage of multiprocess architecture.

*Multicore programming* (aka mutliprocessor) makes use of all cores to run a
program. The challenges with multiprocessor systems include:
Dividing activities
balance
data splitting
data dependency
testing and debugging (non-deterministic)

Parallelism - a system can perform more than one task simultaneously
Concurrency - more than one task is making progress (time sharing and switching)

Data parallelism - distributes subsets of data across multiple cores, same operation on each.
Task parallelism - distribute threads across cores, each thread is performing unique operations.

Throughout the years as the number of threads a program runs increases so does
the architectural support for threading. A CPU can have *hardware threads*.
e.g. the Oracle SPARC T4 with 8 cores and 8 hardware threads per core.

4.9 shows a clear diagram of the difference between single and multithreaded.
Each thread requires its own registers and stack but they all share the same
code, data and files.

Amdahl's law
$$$
speedup \leq \frac{1}{S+\frac{1-S}{N}}
$$$
Where S is the serial portion (percentage of the program that is in serial)
N is the number of cores

We can see as the number of cores approaches infinity the speedup depends on
1/S.

*User threads* where management (creating, destroying, etc.) is done by
user-level thread libraries. Such
threads are POSIX Pthreads, Windows threads and Java threads.

*Kernel threads* where the kernel handles it. e.g. Windows, Solaris, Linux,
MACOSX

There are three models for multithreading
Many-to-one where many userland threads are mapped to a single kernel thread.
If one of the threads blocks then all the threads have to wait.
This is a system that is no longer used.
e.g. GNU Portable threads, Solaris Green Threads

One-to-one where each userland thread is mapped to a single kernel thread.
The number of threads per process sometimes gets restricted due to overhead.
e.g. Windows, Linux, Solaris 9 and later

Many-to-many where many user threads can be mapped to many kernel threads.
Sometimes sharing a kernel thread and sometimes not. This allows the OS
to create a sufficient number of threads then, when needed, make the userland
threads share kernel threads.
e.g. Solaris prior to version 9, windows with ThreadFiber package

User-level threading libraries-------------------------------------------------
Pthreads may be provided either as user-level or kernel-level (pthreads work
in windows)
POSIX is a standard (IEEE 1003.1c) API for thread creation and synchronization.
NOTE it is a specification (also a standard) NOT an implementation.
Many UNIX systems have Pthreads by default.

Java threads are managed by the JVM. Typically implemented with the underlying
OS's threading model.
4.18 for how to create java threads

Implicit threading. Since the number of threads a program can create is
becoming unmanageable and becoming difficult to ensure correctness.
Implicit threading lets the compiler and runtime libraries manage threading.
We explore the following methods:
Thread Pools
OpenMP
Grand Central Dispatch
Microsoft Threading Building Blocks (not explored)
java.util.concurrent (not explored)

Thread pools-------------------------------------------------------------------
Create a bunch of threads where they await work.
This is a good idea because requesting an existing thread is usually faster
than creating a new one.
The thread limit is determined by what's in the pool.
Paradigm is easier to work with (apparently).

OpenMP-------------------------------------------------------------------------
Set of compiler directives (the #pragma thingy) with an API for
C, C++ and FORTRAN.
It identifies parallel regions in the code where it can parallelise.
see 4.21 for an example of using the compiler directives.

Grand Central Dispatch---------------------------------------------------------
A technology made by Apple for Mac OSX and iOS.
It identifies parallel sections like OpenMP
Each block is placed in a dispatch queue. The block then leaves the queue and
gets assigned to a thread in the thread pool.

The dispatch queue could be:
serial (aka main queue) there is a single queue per process
concurrent where several blocks can be popped off at once (there are three queues labelled "high priority" "default" and "high")

Tough decisions implementing threads-------------------------------------------
Does fork() duplicate only the calling thread of all threads?
Does exec() replace all running threads in the process? (usually yes)

Signals are defined below (somewhere)
Should a signal be sent to the thread which the signal applies?
Send to every thread in the process?
Send to specific threads?
Assign an signal handling thread?

Thread-Local Storage (TLS)-----------------------------------------------------
is where a thread has its own copy of data.
This is useful when thread creation process is implicit (OpenMP etc.)

This is different to local variables since local variables are only
visible during single function invocation. TLS is visible across functions
invocations. TLS is like global and static variables but only for that thread.

Signals------------------------------------------------------------------------
Signals are used in UNIX systems to notify a process of an event.
A signal handler is used to process signals. A programmer can override the default signal handle.

Linux threads------------------------------------------------------------------
Linux refers to threads as tasks. See 4.27 for an details on creating threads for Linux.

CPU scheduling---------------------------------------------------------
(aka short-term scheduler)
Maximum CPU utilization is achieved via multiprogramming.
In this paradigm we have a CPU-I/O Burst cycle. Where we have a burst of CPU
time then we wait for I/O, then we repeat.

CPU scheduling decisions may take place when a process:
Switches from running to waiting
Terminates
Switches from running to ready
Switches from waiting to ready,

Switching from the first two cases is nonpreemptive this means it waits till
the process has done what it wants to. In comparison, with preemptive
we cut the process off whenever we want. Consider when an important OS task
must be completed then we perform preemptive scheduling and kick our current
process back on the ready queue.

Dispatcher-----------------------------------------------------------------
The dispatcher module gives control of the CPU to the process selected
by the short term scheduler.
This involves:
switching context
switching to user land
jumping to the proper location in the user program to restart the program

Dispatch latency is the time it takes for the dispatcher to stop one process
and start another running

Scheduling Criteria-------------------------------------------------------
Scheduling can be measured by
CPU utilization is maximal
Throughput (num of processes that complete execution per time unit)
Turnaround time (amount of time to execute a process)
Waiting time (amount of time a process has been in the waiting queue)
Response time (amount of time it takes from request to first response)

Optimally, we want to:
Maximise CPU utilization and throughput
Minimise turnaround time, waiting time and response time.

Some scheduling methods----------------------------------------------------
First come first served (FCFS) [see fcfs.png for example]
Notice that the waiting times and process time change when we reorder the
example.

This is the convoy effect that occurs when short processes are put before
long processes.

Shortest job first (SJF) makes use of this convoy rule. SJF is acutally
optimal the only problem is it's hard to know the size of the request
beforehand.
No examples is shown for SJF since it's just like the above but you
reorder beforehand.

You could estimate the length of the next CPU burst by assuming that the
next burst will be like the previous one. Given
t_n = actual length of the nth CPU burst
\tau_n = the length that was predicted
\tau_{n+1} = the prediction for the next length
\alpha = some arbitrary number for weighing prediction against data (usually 0.5)
\tau_{n+1} = \alpha t_n + (1-\alpha)\tau_n

This method is called exponential averaging [see expavg.png]

Shortest remaining time accounts for variable arrival time and uses
preemption to cut processes short. i.e. P_1 is run for 1 second then P_2
arrives and requires less time so we move over to that one. After completing
P_2 we swap back to P_1 to finish.

Priority scheduling defines priority numbers to each process which determines
how much sequential CPU time a process gets. A smaller intger means
a higher priority. SJF is essentially priority scheduling but where the
priorities are determined by the inverse of the predicted CPU burst time.

Starvation (low priority processes always get superseded)
may occur in these cases we make it that as time progresses the priority
increases over time.

Round robin (RR) gives each process an equal and small amount of CPU time
called time quantum q; q is usually 10-100 milliseconds. After this the
process is preempted and requeued.

Round robin sucks if the overhead for context switching is higher than
the benefits of such a simple scheduling system.

RR typically has a higher average turnaround but better response time.
A context switch is usually 10 microseconds.
The time quantum should be greater than 80% of all CPU bursts.

Multilevel queue-------------------------------------------------------
Multilevel queue involves creating several ready queues. e.g. A foreground
queue (interactive) and a background queue (batch).
Processes permanently live in one of these queues. Each queue uses its own
scheduling algorithm. foreground uses RR while background uses FCFS.

The issue now becomes how we can schedule the different queues.
Fixed priority scheduling attempts to evaluate all in foreground then
from background. This causes issues with starvation again.

Time slice where each queue is treated round robin style (maybe giving
each queue a different weighting 80% forground 20% background).

Multilevel Feedback Queue--------------------------------------------
Modifies multilevel queues. Processes are now allowed to move between the
queues. We implement aging with this new ability.
Now we need to know how to upgrade a process to a new queue, how to demote
a process, and which queue to put a new process into. [see 6.27 for an
example of a multilevel feedback queue]

Thread scheduling------------------------------------------------------
When threads are supported in the OS threads are what are scheduled and not
processes.

Thread libraries that implement many-to-one and many-to-many user-level threads
they use lightweight processes (LWP).
Since scheduling is now a competition within a process this is called
process-contention scope (PCS). This is typically handled by the programmer.
On the other hand system-contention scope (SCS) occurs when competition
occurs between all threas on the system regardless of process.

Multi-processor scheduling----------------------------------------------
CPU scheduling becomes far more complex when multiple CPUs are available.
Various algorithms can alleviate this:

Homogeneous processors which all do the same tasks

Asymmetric multiprocessing which assigns one processor to accessing system
data structures which alleviates the need for data sharing

Symmetric multiprocessing (SMP) each processor is self-scheduling and
may or may not share a common ready queue. (This method is currently the
most common among real systems)

Processor affinity where processes has a bias to be run on a certain processor
either soft affinity, hard affinity and other variations.

Load balancing-------------------------------------------------------------
If we're using SMP then we must ensure the workload is evenly distributed
across all the cores.
Push migration is a periodic task that checks the load of various CPUs to move
tasks around to other processes.
Pull migration on the other hand lets idle processors steal waiting tasks from
busy processors.

Another problem with multi-processor is retrieving memory that the current
CPU is not privy to or does not have fast access for. This causes obvious
overhead cost.

The current trend is having multiple processors on the same chip which
consumes less power than mutliple chips. There is also a desire to increase
the number of threads per core, this allows the system to take advantage of
memory stalls and work on another thread.

Virtualisation and scheduling----------------------------------------------
Virtualisation software does the scheduling for multiple guests on the CPU.
Each guest does its own scheduling completely oblivious to the fact that
they don't own the CPU.
This can cause problems with poor response time and even can effect
the clocks of the guest.

Real-time CPU scheduling---------------------------------------------------
Real time systems can be categorised as either:
Soft real-time systems where there is not guarantee to when critical time-based
processes will be scheduled.
Hard real-time systems where there is a stronger guarantee to meeting deadlines.

Hard real-time systems have to deal with two types of latencies:
Interrupt latency - time from input of interrupt to start of interrupt process
Dispatch latency - essentially context switch overhead

As shown in diagram conflict.png
The dispatch latency contains a conflict phase. This is due to either
preemption (context switching) of processes running in kernel mode or
releasing low-priority processes for high-priority processes.

It's a good idea to implement real-time systems using priority based
scheduling to ensure certain tasks get completed quicker than others.
Unfortunately this only gets us to soft real-time.

To implement hard real-time we add a new characteristic called periodics.
Processing time (t), deadline (d), period (p).
0 \leq t \leq d \leq p
The rate of a period task is 1/p
The idea is that in a section of time (the period) we MUST complete t
processing time before the deadline (d). This gets repeated every period.

One way of implementing priority now is to make it the inverse of the period.
Shorter period = higher priority.
Note: it's still possible to miss the deadline.
To guarantee scheduling we must have a CPU utilization that is at least
N(2^(1/N) - 1)

Now instead of priority based on period we base it on deadlines. Earlier
deadline gets top priority.

Proportional Share Scheduling (ZZZ I don't know where this is meant to go)

Real world examples----------------------------------------------------
Linux scheduling works by a system known as Completely Fair Scheduler (CFS).
There are scheduling classes which are ordered in priority. The scheduler
will pick the highest priority task in the highest scheduling class.
Instead of fixed time, quantum time based allotment we have CPU time
proportions.
By default there are 2 scheduling classes but more can be added;
default and real-time.

A *target latency* is calculated which gives and interval of time during
which a task should run at least once. The target latency can be increased
if maintaining that target is too hard.

The predicted run time is stored in a variable *vruntime*. This runtime also
considers the priority of the process. The scheduler chooses the lowest
virtual time.

The linux scheduler has a *global priority scheme* which maps 100 to 139 as
normal priority (nice value -20 to +19). The range 0 to 99 is for the
realtime class.

Windows scheduling uses priority-based preemptive scheduling. It's simply the
highest priority thread that runs next.
The dispatcher is the scheduler.
Threads run until a block happens, using time-slice or preempted by a higher
priority process.
Real-time threads can preempt non-real-time.
There are 32 levels in the windows priority scheme. 1-15 is variable and
16-31 is real-time.
Priority 0 is memory-management thread. Each priority has it's own queue.
If no thread is ready ye tthen run idle thread.

Solaris uses priority based scheduling. Six classes are used:
Time sharing (default) (TS)
Interactive (IA)
Real time (RT)
System (SYS)
Fair Share (FSS)
Fixed priority (FP)

Each thread is associated with a single class. Each class has its own
scheduling algorithms. See multi-level feedback queue.

Evaluating algorithms----------------------------------------------------
We can either do it deterministically via deterministic modelling where
we consider an example case and see how our model performs in it.

We could use queue models where we consider the average number of arrivals,
I/O bursts etc.

Little's Formula assumes the system is in a steady state which means the
number of processes arriving is equal to the number of processes leaving.
n = \lambda \times W
Where n is the average queue length
W is the average waiting time
\lambda is the average arrival rate
Little's law states \lambda \times W = n

Another way to evaluate algorithms is to run them in a simulation which has
many variables but perfectly describes what will happen in a real-world
setting. But even the simulations can't to everything, high cost and
environments vary.

[ZZZ Stopped at Week 3 8 Aug]
