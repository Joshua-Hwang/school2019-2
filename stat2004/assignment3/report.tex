\documentclass{article}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{bm}

\linespread{1.3}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\setcounter{secnumdepth}{0}
\setcounter{MaxMatrixCols}{20}
\renewcommand{\arraystretch}{1.5}

\newcommand{\ts}{\textsuperscript}
\newcommand{\diff}{\mathop{}\!\mathrm{d}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\var}{\text{Var}}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\p{\lparan}{\rparan}

\title{STAT2004 Assignment 3}
\author{Joshua Hwang (44302650)}

\begin{document}
All work in this assignment, unless otherwise specified, is set to 4 significant
figures.

\section{Exercise 1}
\subsection{a}
Using R we determined some summary statistics.

\begin{verbatim}
    db <- read.csv("likes.csv")
    db$user <- as.factor(db$user)
    summary(db)
    summary(db[db$user == "influencer",])
    summary(db[db$user == "regular",])
\end{verbatim}

Which gave the following summaries
\begin{verbatim}
> summary(db)
         user         rating     
 influencer: 98   Min.   :1.000  
 regular   :100   1st Qu.:2.000  
                  Median :3.000  
                  Mean   :3.096  
                  3rd Qu.:5.000  
                  Max.   :5.000  
> summary(db[db$user == "influencer",])
         user        rating     
 influencer:98   Min.   :1.000  
 regular   : 0   1st Qu.:1.000  
                 Median :4.000  
                 Mean   :3.133  
                 3rd Qu.:5.000  
                 Max.   :5.000  
> summary(db[db$user == "regular",])
         user         rating    
 influencer:  0   Min.   :1.00  
 regular   :100   1st Qu.:2.00  
                  Median :3.00  
                  Mean   :3.06  
                  3rd Qu.:4.00  
                  Max.   :5.00  
> 
\end{verbatim}

\subsection{b}
The null hypothesis is that both the influencers and the regular users hold the
same opinions on the removal of likes.

The alternate is that influencers and regular users have differing opinions.
Note that the alternate hypothesis is two sided.

Before running any analysis we shall determine our decision rule.
Since this is a social science experiment we will take a $p < 0.1$ as evidence
against the null hypothesis.

Consider two vectors,
\begin{align*}
    (X_1, X_2, \ldots, X_n) \\
    (Y_1, Y_2, \ldots, Y_m) \\
\end{align*}

Where all $X_i$ are taken from the regular users distribution and $Y_i$ is taken
from the influencers distribution.

We don't know the underlying distribution of each $X$ and $Y$
so we shall approximate them with t-distributions. We have $n + m - 2$ degrees
of freedom. We choose the t-statistic because it's asymptotically most powerful
even for non-normal distributions.

Under the null hypothesis $X$ and $Y$ share the same distribution thus,
\begin{align*}
    0.05 &= \prob(t_{n+m-2} > c)
    &&\text{Where $c$ the value satisfying this equation} \\
    &= \prob\left(\frac{\bar{X} - \bar{Y} - \Delta_0}{S_p\sqrt{1/n + 1/m}} > c\right)
    &&\text{Where $\Delta_0$ is the difference in means} \\
    &= \prob\left(\frac{\bar{X} - \bar{Y}}{S_p\sqrt{1/n + 1/m}} > c\right) \\
    &= \prob\left(\bar{X} - \bar{Y} > c S_p\sqrt{1/n + 1/m}\right)
\end{align*}

Using R we determine that $c=1.653$ for $p=0.1$ double sided.
Since it is double sided (and luckily t distribution is symmetric)
we actually have $c=\pm1.653$.

Now we look at the data and determine the p-value for it.
We work this process backwards to determine the $c$ value to calculate $p$.
\begin{align*}
    c &= \frac{\bar{X} - \bar{Y}}{S_p\sqrt{1/n + 1/m}} \\
    &= \frac{\bar{X} - \bar{Y}}{S_p\sqrt{1/n + 1/m}} \\
    c &= -0.2174 \\
\end{align*}

Using the $t_{n+m-2}$ and R we get $p = 0.4141$.
\begin{verbatim}
    avg <- mean(db[db$user=="regular",]$rating)
    avg <- avg - mean(db[db$user == "influencer",]$rating)
    pt(avg/(var(db$rating)*sqrt(1/100+1/98)), 198-2)
\end{verbatim}

The p-value shows that assuming the null hypothesis is true, that we have a $0.4141$ 
chance of achieving this result or larger.

The p-value on its own does not provide a strong enough conclusion. Instead
consider the previous cutoff we established with $p=0.1$, $c=1.653$.
Since we're in the context of social science we determined a p-value of
$0.1$ was sufficient to consider this data against the null hypothesis.

Using this $c$ we determine a cutoff for $\bar{X} - \bar{Y}$;
$\pm0.5523$. If we 
compare this with our \texttt{avg} which was $-0.07265$. We see that our data
does not meet the cutoffs on either side. Thus we conclude that our data is
consistent with the null hypothesis.

\subsection{c}
The first assumption was that each data point was independent of one another.
Given this is a form of social media it is likely that those unhappy with the
change were more vocal which might have waived previously neutral people.

Since we did not know the distribution our data came from we had assumed that
our data would asymptotically approximate a t-distribution. The underlying data
is not only \textbf{not} continuous but it is also bounded.

Most importantly we are taking measurements of categorical data. Though there is
an order the difference between a score of 1 and 2 is not necessarily equal to
the difference between a score of 4 vs 5.

\section{Exercise 2}
\subsection{a}
The null hypothesis is that the blood type has no impact on tuberculosis 
incidence. We should expect to see evenly distributed tuberculosis incidence
for each blood type.

The alternative hypothesis is that blood type has some kind of effect on
tuberculosis incidence.

\subsection{b}
The data fits a multinomial distribution thus the likelihood function we get is,
\begin{align*}
    n! \prod_{i} \prod_{j} \frac{1}{x_{ij}!} p_{ij}^{x_{ij}}
\end{align*}

\subsection{c}
We will make use of the fact that under the null hypothesis
$p_{ij} = p_{i.} \times p_{.j}$

\begin{align*}
    L(p|x) &= n! \prod_{i} \prod_{j} \frac{1}{x_{ij}!} p_{ij}^{x_{ij}} \\
    &= n! \prod_{i} \prod_{j} \frac{1}{x_{ij}!} (p_{i.} \times p_{.j})^{x_{ij}} \\
    &= n! \left(\prod_i p_{i.}^{\sum_j x_{ij}} \times \prod_j p_{.j}^{\sum_i x_{ij}}\right)
    \prod_{i} \prod_{j} \frac{1}{x_{ij}!} \\
    &= n! \left(\prod_i p_{i.}^{x_{i.}} \times \prod_j p_{.j}^{x_{.j}}\right)
    \prod_{i,j} \frac{1}{x_{ij}!} 
\end{align*}

We note a secondary contraint on both $p_{i.}$ and $p_{.j}$.
\begin{align*}
    \sum_i^n p_{i.} &= 1 \\
    \sum_j^m p_{.j} &= 1 \\
\end{align*}

Thus,
\begin{align*}
    p_{n.} &= 1 - \sum_i^{n-1} p_{i.} \\
    p_{.m} &= 1 - \sum_j^{m-1} p_{.j} \\
\end{align*}

We add this secondary contraint,
\begin{align*}
    &= n! \left(\prod_i^n p_{i.}^{x_{i.}} \times \prod_j^m p_{.j}^{x_{.j}}\right)
    \prod_{i,j} \frac{1}{x_{ij}!}  \\
    &= n! \left(\prod_i^{n-1} p_{i.}^{x_{i.}} \times \prod_j^{m-1} p_{.j}^{x_{.j}}\right)
    \left(1 - \sum_i^{n-1} p_{i.}\right)^{x_{n.}}\left(1 - \sum_j^{m-1} p_{.j}\right)^{x_{.m}}
    \prod_{i,j} \frac{1}{x_{ij}!} 
\end{align*}

We take the log of this,
\begin{align*}
    L(p|x) &= n! \left(\prod_i^{n-1} p_{i.}^{x_{i.}} \times \prod_j^{m-1} p_{.j}^{x_{.j}}\right)
    \left(1 - \sum_i^{n-1} p_{i.}\right)^{x_{n.}}\left(1 - \sum_j^{m-1} p_{.j}\right)^{x_{.m}}
    \prod_{i,j} \frac{1}{x_{ij}!} \\
    \ln(L(p|x)) &= \ln(n!) +  \ln\left(\prod_i^{n-1} p_{i.}^{x_{i.}} \times \prod_j^{m-1} p_{.j}^{x_{.j}}\right)
    + {x_{n.}} \ln\left(1 - \sum_i^{n-1} p_{i.}\right) + {x_{.m}}\ln\left(1 - \sum_j^{m-1} p_{.j}\right)
    - \sum_{i,j} \ln(x_{ij}!) \\
    &= \ln(n!) + \sum_i^{n-1} {x_{i.}}\ln(p_{i.}) + \sum_j^{m-1} {x_{.j}}\ln(p_{.j})
    + {x_{n.}} \ln\left(1 - \sum_i^{n-1} p_{i.}\right) + {x_{.m}}\ln\left(1 - \sum_j^{m-1} p_{.j}\right) \\
    &- \sum_{i,j} \ln(x_{ij}!) \\
\end{align*}

We partially differentiate this with respect to $p_{k.}$.
\begin{align*}
    \ln(L(p|x)) &= \ln(n!) + \sum_i^{n-1} {x_{i.}}\ln(p_{i.}) + \sum_j^{m-1} {x_{.j}}\ln(p_{.j})
    + {x_{n.}} \ln\left(1 - \sum_i^{n-1} p_{i.}\right) + {x_{.m}}\ln\left(1 - \sum_j^{m-1} p_{.j}\right) \\
    &- \sum_{i,j} \ln(x_{ij}!) \\
    \frac{\partial\ln(L(p|x))}{\partial p_{k.}}
    &=
    \frac{x_{k.}}{p_{k.}} - \frac{x_{n.}}{1 - \sum_i^{n-1} p_{i.}} \\
    &=
    \frac{x_{k.}}{p_{k.}} - \frac{x_{n.}}{p_{n.}} \\
\end{align*}

\subsection{d}
The null hypothesis allows that $p_{ij} = p_{i.} \times p_{.j}$ since blood type
tuberculosis should be independent.
We also found that $\hat{p}_{i.} = x_{i.}/n$ thus,
\begin{align*}
    \hat{p}_{ij} &= x_{i.}/n \times x_{.j}/n \\
    &= \frac{x_{i.}\times x_{.j}}{n^2} \\
\end{align*}

Using Matlab to do the grunt work we then determine each cell of the matrix.
\begin{verbatim}
A = [7 5 3 13; 27 32 8 18; 55 50 7 24]
B = zeros(3,4)

for row = 1:3
    for col = 1:4
        B(row,col)=sum(A(row,:))*sum(A(:,col))/249^2
    end
end

B
\end{verbatim}

\begin{align*}
    \begin{matrix}
        0.0401929 & 0.0392897 & 0.0081289 & 0.0248383 \\
        0.1220142 & 0.1192723 & 0.0246770 & 0.0754020 \\
        0.1952227 & 0.1908356 & 0.0394832 & 0.1206432 \\
    \end{matrix}
\end{align*}

To get the expected count from this we take $p_{ij} \times n$. Which is taken
from the binomial (and by extension the multinomial) distribution.
\begin{align*}
    \begin{matrix}
        10.0080 &  9.7831 &  2.0241 &  6.1847 \\
        30.3815 & 29.6988 &  6.1446 & 18.7751 \\
        48.6104 & 47.5181 &  9.8313 & 30.0402 \\
    \end{matrix}
\end{align*}

\subsection{e}
Without proof the most likely estimate for each $\hat{p}_{ij}$, under the
alternative hypothesis, is, $x_{ij}/n$.

\subsection{f}
Using our previous results we can now evaluate the generalised likelihood ratio
test statistic.

For the null hypothesis we have
$\hat{p}_{ij} = \frac{x_{i.}\times x_{.j}}{n^2}$.
So our likelihood for the null hypothesis is,
\begin{align*}
    &n! \prod_{i} \prod_{j} \frac{1}{x_{ij}!} p_{ij}^{x_{ij}} \\
    &= n! \prod_{i} \prod_{j}
    \frac{1}{x_{ij}!}
    \left(\frac{x_{i.}\times x_{.j}}{n^2}\right)^{x_{ij}} \\
\end{align*}

For the alternate hypothesis we get,
\begin{align*}
    &n! \prod_{i} \prod_{j} \frac{1}{x_{ij}!} p_{ij}^{x_{ij}} \\
    &= n! \prod_{i} \prod_{j}
    \frac{1}{x_{ij}!}
    \left(x_{ij}/n\right)^{x_{ij}} \\
\end{align*}

Using both of these we get,
\begin{align*}
    \Lambda &= \frac{\sup_{H_0} L(p|x)}{\sup_{H_1} L(p|x)} \\
    &= \frac{n! \prod_{i} \prod_{j}
    \frac{1}{x_{ij}!}
    \left(\frac{x_{i.}\times x_{.j}}{n^2}\right)^{x_{ij}}}
    {n! \prod_{i} \prod_{j}
    \frac{1}{x_{ij}!}
    \left(x_{ij}/n\right)^{x_{ij}}} \\
    &= \prod_{i} \prod_{j}
    \frac{\left(\frac{x_{i.}\times x_{.j}}{n^2}\right)^{x_{ij}}}
    {\left(x_{ij}/n\right)^{x_{ij}}} \\
    &= \prod_{i} \prod_{j}
    \left(\frac{\frac{x_{i.}\times x_{.j}}{n^2}}
    {x_{ij}/n}\right)^{x_{ij}} \\
    &= \prod_{i} \prod_{j}
    \left(\frac{x_{i.}\times x_{.j}}
    {x_{ij}n}\right)^{x_{ij}}
\end{align*}

We put this into R to get,
\begin{verbatim}
tb <-  matrix(c(7,5,3,13,27,32,8,18,55,50,7,24),nrow=3,byrow=TRUE)
res <- {}
for (row in 1:3){
    for (col in 1:4){
        res <- c(res, (sum(tb[row,])*sum(tb[,col])/(tb[row,col]*sum(tb)))^tb[row,col])
    }
}
lambda <- prod(res)
-2*log(lambda)
\end{verbatim}

The result we get for $\Lambda = 0.0008200$ and $-2\log\Lambda = 14.21$.

\subsection{g}
We now use R again to compute the Pearson's $\chi^2$ statistic.
\begin{verbatim}
tbnull <- matrix(c(10.0080, 9.7831, 2.0241, 6.1847, 30.3815,
29.6988, 6.1446, 18.7751, 48.6104, 47.5181, 9.8313, 30.0402),
nrow=3, byrow=TRUE)
sum((tb - tbnull)^2/tbnull)
\end{verbatim}

The code gives us $\chi^2 = 15.37$. From observation alone it appears that
Pearson's $\chi^2$ is a good approximation for our $-2\log\Lambda$.

\subsection{h}
Using our Pearson's $\chi^2$ and R we can calculate our p-value.
The degrees of freedom is based on the null hypothesis which gives us $5$ since
we can only make choices about each row or column and the final row/column must
allow us to sum to 1.
\begin{verbatim}
pchisq(sum((tb-tbnull)^2/tbnull),5,lower.tail=FALSE)
\end{verbatim}

Which gives us a p-value of $0.008894$.

The p-value does not mean anything without a proper decision cutoff
(that should have been determined prior the calculations). Since we're
dealing with population health which may inform the treatment of humans a
p-value of 0.05 seems like a reasonable choice.

Since the p-value we've found is far lower we decide that our data is evidence
against the null hypothesis. There exists a relationship between
blood type and tuberculosis.

\subsection{i}
We repeat the process but remove the advanced severity in the
hope that the $\chi^2$ may become more accurate.

We repeat the R code but with an alternative matrix.
\begin{verbatim}
xtb <- matrix(c(34,27,11,31,55,50,7,24),ncol=4,byrow=TRUE)
xtbnull <- {}
for (row in 1:2){
    for (col in 1:4){
        xtbnull <- c(xtbnull, sum(xtb[row,])*sum(xtb[,col])/sum(xtb))
    }
}
xtbnull <- matrix(xtbnull, nrow=2, ncol=3, byrow=TRUE)
sum((xtb - xtbnull)^2/xtbnull)
\end{verbatim}

This gives us a Pearson's $\chi^2$ of 9.224. The degrees of freedom are 4
since we only have one option for the rows and three options for the columns.

The p-value we get from this is, 0.05573. Since our decision cutoff was 0.05
We find that the study has radically changed its decision after
collecting two rows. Our conclusion is now, we now find the data 
is consistent with the null hypothesis.

\section{Exercise 3}
\subsection{a}
Before doing any data crunching it is important we define an appropriate
decision. Since we're taking from a group of 100 students and we're doing a
study on human behaviour we shall choose a p-value less than 0.1 means our data
is evidence against the null hypothesis.

We define the null hypothesis to be there is no difference in symbol frequency
for incorrect and correct solutions. The alternative hypothesis is that there is
some sort of difference between the frequency of symbols.

We model our data in a multinomial distribution.
The parameters we will look at are $p_{\mu i}$ where we define the probability
of the symbol for the incorrect solutions and $p_{\bar{X} c}$ for the $\bar{X}$
symbol for the correct solution and so on.

Our model and hypotheses mirror the types of question posed in the previous
question. We will make use of the results we got from there.

Under the null model expected result is,
\begin{align*}
    \hat{p}_{ij} &= \frac{x_{i.} \times x_{.j}}{n^2} \\
\end{align*}

The observed is,
\begin{align*}
    \hat{p}_{ij} &= \frac{x_{ij}}{n^2} \\
\end{align*}

We now use R on this data to get our p-value.
\begin{verbatim}
sym <- c(298/25, 165/15, 146/25, 264/15, 119/25, 16/15, 106/25, 328/15,
29/25, 203/15, 133/25, 165/15, 29/25, 30/15)
sym <- matrix(sym, ncol=2, byrow=TRUE)
symnull <- matrix(0, dim(sym)[1], dim(sym)[2])
for (row in 1:7){
    for (col in 1:2){
        symnull[row,col] <- sum(sym[row,])*sum(sym[,col])/sum(sym)
    }
}

sum((sym - symnull)^2/symnull)
\end{verbatim}

This gives us a Pearson $\chi^2$ of 18.65745. The degrees of freedom for our
statistic will be $6 \times 1 = 6$ since the contraint is $(rows-1)(cols-1)$.
The corresponding p-value is, $4.783 \times 10^{-3}$ which provides strong
evidence against the null hypothesis.

From this we can conclude that there exists a difference between the frequency
of certain symbols in incorrect student answers and correct student answers.

\subsection{b}
Since we're taking only 8 human responses we should expect a lot of variance.
Because of this we shall determine a p-value of 0.1 or less as our cutoff.

Our null hypothesis is that there is no difference in the inherent distribution
where these 8 incorrect solutions arise from. Our alternative hypothesis is that
there is a difference between each of the 8 incorrect answers.

We repeat the process from the previous question.
\begin{verbatim}
sols <- matrix(c(
        12,11,12,13,14,11,13,12,
        06,06,07,06,05,08,07,05,
        05,05,05,04,05,03,04,04,
        04,04,06,02,04,02,06,06,
        02,00,00,00,02,00,02,02,
        05,06,04,05,04,04,06,05,
        02,00,02,02,00,00,00,02),
        ncol=8, byrow=TRUE)
solsnull <- matrix(0,7,8)
for (row in 1:7){
    for (col in 1:8){
        solsnull[row,col] <- sum(sols[row,])*sum(sols[,col])/sum(sols)
    }
}
chi <- sum((sols - solsnull)^2/solsnull)
\end{verbatim}

We determine we have $6 \times 7$ degrees of freedom since we have 7 rows but a 
constraint among them and 8 columns with a constraint between them.

With this information we compute our p-value is 0.9940.
Based on the decision cutoff we declared earlier we accept this data as evidence
against the null hypothesis. There is evidence that the 8 incorrect solutions
have different frequencies.

\subsection{c}
The p-value is extremely high this suggests that the bad answers are all copying
each other.

\subsection{d}
Perhaps studying more data from each paper. The words used in the 8 worst
solutions may also be equal.


\section{Exercise 4}
\subsection{a}
We take the product of all samples which gives us,
\begin{align*}
    L(\mu, \alpha_j, \beta_k, \delta_{jk} | Y)
    &=
    \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^{12}
    e^{-\frac{1}{2\sigma^2}\sum_{j=1}^2\sum_{k=1}^2\sum_{i=1}^3
    (Y_{jki} - \mu - \alpha_j - \beta_k - \delta_{jki})^2} \\
    \ln L(\mu, \alpha_j, \beta_k, \delta_{jk} | Y)
    &=
    12 \ln\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)
    -\frac{1}{2\sigma^2}\sum_{j=1}^2\sum_{k=1}^2\sum_{i=1}^3
    (Y_{jki} - \mu - \alpha_j - \beta_k - \delta_{jki})^2 \\
    \frac{\partial\ln L(\mu, \alpha_j, \beta_k, \delta_{jk} | Y)}{\partial\mu}
    = 0 &=
    -\frac{1}{2\sigma^2}\sum_{j=1}^2\sum_{k=1}^2\sum_{i=1}^3
    -2(Y_{jki} - \mu - \alpha_j - \beta_k - \delta_{jki}) \\
    &=
    \frac{1}{\sigma^2}\sum_{j=1}^2\sum_{k=1}^2\sum_{i=1}^3
    (Y_{jki} - \mu - \alpha_j - \beta_k - \delta_{jki}) \\
    &\text{Under sum constraint we have} \\
    &=
    \frac{-12\mu}{\sigma^2} + \frac{1}{\sigma^2}\sum_{j=1}^2\sum_{k=1}^2\sum_{i=1}^3
    Y_{jki} \\
    \frac{12\mu}{\sigma^2} &=
    \frac{1}{\sigma^2}\sum_{j=1}^2\sum_{k=1}^2\sum_{i=1}^3
    Y_{jki} \\
    \mu &=
    \frac{\sum_{j=1}^2\sum_{k=1}^2\sum_{i=1}^3 Y_{jki}}{12} \\
    \mu &= \bar{Y}_{\bullet\bullet\bullet} \\
\end{align*}

We repeat this process but take the partial of $\alpha_1$.
\begin{align*}
    \ln L(\mu, \alpha_j, \beta_k, \delta_{jk} | Y)
    &=
    12 \ln\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)
    -\frac{1}{2\sigma^2}\sum_{j=1}^2\sum_{k=1}^2\sum_{i=1}^3
    (Y_{jki} - \mu - \alpha_j - \beta_k - \delta_{jki})^2 \\
    &=
    12 \ln\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)
    -\frac{1}{2\sigma^2}
    \sum_{k=1}^2\sum_{i=1}^3
    (Y_{1ki} - \mu - \alpha_1 - \beta_k - \delta_{jki})^2 \\
    &-\frac{1}{2\sigma^2}\sum_{k=1}^2\sum_{i=1}^3
    (Y_{2ki} - \mu - \alpha_2 - \beta_k - \delta_{jki})^2 \\
    \frac{\partial\ln L(\mu, \alpha_j, \beta_k, \delta_{jk} | Y)}{\partial \alpha_1}
    = 0 &=
    \frac{1}{\sigma^2}
    \sum_{k=1}^2\sum_{i=1}^3
    (Y_{1ki} - \mu - \alpha_1 - \beta_k - \delta_{jki}) \\
    &\text{Under sum constraint we have} \\
    &=
    \frac{1}{\sigma^2}
    \sum_{k=1}^2\sum_{i=1}^3
    Y_{1ki}
    - \sum_{k=1}^2\sum_{i=1}^3
    \frac{\mu}{\sigma^2}
    - \sum_{k=1}^2\sum_{i=1}^3
    \frac{\alpha_1}{\sigma^2}
    - \sum_{k=1}^2\sum_{i=1}^3
    \frac{\beta_k}{\sigma^2}
    - \sum_{k=1}^2\sum_{i=1}^3
    \frac{\delta_{jki}}{\sigma^2} \\
    &=
    \frac{\sum_{k=1}^2\sum_{i=1}^3Y_{1ki}}{\sigma^2}
    - \frac{6\mu}{\sigma^2}
    - \frac{6\alpha_1}{\sigma^2} \\
    \frac{6\alpha_1}{\sigma^2}
    &= \frac{\sum_{k=1}^2\sum_{i=1}^3Y_{1ki}}{\sigma^2} - \frac{6\mu}{\sigma^2} \\
    6\alpha_1 &= \sum_{k=1}^2\sum_{i=1}^3Y_{1ki} - 6\mu \\
    \alpha_1 &= \frac{\sum_{k=1}^2\sum_{i=1}^3Y_{1ki}}{6} - \mu \\
    \alpha_1 &= \bar{Y}_{1\bullet\bullet} - \bar{Y}_{\bullet\bullet\bullet} \\
\end{align*}

The same can be done for $j=2$.

We repeat this process but take the partial of $\beta_1$.
\begin{align*}
    \ln L(\mu, \alpha_j, \beta_k, \delta_{jk} | Y)
    &=
    12 \ln\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)
    -\frac{1}{2\sigma^2}\sum_{j=1}^2\sum_{k=1}^2\sum_{i=1}^3
    (Y_{jki} - \mu - \alpha_j - \beta_k - \delta_{jki})^2 \\
    &=
    12 \ln\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)
    -\frac{1}{2\sigma^2}\sum_{j=1}^2\sum_{i=1}^3
    (Y_{j1i} - \mu - \alpha_j - \beta_1 - \delta_{j1i})^2 \\
    &-\frac{1}{2\sigma^2}\sum_{j=1}^2\sum_{i=1}^3
    (Y_{j2i} - \mu - \alpha_j - \beta_2 - \delta_{j2i})^2 \\
    \frac{\partial\ln L(\mu, \alpha_j, \beta_k, \delta_{jk} | Y)}{\partial \beta_1}
    = 0 &=
    \frac{1}{\sigma^2}\sum_{j=1}^2\sum_{i=1}^3
    (Y_{j1i} - \mu - \alpha_j - \beta_1 - \delta_{j1i}) \\
    0 &=
    \sum_{j=1}^2\sum_{i=1}^3
    (Y_{j1i} - \mu - \alpha_j - \beta_1 - \delta_{j1i}) \\
    &= \sum_{j=1}^2\sum_{i=1}^3Y_{j1i} - 6\mu - 6\beta_1 \\
    \beta_1 &= \frac{\sum_{j=1}^2\sum_{i=1}^3Y_{j1i}}{6} - \mu \\
    &= \bar{Y}_{\bullet 1 \bullet} - \bar{Y}_{\bullet\bullet\bullet} \\
\end{align*}

The same can be done for $k=2$.

We repeat this process but take the partial of $\delta_{JK}$. Where $J$ and $K$
and the anti-$J$ and anti-$K$ are $\bar{J}$ and $\bar{K}$ respectively.
\begin{align*}
    \ln L(\mu, \alpha_j, \beta_k, \delta_{jk} | Y)
    &=
    12 \ln\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)
    -\frac{1}{2\sigma^2}\sum_{j=1}^2\sum_{k=1}^2\sum_{i=1}^3
    (Y_{jki} - \mu - \alpha_j - \beta_k - \delta_{jk})^2 \\
    &=
    12 \ln\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)
    -\frac{1}{2\sigma^2}\sum_{i=1}^3
    (Y_{JKi} - \mu - \alpha_J - \beta_K - \delta_{JK})^2 \\
    &-\frac{1}{2\sigma^2}\sum_{i=1}^3
    (Y_{J\bar{K}i} - \mu - \alpha_J - \beta_{\bar{K}} - \delta_{J\bar{K}})^2 \\
    &-\frac{1}{2\sigma^2}\sum_{i=1}^3
    (Y_{\bar{J}Ki} - \mu - \alpha_{\bar{J}} - \beta_K - \delta_{\bar{J}K})^2 \\
    &-\frac{1}{2\sigma^2}\sum_{i=1}^3
    (Y_{\bar{J}\bar{K}i} - \mu - \alpha_{\bar{J}} - \beta_{\bar{K}} - \delta_{\bar{J}\bar{K}})^2 \\
    \frac{\partial\ln L(\mu, \alpha_j, \beta_k, \delta_{jk} | Y)}{\partial \delta_{JK}}
    = 0 &=
    \frac{1}{\sigma^2}\sum_{i=1}^3
    (Y_{JKi} - \mu - \alpha_J - \beta_K - \delta_{JKi}) \\
    0 &= \sum_{i=1}^3(Y_{JKi} - \mu - \alpha_J - \beta_K - \delta_{JK}) \\
    0 &= \sum_{i=1}^3Y_{JKi} - 3\mu - 3\alpha_J - 3\beta_K - 3\delta_{JK} \\
    \delta_{JK} &= \frac{\sum_{i=1}^3Y_{JKi}}{3} - \mu - \alpha_J - \beta_K \\
    \delta_{JK} &= \bar{Y}_{JK\bullet} - \bar{Y}_{J\bullet\bullet} - \bar{Y}_{\bullet K \bullet} + \bar{Y}_{\bullet\bullet\bullet} \\
\end{align*}

\subsection{b}
We first note that
\begin{align*}
    Y_{jki} - \bar{Y}_{\bullet\bullet\bullet}
    &=
    (Y_{jki} - \bar{Y}_{jk\bullet})
    + (Y_{j\bullet\bullet} - \bar{Y}_{\bullet\bullet\bullet})
    + (Y_{\bullet k \bullet} - \bar{Y}_{\bullet\bullet\bullet})
    + (Y_{jk\bullet} - \bar{Y}_{j\bullet\bullet} - \bar{Y}_{\bullet k \bullet} + \bar{Y}_{\bullet\bullet\bullet}) \\
\end{align*}

Using this we turn to the $\text{SS}_{\text{total}}$.
\begin{align*}
    \sum_{jki}(Y_{jki} - \bar{Y}_{\bullet\bullet\bullet})^2
    &=
    \sum_{jki}((Y_{jki} - \bar{Y}_{jk\bullet})
    + (Y_{j\bullet\bullet} - \bar{Y}_{\bullet\bullet\bullet})
    + (Y_{\bullet k \bullet} - \bar{Y}_{\bullet\bullet\bullet})
    + (Y_{jk\bullet} - \bar{Y}_{j\bullet\bullet} - \bar{Y}_{\bullet k \bullet} + \bar{Y}_{\bullet\bullet\bullet}))^2 \\
    &=
    (Y_{jki} - \bar{Y}_{jk\bullet})
        ((Y_{jki} - \bar{Y}_{jk\bullet})
        + (Y_{j\bullet\bullet} - \bar{Y}_{\bullet\bullet\bullet})
        + (Y_{\bullet k \bullet} - \bar{Y}_{\bullet\bullet\bullet})
        + (Y_{jk\bullet} - \bar{Y}_{j\bullet\bullet} - \bar{Y}_{\bullet k \bullet} + \bar{Y}_{\bullet\bullet\bullet})) \\
    &+ (Y_{j\bullet\bullet} - \bar{Y}_{\bullet\bullet\bullet})
        ((Y_{jki} - \bar{Y}_{jk\bullet})
        + (Y_{j\bullet\bullet} - \bar{Y}_{\bullet\bullet\bullet})
        + (Y_{\bullet k \bullet} - \bar{Y}_{\bullet\bullet\bullet})
        + (Y_{jk\bullet} - \bar{Y}_{j\bullet\bullet} - \bar{Y}_{\bullet k \bullet} + \bar{Y}_{\bullet\bullet\bullet})) \\
    &+ (Y_{\bullet k \bullet} - \bar{Y}_{\bullet\bullet\bullet})
        ((Y_{jki} - \bar{Y}_{jk\bullet})
        + (Y_{j\bullet\bullet} - \bar{Y}_{\bullet\bullet\bullet})
        + (Y_{\bullet k \bullet} - \bar{Y}_{\bullet\bullet\bullet})
        + (Y_{jk\bullet} - \bar{Y}_{j\bullet\bullet} - \bar{Y}_{\bullet k \bullet} + \bar{Y}_{\bullet\bullet\bullet})) \\
    &+ (Y_{jk\bullet} - \bar{Y}_{j\bullet\bullet} - \bar{Y}_{\bullet k \bullet} + \bar{Y}_{\bullet\bullet\bullet}))
        ((Y_{jki} - \bar{Y}_{jk\bullet})
        + (Y_{j\bullet\bullet} - \bar{Y}_{\bullet\bullet\bullet})
        + (Y_{\bullet k \bullet} - \bar{Y}_{\bullet\bullet\bullet})
        + (Y_{jk\bullet} - \bar{Y}_{j\bullet\bullet} - \bar{Y}_{\bullet k \bullet} + \bar{Y}_{\bullet\bullet\bullet})) \\
\end{align*}

We note that $\sum (Y_i - \bar{Y}_{\bullet}) = 0$. Due to how the mean works.
\begin{align*}
    \sum_{jki}(Y_{jki} - \bar{Y}_{\bullet\bullet\bullet})^2
    &=
    \sum_{jki}(Y_{jki} - \bar{Y}_{jk\bullet})^2 \\
    &+ \sum_{jki}(Y_{j\bullet\bullet} - \bar{Y}_{\bullet\bullet\bullet})^2 \\
    &+ \sum_{jki}(Y_{\bullet k \bullet} - \bar{Y}_{\bullet\bullet\bullet})^2 \\
    &+ \sum_{jki}(Y_{jk\bullet} - \bar{Y}_{j\bullet\bullet} - \bar{Y}_{\bullet k \bullet} + \bar{Y}_{\bullet\bullet\bullet}))^2 \\
    SS_{total} &= SS_{A} + SS_{B} + SS_{AB} + S_{Residual} \\
\end{align*}

\subsection{c}
$SS_{residual}$ is the same as taking a sample variance. Sample variances of
normal distributions are in fact distributed as
$\sigma^2 \chi^2_{df_{residual}}$.

Since there are $JK$ datapoints which have been taken 3 times we have $3-1 = 2$
degrees of freedom for each.

\subsection{d}
We must first recall that the variance around the sample mean is independent of
the sample mean itself. Likewise we have,
\begin{align*}
    SS_{residual} &= \sum_{jki} (Y_{jki} - \bar{Y}_{jk\bullet})^2 \\
\end{align*}

is independent $\forall Y_{jk\bullet}$. Since $SS_{AB}$ is a function of
numerous $Y_{jk\bullet}$ we know it is independent of $SS_{residual}$.

\subsection{e}
The ratio of two independent $\chi^2$ distributions produces a
$\mathcal{F}$. Hence,
\begin{align*}
    F &= \frac{SS_{AB}/df_{AB}}{SS_{residual}/df_{residual}} \\
    &\sim \mathcal{F} \\
\end{align*}

\subsection{f}
\begin{center}
    \begin{tabular}{c c c c c c}
        Source & df & SS & MS & F & P \\
        \hline
        VitaminB12 & 1 & 218700 & 218700 & 60.33 & 0.08151 \\
        Antibiotics & 1 & 19200 & 19200 & 5.297 & 0.2610 \\
        VitaminB12:Antibiotics & 1 & 172800 & 172800 & 47.67 & 0.09157 \\
        Residuals & 8 & 29000 & 3625 &  &  \\
        \hline
        Total & 11 & 439700 &  &  &  \\
    \end{tabular}
\end{center}

The data finds that antibiotics on its own does not contribute
to the weight gain of pigs. In comparison we find marginal strength
in the relationship between VitaminB12 and strength and we also find
that the addition of Antibiotics contributes to the effect of
VitaminB12.

\end{document}
