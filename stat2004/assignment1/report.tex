\documentclass{article}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage{verbatim}

\linespread{1.3}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\setcounter{secnumdepth}{0}
\setcounter{MaxMatrixCols}{20}
\renewcommand{\arraystretch}{1.5}

\newcommand{\ts}{\textsuperscript}
\newcommand{\diff}{\mathop{}\!\mathrm{d}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\var}{\text{Var}}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\p{\lparan}{\rparan}

\title{STAT2004 Assignment 1}
\author{Joshua Hwang (44302650)}

\begin{document}
\maketitle

\section{Exercise 1}
\subsection{a}
Contradiction with B is the empty set. Thus probability is 0 but our definition
states that P(B) > 0.
A sample space must be non-empty and hold all the possible outcomes of the
experiment. We then create a new experiment where B must always happen, based
on the previous experiment where Omega happens.

\subsection{b}
We prove three properties of event sets,
It's non-empty which is true since A in F A must be Omega once. From this
we conclude that B in G.

We must prove the complement exists (complement exists w.r.t. B). We do this
by creating a new complement which satisfies the fact they must be disjoint
but union to give the whole set B.

We must prove the union of all elements in G must also be in G. We do this
by doing set algebra on the elements to get the intersection with B on the
outside. Since the complement of all As are also in the event set then
our unions are also.

\subsection{c}
Three properties must be satisfied for it to be a probability measure.
Must be >= 0 for all inputs. P(\Omega) = 1. P(Union of As) = sum of
P(A).
The first property is simple since P(X) is always non-negative and division
of two non-negatives is always non-negative.
The final part is simple since we make use of the fact that Q(A U B U C)
= P(A U B U C)/P(B) and A, B, C are all dijoint and we're using the standard
probability measure so we're good.

\subsection{d}
Thus we satisfy all required properties for a probability space.
We can think of this probability space as the same test as omega but where
we discount all tests where B did not happen. Or we ensure that B happens
in all our tests.

\section{Exercise 2}
\subsection{a}
The chance that Ricky is the killer given that there are $n+1$ equally likely
suspects is $\frac{1}{n+1}$.
\begin{align*}
    \prob \left(E \middle| R^c\right)
    &= \frac{\prob \left(E \cap R^c\right)}{\prob \left(R^c\right)} \\
    &= \frac{p \times \frac{n}{n+1}}{\frac{n}{n+1}} \\
    &= p \\
\end{align*}

This answer makes sense as the killer must be someone from the general public.
A person in the general public has a $p$ chance of having this DNA profile.

\subsection{b}
\begin{align*}
    \prob(R|E)
    &= \frac{\prob(E|R) \prob(R)}{\prob(E|R)\prob(R)
        + \prob\left(E\middle|R^c\right)\prob\left(R^c\right)} \\
    &= \frac{1 \times \frac{1}{n+1}}{1 \times \frac{1}{n+1} + p \times \frac{n}{n+1}} \\
    &= \frac{1}{1 + pn} \\
\end{align*}

\subsection{c}
Consider our previous example with some reasonable parameters. We'll assume
that our test DNA profile is very rare meaning only $p=0.05$ of the population
has it. Let's also take the number of possible suspects to be 20 people.

Replacing $R$ with $G$ we get $P(E|G) = 1$. But $P(G|E) = \frac{1}{2}$.
There's only a $50\%$ chance that he committed the crime.

\section{Exercise 3}
\subsection{a}
\begin{align*}
    \expect \bar{X}_n
    &= \expect \frac{1}{n} \sum_{i=1}^n X_i \\
    &= \frac{1}{n} \expect \sum_{i=1}^n X_i \\
    &= \frac{1}{n} \sum_{i=1}^n \expect X_i & \text{This is okay since iid} \\
    &= \frac{1}{n} \sum_{i=1}^n \mu \\
    &= \mu \\
\end{align*}

\subsection{b NOT DONE YET}
% ZZZ I don't know how to do this

\subsection{c NOT DONE YET}
% ZZZ Research biased variance quickly

\section{Exercise 4}
We consider a new random variable $D = X - Y$. We can create samples of this
random variable by taking our $X_i$ and $Y_i$; $D_i = X_i - Y_i$.

We now try and put $D$ into the form that satisfies the Central Limit Theorem.
From there we expand to reveal that this is $D_n$. Since $W$ in this form
converges to the standard normal, we conclude $D_n$ converges in distribution
to a standard normal distribution.

To put $D$ in standard normal form, we need to find the variance and expectation.
\begin{align*}
    \expect [D] &= \expect [X - Y] \\
    &= \expect [X] - \expect [Y] & \text{Since X and Y are independent} \\
    &= \expect [X] - \expect [Y] \\
    &= \mu - \mu \\
    &= 0 \\
\end{align*}

\begin{align*}
    \var (D) &= \var (X - Y) \\
    &= \var (X)+ \var(-Y) & \text{Since they are independent} \\
    &= \var (X)+ {(-1)}^2 \var(Y) \\
    &= \sigma^2 + \sigma^2 \\
    &= 2 \sigma^2 \\
\end{align*}

From here we generate the form that satifies the CLT\@.

\begin{align*}
    \sqrt{n} \frac{\bar{D} - 0}{\sqrt{2} \sigma}
    &= \sqrt{n} \frac{\frac{1}{n}\sum D_i}{\sqrt{2} \sigma} \\
    &= \frac{\sum D_i}{\sqrt{2n\sigma^2}} \\
    &= \frac{\sum X_i - \sum Y_i}{\sqrt{2n\sigma^2}} \\
\end{align*}

Since $\sqrt{n} \frac{\bar{W} - 0}{\sqrt{2} \sigma}$ will approach the standard
normal as $n \to \infty$. The bottom equation will as well.

\end{document}
