Probability theory is concerned with mathematical
models for uncertainty.

Th starting point is the idea of a 'random experiment'

The idea of a random experiment is captured by the
mathematical idea of a *probability space*. It is a core concept
in probability theory and statistics.

Thus to represent a probability space we need a triplet.
(Omega, F, P)
A probability space comprises:
* (Omega) a set of all possible outcomes of the random experiment
* (F) a collection of all possible combinations of events that may occur (the die rolls a prime) (all subsets) (the power set of Omega)
* (P) a function that tells us how likely each even is

The powerset is trivially the event set. It is not equal to however, there
can be event sets smaller than the powerset. (Take the second example on page 02)

Definition of Omega------------------------------------------------
Omega is a non-empty set called the sample space,
F is an event set on Omega
P is a probability of F.

Definition of F------------------------------------------------------
F is nonempty
If A is in F then the complement is also in F. (Complement with respect to Omega)
The union of some unique subsets of F is an element of F.

The power set of Omega is an event set.

Claims-----------------------------------------------------------------
The subsets of an event set are closed under union.
Proof: Consider duplicates A and B, they will cancel to a single element. A union B will give A.
Repeating this sends you to the definition stated eariler. (see page 02-04)

Omega is an element of the event set but also the empty set is an element of the event set.
Proof: Since F is non-empty there exists A in F. We also now the complement of A exists in F.
The union of A and complement of A gives F. We may do this because of the previous proof.

The intersection of subsets is also a subset.
[figure out yourself]

The abritrary set of intersections is also an element of F


Definition of P---------------------------------------------------------
(aka probability measure)
P can be seen as a function that takes an event set and maps it to a real number.
The obvious limits of the function must apply,
P >= 0 for all F
P(Omega) = 1
P(Union of As) = sum of each P(A) (must be unions of disjoint) (is infinite sum/union)

Claims-----------------------------------------------------------------
P(empty set) = 0
Proof: Omega = Omega union empty set.
We convert union into sum and conclude P(empty set) = 0
(I have given up trying to write these down. See 02-07)

P is finitely additive:
If we have A1, A2 ... An which are all dijoint then
the union to sum definition holds for finite n sets.

Claims-----------------------------------------------------------------
P(A) + P(complement A) = 1
Proof: Union of A and complement of A give Omega. Using this union
and definition of P(Omega) we can prove this. (see page 02-04)

If A is a strict subset of B then P(A) <= P(B)
Proof: replaces B as A union (B interset complement A) (see page 02-08)

[Stopped at 27 July Lecture 3]

Definition of random variable------------------------------------------------
A random variable is defined as a mapping from Omega to the reals.
With an extra requirement that the sets {w in Omega : X(w) <= x} for some x are
events in F.

The idea of using an element from Omega seems confusing. I think it's a way
to decouple the idea of events with a score.

A random set of students turn up to class on particular day (Omega)
X is the number of students who attend (Omega to Integer which is a subset of reals)
Y is the average grade of the students who attended (Omega to Reals)

Clearly many random variables can be produced from Omega.

This enitre course assumes the sets {w in Omega : X(w) <= x} for some x are
events in F always holds.

Example:
Let Omega = {(h,w) : h, w > 0} be a sample space for the height and weight of
a random individual. We define a random variable X which is that persons BMI.
X(h,w) = w/h^2

We can specify the probability distribution of random variable X if we can
specify the probability of each event involving X.
As a matter of fact we can create a function that perfectly encapsulates this
distribution F_x (x) = P(X \leq x) for all x in the reals.

We've seen this function before and it's called the cumulative distribution
function. This function takes values in [0,1] is right-continuous and is
non-decreasing.

We say a random variable is discrete or has a discrete distribution if X takes
values in a set (at most countably infinite) with P(X=x_i) > 0. In this case
we define a probability mass function (PMF) by f(x) = P(X=x) (different to
what was discussed before).

A continuous random variable has a distribution defined by
P(X \in A) = \int_A f(x) dx for the event set A. This function is called
the probability density function. The function is defined on the reals
(or something that can map to it)
and takes values in [0, \infty)

Definitions of Independence---------------------------------------------------
Two events are independent if P(F intersect G) = P(F) P(G). Also note if
P(F) > 0 and P(G) > 0 then P(F|G) = P(F). This definition can be extended for
collections of events.

We say two random variables are independent if P(X;Y) = P(X) P(Y).
If the random variable is discrete then its pmf specifies the probability.
P(X \in A) = \sum P(X=x)

Definitions of expectation and variance---------------------------------------
For discrete random variables it is as discussed in previous courses.
E(X) = \sum x P(X=x) = \sum x f(x)
Var(x) = E(X-E(X))^2
standard deviation = \sqrt{Var(x)}
k^{th} moment = E(X^k)

We now list the same properties for continuous functions
E(X) = \int_{reals} x f(x) dx.
Variance and moment are defined equivalent to the discrete case.
[05-06 contains examples of continuous distributions]

Helpful tools:
E(aX+b) = a(E(X)) + b (linearity of expectation)
Var(X) = E(X^2) - E(X)^2
Var(aX+b) = a^2 Var(X)
Var(X_1 + X_2 + ... + X_n) = \sum Var(X_i) assuming they are independent

We explore an example looking for a parameter \theta. We have a set of
random variables that X_1 ... X_n which are indpendent and identically
distributed (iid) with X_i \sim Unif[0,\theta].

We can calculate the sample mean in the usual way with our samples.
We consider 2\bar{X} (where the bar is the sample mean)
as a good estimator for our parameter.
They prove this [see 05-08] by taking the expectation of 2\bar{X} and
evaluating it to \theta.
We then ask the question, what is the variance of this estimator?
We solve this by evaluating E(\bar{\theta} - \theta)^2

Chebyshev's Inequality--------------------------------------------------------
If Y is a random variable and E(Y^2) < \infty then
P(|Y|>t) \leq \frac{1}{t^2} E(Y^2) for t>0

Proof: We define a random variable Z which takes event sets
Z(w) = t if |Y(w)| > t
Z(w) = 0 if |Y(w)| \leq t
Where each Y(i) is a new result
(think of an indicator that is either 0 or is less than Y(w) at all w)

Observe Y^2 \geq Z^2 so
E(Y^2) \geq E(Z^2) = t^2 P(Z=t) + 0P(Z=0) [since Z has only two options, it's either t or 0]
= t^2 P(|y|>t)

Thus we get our result
P(|Y|>t) \leq \frac{1}{t^2} E(Y^2) for t>0

Limits and probability--------------------------------------------------------
We say that a sequence of random variables converges in probability if,
lim_{n\to\infty} P(|y_n - y| > \epsilon) = 0

Using the same kind of MATH2400 ideas.

Weak law of large numbers-----------------------------------------------------
We now attempt to prove the sample mean converges in probability to the actual mean.

P(|\frac{1}{n} \sum X_i - \mu| > \epsilon) = P((\frac{1}{n} \sum X_i - \mu)^2 > \epsilon^2)
\leq \frac{1}{\epsilon^2} E((\frac{1}{n} \sum X_i - \mu)^2) [by Chebyshev's Inequality]

Now we prove E((\frac{1}{n} \sum X_i - \mu)^2) approaches 0
E(\bar{X} - \mu) = E(\bar{X} - E(\bar{X})) (a proof for this is similar the the example above)
= Var(\bar{X})
We note that the X_i that make up \bar{X} are independent. So,
Var(\bar{X}) = \frac{1}{n^2} \sum Var(X_i) = \frac{1}{n^2} \sum_{to n} \sigma^2
= \frac{1}{n}\sigma^2
As n \to \infty our result approaches 0.

Central limit theorem---------------------------------------------------------
No proof here but consider the sequence
Z_n = \sqrt{n} \frac{\bar{X} - \mu}{\sigma}

Z_\infty \sim N(0,1)

Family of distributions-----------------------------------------------------
Given a random variable X (usually some standard like the standard normal)
Y = a + bX where b > 0 can create the set also known as the *location-scale
family of distributions* associated with X. i.e. all normal distributions
are just the standard normal.

Probability theory vs statistics--------------------------------------------
Probability theory starts with a probability space or measure and is
interested in the likelihood of certain outcomes.

Statistics starts with an outcome and is interested in the probability space.

Statistics follows this cyclic flow:
We have reality/data which, when mixed with expert knowledge, generates a model.
Mathematical analysis is applied to the model to generate conclusions and
implications from the model. Expert knowledge comes back in to make conclusions
about reality. When/if we find data that contradicts our model then we're back
to a bunch of data with no model and we start again.

When statisticians gather data they try and calculate metrics about certain
properties. Sample median, sample mean, sample variance, sample quartiles.
They might even make graphical interpretations of the data through graphs and
boxplots etc.

QQ-plots
A \alpha quantile of a cdf is defined as the area under the curve that
amasses to \alpha (from the tail end).

The function that takes a quantile and returns the score is the anti-cdf.
In this case we refer to it as F^-1 (\alpha). The inverse of the cdf.

Interestingly the quantile is a linear "distance" [made that up] from any
other cdf in the location-scale family. Let F_{a,b}^{-1} represent the quantile
function of the a + bX where X is the "standard" random variable for that
family.
F_{a,b}^{-1} = a + b F^{-1}

A QQ-plot associates each data point with an index (determined by ordering the data).
The y-value for this plot then takes quantile function with respect to the
percentage of the way through the data.

If i is the index and n is the maximum then F^{-1} (i/n).

When modelling data we go through an interesting process.
We first assume our data fits a probability distribution P_\theta with
parameter (or set of parameters) \theta.

When we run our experiment we gather n independent realisations of random
variables which all share a distribution depending on some unknown \theta.

The question becomes how we find this theta and if our initial assumption
about our statistical model is valid.

We find our parameter by taking a function T that maps data to a value
close to the "true" parameter \theta.
An estimator or statistic is such a T that only depends on the data points.
An estimator cannot rely on the unknown parameter. i.e. T(X) = \expect (X)
won't work since knowing the expectation of the random variable requires
knowledge of the parameter (usually).

We now ask the question, "Can we measure the quality of the estimator?" how
quickly can we converge to the true parameter.

We create a metric called the mean-square error formed as,
MSE(\theta ; T) = \expect_\theta (T - \theta)^2
We desire a small MSE.

Note that his formula requires \theta. The final evaluation will contain
\theta in it. [see 09-08]

We can decompose this definition into
MSE(\theta ; T) = Var_\theta (T) + (\expect_\theta T - \theta)^2

In words, this means "T is equal to the variance + the squared bias"

[10-03 shows a proof of this decomposition.

The quality of an estimator becomes this balancing act between variance and
bias. Typically aiming for a small bias leads to a large variance and vice versa.
For example, given a distribution U[0,\theta] (uniform from 0 to \theta).
Can be estimated by 2\bar{X}. This estimator is unbiased but has a large
variance (2^2 I believe). On the opposite end the estimator \bar{X} has better
variance but is now biased.

[10-06 shows an example of analysing the quality of an estimator]
The conclusion of this example is that there could be multiple estimators that
unbiased and some that are objectively better. In this example a stratified
sample of the population gives a lower MSE (which is variance for unbiased).

Moments-------------------------------------------------------------------------
The k-th moment of a random variable X with some unknown parameter \theta is
given by \expect_\theta X^k.
The moment can be estimated "sensibly" by
\bar{X}_n^k = \frac{1}{n} \sum_{i=1}^n X_i^k

This is proven with the law of large numbers. Since we're summing a bunch of
iid random variables then dividing by the number of random variables we achieve
our result.

Method of moments estimator-----------------------------------------------------
This is defined as \hat{\theta} which satisfies,
\expect_\hat{\theta} X_i^k = \bar{X}_n^k
This means that the expectation from the estimated parameter \hat{\theta} for
any of the data points we have is equal to the global estimated moment.
[11-04 examples shown]

In practice, we prefer to calculate the smallest possible moment (smallest k).
In a multi dimensional case we take the smallest k such that our system of
equations is unique.

Multidimensional random variables-----------------------------------------------
We now try and quantify a multidimensional case. not only could there be
multiple variables to consider but some of these variables may be dependent
on one another.

We denote a random vector where each element of the vector is a random
variable. The joint CDF is fully defined by

F(X_1 \leq x_1; X_2 \leq x_2 etc)

We could extend this for discrete random variables via
\prob (X=s) where s is a vector containing all x_1, x_2 etc.

To find the probability of being in a family of states A,
\prob (X \in A) = \sum \prob(X = s)

The marginal pmf of X_1 is defined by
\prob (X_1 = x_1)
= \sum_{all other dimensions} \prob(X_1 = x_1; X_2 = x_2; X_3 = x_3 ...)

We can always get the marginal pmf from the join pmf but this is not
true in general for the other direction. It does work in general for
independent RVs.

A random vector has independent elements if and only if their marginal
PMFs can construct the joint PMF via,
\prob(X_1 = x_1; X_2 = x_2 ...) = \prob(X_1=x_1)\times\prob(X_2=x_2)...

In the continuous case instead of X_1 = x_1 we have a_1 \leq X_1 \leq b_1.
This is evaulated via multiple integrals.

You can find the marginal pdf f_{x_1} via
f_{X_1}(x_1) = \int_{x_2=-\infty}^\infty f_{X_1, X_2}(x_1, x_2) \diff x_2

The same if-and-only-if holds for continuous PDFs.

Since there's no real idea of score in a multidimensional case we introduce
a function h(X) which takes a realisation of our random vector and outputs
a single number. We then take multiple sums (in the discrete case)
\sum \ldots \sum h(x_1,x_2...)f(x_1,x_2...)

We measure how linearly dependent a variable is to another using
covariance. We also introduce the correlation coefficient \rho.
This is defined via
Cov(X_i,X_j) = \expect (X_i - \expect X_i)(X_j - \expect X_j)
\rho (X_i, X_j) = \frac{Cov(X_i, X_j)}{\sqrt{Var X_i}\sqrt{Var X_j}}
Note:
Cov(X_i, X_i) = Var(X_i)
Cov(X_i, X_j) = 0 [if X_i and X_j are independent]
\rho(X_i, X_j) \in [-1,+1]

It can be derived (assuming the expectation exists) that
Cov(X_i, X_j) = \expect X_i X_j - \expect X_i \expect X_j

It's clear that independent random variables implies a covariance of 0.
This is not true the other way however. Counterexample,
X = {-1, 0, +1} with equal probability and Y = X^2.
Cov(X,Y) = 0 despite the fact that they are dependent.

The expectation vector is when you take the expectation of each
element of the random vector.
The covariance matrix \sigma contains all combinations of covariances
of the vector. What's neat is that we can calculate the covariance matrix
via our definition of expectation.
\sigma = \expect (X-\mu)(X-\mu)^T

Also note that if all elements are independent then the \sigma will be the
identity matrix.

If we construct a new random variable Y
via a linear transformation of another X;
this is done through multiplying by a matrix A.
The mean of Y is just AX.
The covariance of Y is A Cov(X) A^T

[Proofs at 14-03]

Multivariate normal-------------------------------------------------------------
Given a random vector z containing iid standard normal distribution the joint
PDF of these distributions is,
f_Z (z) = \frac{1}{\sqrt{2\pi}^d} e^{-0.5 z^t z}

This z is called a multivariate standard normal.

We can generate a multivariate standard normal distribution (not standard) via
X = \mu + AZ
The expectation vector is \mu and the covariance is AA^T.

A strange point, the PDF (not PMF) of X does not always exist.
It does exist in the special case that A is an invertible matrix.
[14-07 shows the PDF of X]

Interestingly, in the multivariate normal case the covariance = 0 implies
independence. A multivariate normal distribution is independent iff
covariance = 0.

Also, the sample mean (treated as a random variable) and the sample variance
(also treated as a random variable) are independent.

Maximum likelihood (ML)---------------------------------------------------------
Suppose we're testing a biased coin multiple times. Suppose we get 3 heads for
data. The probability of this occurring (with unknown parameter p) is,
\prob_p (X=3) = {}^{10}C_3 p^3 (1-p)^7
The maximum likelihood method attempts to find the value \hat{p} that maximises
this equation.

If we have a vector of iid random variables then we could get the joint PMF/PDF
by multiplying all the single-variable PMF/PDFs. Of course this could turn
really ugly if we're trying to optimise the function.
\prod_{i=1}^n g_\theta (x_i)

Instead we transform it by taking the logarithm of the pdf/pmf. We can do this
since logarithms are strictly increasing functions.

From there the product becomes a summation of logarithms.
\log \prod_{i=1}^n g_\theta (x_i)
\sum_{i=1}^n \log g_\theta (x_i)

When we differentiate the logged version of our equation, our resulting maximum
point is valid (in the x axis).

Should be noted that the pdf/pmf with unknown \theta which gives the probability
of obtaining the datapoints we've retrieved is known as the likelihood function,
L(\theta;x). Note that the log transformed version of the
function is called the log-likelihood function.
We denote T(x) as a pmf/pdf that is a guess for \theta that maximises the
function for the given datapoints.

[lecture 16 week 6 provides examples of using likelihood functions. These
include Bernoulli, normal distribution and uniform]

[lecture 17 week 6 provides a recap of all stat2004]

Pivot variables----------------------------------------------------------------
A pivot variable (aka pivot qunatity or just pivot) is a theoretical
(and generally non-computable) transformation
of a random variable (i.e. data) that no longer depends on any unknown
parameters.
For example taking a normal distribution and doing the following transformation
will give you
(\bar{X}-\mu)/(\sigma/\sqrt{n}) ~ N(0,1)
Note that this transformation is relying on \sigma actually being known
This is made apparent when using this pivot transformation to determine a
confidence interval.

We now try and repeat this without knowing \sigma instead we have the sample
variance of S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2 which has a
chi-squared distribution; \frac{n-1}{\sigma^2}S^2 ~ \chi_{n-1}^2
Also note that \bar{X} is independent of S^2 which I guess makes sense.
We will make use of this fact to do the division below.

Now we set our ML to
T = \frac{\bar{X} - \mu}{S/\sqrt{n}}
&= \frac{\frac{\bar{X} - \mu}{\sigma/\sqrt{n}}}{S/\sigma}
&= \frac{N(0,1)}{\sqrt{\chi_{n-1}^2/(n-1)}}
&= T_{n-1}
Since this is the definition of the t distribution.
By repeating the distribution process that was previously done for standard
error margins we get the following [see page 3 lecture 19 week 7].
(\bar{X} \pm t^{(1-\alpha/2)}S/\sqrt{n})
please remember that degrees of freedom is n-1.

We repeat this process for sample variance. Since sample variance is being taken
from a chi-squared distribution, we have to build our confidence interval from
this chi-squared distribution. [see page 1 lecture 20 week 7]

Unfortunately we cannot do pivot transforms of discrete distributions because
"""reasons"""
The shifting and or scaling will change the support of the distribution to
something weird and something that always still depends on the unknown
parameter.

To get around this we can construct asymptotic pivots. We make use of the
Central Limit Theorem. So instead of directly calculating the ML of a binomial
directly instead we take \hat{p} = X/n where \hat{p} is the sample probability.
We also need to use se(\hat{p})=\sqrt{\hat{p}(1-\hat{p})}
Now from here we can use the asymptotic pivot transformation.
Of course the quetion now becomes when does the approximation hold? A good rule
of thumb is there must be 10 successes and 10 failures (whatever the definition
of success and failure are).

Hypothesis testing and decision making---------------------------------------
A hypothesis testing procedure (a test) is a decision rule that specifies when
to reject some H_0 (null hypothesis) in favor of H_1.

Hopefully this should all be familiar to you from SCIE1000 and STAT1201.

We define a decision rule before we run our test (it would be unethical to
determine our rule after the fact).
Perhaps something like if our sample mean meets some
cutoff, maybe if the smaple median meets a cutoff or maybe even if the minimum
data points meets some cutoff. There is no unique decision rule in general. We
have to inuitively feel that some rules will be better than other rules
depending on scenario and context.

The null hypothesis is the "boring" or "status quo" hypothesis that esesntially
claims that nothing is happening in this random experiment. The alternative is
typically the interesting hypothesis that claims that something is indeed
happening.

Mathematically speaking, the null and alternative hypothesis can be considered
symmetric. But there is generally a good and historical reason to treat the null
and alternative hypothesis differently. The reason is the impact of
a false decision in one way can have brutal consequences, in the other we have
very little consequences.

In this course we will only consider the *single point* null hypothesis of the
form H_0: \theta = \theta_0 for some hypothetical \theta_0.

The alternative is typically a *composite* hypothesis. Composite hypothesis
means that our hypothesis lands our parameter in some domain; \theta > cutoff.

Please note that it's by convention that we should always include our endpoints.

A test statistic is a function T()
that transforms the data. This is usually done in
context of whatever you're working on.

It should be noted that the test statistic transformation is different to the
decision rule. The decision rule is the cutoff we choose and if it's one-sided
or two-sided.

An even more general approach for constructing test statistics is the likelihood
ratio principle.

Likelihood ratio test-----------------------------------
Suppose both the alternative and null hypothesis H_0 and H_1 are single point
hypothesis. Now consider the ratio
\frac{L(\theta_0, \bar{X})}{L(\theta_1, \bar{X})} < c

The numerator is the null hypothesis and c is the cutoff value.

Observe, if the ratio was small then the data would have been more likely to be
observed under \theta_1 than \theta_0. Thus there would be evidence against H_0
in favor of H_1.

A uniformly most powerful test is a test that is the gives the greatest power
compared to all other tests on the given dataset \alpha.

This LRT is the most powerful test possible as proven by
Neyman-Pearson. It is also the most uniformly most powerful test for testing
a composite alternative hypothesis.
[An example of doing LRT is done in 30:00 16 Sept]

Two types of errors----------------------------------------------------------
[see in imgs/errors.png]

Type I error is the significance level,
\alpha = P(T(X)\in C | H_0)

Type II error related to power by $1 - \beta$
\beta = 1 - P(T(X) \in C | H_1)

This is harder to calculate since the H_1 is a composite hypothesis we actually
need to sum all of them to get the proper result. Another way to see is the
power for each possible realisation of the unknown parameter. This is called a
power curve.

In the justice system, we let the null hypothesis be innocence and the alternate
hypothesis be guilty. Having zero Type I error means we let everyone (including
criminals) go and
always choose the null hypothesis.
Having zero Type II error means we jail
everyone and always choose the alternative hypothesis.

Neyman-Pearson---------------------------------------
Neyman-Pearson proposed a maximisation for reducing both Type I and Type II
errors. The idea is that we aim to construct a test that
1. restricts the Type I error to an appropriately small level \alpha
2. Now we do everything we can to attempt to maximise power given our
predetermined \alpha

Once both conditions/actions are met the resulting test is called a
"most powerful test at level \alpha".

Should be noted that the student t test is "the best" with the obstacle of not
knowing variance. If variance is known then the standard normal confidence
interval is a better test.

The t test can actually be asymptotically most powerful even for non-normal
data.

[10:00 18 September has an excellent example of developing a confidence interval
for Poisson then comparing it to t test]

The results of this example tell us that knowing the model gives us very tight
and accurate bounds for our test but the t test still performs well even with
small sample sizes.

Before any kind of numerical data gathering takes place we should have:
identified the parameter(s) of interest,
formulated null and alternative hypothesis about the parameter(s),
choose a test statistic and the rejection region (the cutoff),
setting the Type I error,
analyse the power of the decision rule,

General thresholds and terminology used for p-values--------------------
A p-value can only reside within 1 and 0.
Between 0 and 0.01 then "very strong evidence"
Between 0.01 and 0.05 then "moderately strong"
Between 0.05 and 0.1 then "marginal strength"
Anything else "little to no evidence"

Duality of pvalues and confidence intervals---------------------
By writing a confidence interval (a, b) you're telling us the range of
values that would not be rejected if were the null hypothesis.

Nuance with pvalues-------------------------------------
A pvalue will never provide evidence FOR the null hypothesis.
All that can be done is provide evidence against or show that the evidence
is consistent with the null hypothesis.

This is because the truth is very very close to the null hypothesis but
not exactly the null hypothesis. As a matter of fact there is no way
to provide evidence for a null hypothesis.

Generalised likelihood ratio tests----------------------------------------
The original issue with the likelihood ratio test was that it required
single point hypothesis. With this new method we can fix this and also
make use of multidimensional parameters.

We now consider H_0 \in \Theta_0 where \Theta_0 is a given subset.
Likewise H_1 \in \Theta_1 where \Theta_0 and \Theta_1 are disjoint subsets.

With this new, generalised definition the idea of "two-sided" no longer
makes sense. Instead consider a "out-sided".

[see outsided.png]

Now the actual formula,
\frac{sup L(\theta | X)}{sup L(\theta | X)}

Where the numerator is the best possible likelihood of the data under the
null hypothesis and the denominator is the same for the alternative
hypothesis.

Usually we have a single point null hypothesis so the numerator is easy.
The denominator will be the sample proportion since intuitively this is the
best guess.

In this same way the previous model concluded. If this fraction is "small"
then it's more likely that the alternative hypothesis was right. If the
fraction is not "small" then it's more likely that the null hypothesis was
right.

[45:00 24 Sep to the lecture after is a good example of a use case for this]

This GLRT can be converted asymptotically
into the Pearson's Chi^2 test statistic.
-2 log \Lambda = Pearson's Chi^2

The GLRT is actually more exact that Pearson's Chi^2 but the Pearson statistic
is commonly used because legacy reasons (pre-computer calculations are easy).

Once you have a Pearson Chi^2 value (with appropriate degrees of freedom)
use a calculator to determine the pvalue from this.

Multinomial distribution----------------------------------------------------
Multinomial distribution is an extension of the binomial distribution.
Much like how in the binomial distribution you actually only have 1 free
parameter in the multinomial situation we have k-1 parameters for
p_1 ... p_k.

The PMF for the multinomial is
P(X_1 = x_1, X_2 = x_2, X_3 = x_3, X_4 = x_4 ...)
= (n x) p_1^{x_1} p_2^{x_2} p_3^{x_3} p_4^{x_4} ...
Where the brackets at the start are the multinomial coefficient.
\frac{n!}{x_1! x_2! x_3! x_3! x_4! ...}

[Finished up to midsem break]
