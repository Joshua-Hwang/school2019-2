Probability theory is concerned with mathematical
models for uncertainty.

Th starting point is the idea of a 'random experiment'

The idea of a random experiment is captured by the
mathematical idea of a *probability space*. It is a core concept
in probability theory and statistics.

Thus to represent a probability space we need a triplet.
(Omega, F, P)
A probability space comprises:
* (Omega) a set of all possible outcomes of the random experiment
* (F) a collection of all possible combinations of events that may occur (the die rolls a prime) (all subsets) (the power set of Omega)
* (P) a function that tells us how likely each even is

The powerset is trivially the event set. It is not equal to however, there
can be event sets smaller than the powerset. (Take the second example on page 02)

Definition of Omega------------------------------------------------
Omega is a non-empty set called the sample space,
F is an event set on Omega
P is a probability of F.

Definition of F------------------------------------------------------
F is nonempty
If A is in F then the complement is also in F. (Complement with respect to Omega)
The union of some unique subsets of F is an element of F.

The power set of Omega is an event set.

Claims-----------------------------------------------------------------
The subsets of an event set are closed under union.
Proof: Consider duplicates A and B, they will cancel to a single element. A union B will give A.
Repeating this sends you to the definition stated eariler. (see page 02-04)

Omega is an element of the event set but also the empty set is an element of the event set.
Proof: Since F is non-empty there exists A in F. We also now the complement of A exists in F.
The union of A and complement of A gives F. We may do this because of the previous proof.

The intersection of subsets is also a subset.
[figure out yourself]

The abritrary set of intersections is also an element of F


Definition of P---------------------------------------------------------
(aka probability measure)
P can be seen as a function that takes an event set and maps it to a real number.
The obvious limits of the function must apply,
P >= 0 for all F
P(Omega) = 1
P(Union of As) = sum of each P(A) (must be unions of disjoint) (is infinite sum/union)

Claims-----------------------------------------------------------------
P(empty set) = 0
Proof: Omega = Omega union empty set.
We convert union into sum and conclude P(empty set) = 0
(I have given up trying to write these down. See 02-07)

P is finitely additive:
If we have A1, A2 ... An which are all dijoint then
the union to sum definition holds for finite n sets.

Claims-----------------------------------------------------------------
P(A) + P(complement A) = 1
Proof: Union of A and complement of A give Omega. Using this union
and definition of P(Omega) we can prove this. (see page 02-04)

If A is a strict subset of B then P(A) <= P(B)
Proof: replaces B as A union (B interset complement A) (see page 02-08)

[Stopped at 27 July Lecture 3]

Definition of random variable------------------------------------------------
A random variable is defined as a mapping from Omega to the reals.
With an extra requirement that the sets {w in Omega : X(w) <= x} for some x are
events in F.

The idea of using an element from Omega seems confusing. I think it's a way
to decouple the idea of events with a score.

A random set of students turn up to class on particular day (Omega)
X is the number of students who attend (Omega to Integer which is a subset of reals)
Y is the average grade of the students who attended (Omega to Reals)

Clearly many random variables can be produced from Omega.

This enitre course assumes the sets {w in Omega : X(w) <= x} for some x are
events in F always holds.

Example:
Let Omega = {(h,w) : h, w > 0} be a sample space for the height and weight of
a random individual. We define a random variable X which is that persons BMI.
X(h,w) = w/h^2

We can specify the probability distribution of random variable X if we can
specify the probability of each event involving X.
As a matter of fact we can create a function that perfectly encapsulates this
distribution F_x (x) = P(X \leq x) for all x in the reals.

We've seen this function before and it's called the cumulative distribution
function. This function takes values in [0,1] is right-continuous and is
non-decreasing.

We say a random variable is discrete or has a discrete distribution if X takes
values in a set (at most countably infinite) with P(X=x_i) > 0. In this case
we define a probability mass function (PMF) by f(x) = P(X=x) (different to
what was discussed before).

A continuous random variable has a distribution defined by
P(X \in A) = \int_A f(x) dx for the event set A. This function is called
the probability density function. The function is defined on the reals
(or something that can map to it)
and takes values in [0, \infty)

Definitions of Independence---------------------------------------------------
Two events are independent if P(F intersect G) = P(F) P(G). Also note if
P(F) > 0 and P(G) > 0 then P(F|G) = P(F). This definition can be extended for
collections of events.

We say two random variables are independent if P(X;Y) = P(X) P(Y).
If the random variable is discrete then its pmf specifies the probability.
P(X \in A) = \sum P(X=x)

Definitions of expectation and variance---------------------------------------
For discrete random variables it is as discussed in previous courses.
E(X) = \sum x P(X=x) = \sum x f(x)
Var(x) = E(X-E(X))^2
standard deviation = \sqrt{Var(x)}
k^{th} moment = E(X^k)

We now list the same properties for continuous functions
E(X) = \int_{reals} x f(x) dx.
Variance and moment are defined equivalent to the discrete case.
[05-06 contains examples of continuous distributions]

Helpful tools:
E(aX+b) = a(E(X)) + b (linearity of expectation)
Var(X) = E(X^2) - E(X)^2
Var(aX+b) = a^2 Var(X)
Var(X_1 + X_2 + ... + X_n) = \sum Var(X_i) assuming they are independent

We explore an example looking for a parameter \theta. We have a set of
random variables that X_1 ... X_n which are indpendent and identically
distributed (iid) with X_i \sim Unif[0,\theta].

We can calculate the sample mean in the usual way with our samples.
We consider 2\bar{X} (where the bar is the sample mean)
as a good estimator for our parameter.
They prove this [see 05-08] by taking the expectation of 2\bar{X} and
evaluating it to \theta.
We then ask the question, what is the variance of this estimator?
We solve this by evaluating E(\bar{\theta} - \theta)^2

Chebyshev's Inequality--------------------------------------------------------
If Y is a random variable and E(Y^2) < \infty then
P(|Y|>t) \leq \frac{1}{t^2} E(Y^2) for t>0

Proof: We define a random variable Z which takes event sets
Z(w) = t if |Y(w)| > t
Z(w) = 0 if |Y(w)| \leq t
Where each Y(i) is a new result
(think of an indicator that is either 0 or is less than Y(w) at all w)

Observe Y^2 \geq Z^2 so
E(Y^2) \geq E(Z^2) = t^2 P(Z=t) + 0P(Z=0) [since Z has only two options, it's either t or 0]
= t^2 P(|y|>t)

Thus we get our result
P(|Y|>t) \leq \frac{1}{t^2} E(Y^2) for t>0

Limits and probability--------------------------------------------------------
We say that a sequence of random variables converges in probability if,
lim_{n\to\infty} P(|y_n - y| > \epsilon) = 0

Using the same kind of MATH2400 ideas.

Weak law of large numbers-----------------------------------------------------
We now attempt to prove the sample mean converges in probability to the actual mean.

P(|\frac{1}{n} \sum X_i - \mu| > \epsilon) = P((\frac{1}{n} \sum X_i - \mu)^2 > \epsilon^2)
\leq \frac{1}{\epsilon^2} E((\frac{1}{n} \sum X_i - \mu)^2) [by Chebyshev's Inequality]

Now we prove E((\frac{1}{n} \sum X_i - \mu)^2) approaches 0
E(\bar{X} - \mu) = E(\bar{X} - E(\bar{X})) (a proof for this is similar the the example above)
= Var(\bar{X})
We note that the X_i that make up \bar{X} are independent. So,
Var(\bar{X}) = \frac{1}{n^2} \sum Var(X_i) = \frac{1}{n^2} \sum_{to n} \sigma^2
= \frac{1}{n}\sigma^2
As n \to \infty our result approaches 0.

Central limit theorem---------------------------------------------------------
No proof here but consider the sequence
Z_n = \sqrt{n} \frac{\bar{X} - \mu}{\sigma}

Z_\infty \sim N(0,1)

Family of distributions-----------------------------------------------------
Given a random variable X (usually some standard like the standard normal)
Y = a + bX where b > 0 can create the set also known as the *location-scale
family of distributions* associated with X. i.e. all normal distributions
are just the standard normal.

Probability theory vs statistics--------------------------------------------
Probability theory starts with a probability space or measure and is
interested in the likelihood of certain outcomes.

Statistics starts with an outcome and is interested in the probability space.

Statistics follows this cyclic flow:
We have reality/data which, when mixed with expert knowledge, generates a model.
Mathematical analysis is applied to the model to generate conclusions and
implications from the model. Expert knowledge comes back in to make conclusions
about reality. When/if we find data that contradicts our model then we're back
to a bunch of data with no model and we start again.

When statisticians gather data they try and calculate metrics about certain
properties. Sample median, sample mean, sample variance, sample quartiles.
They might even make graphical interpretations of the data through graphs and
boxplots etc.

QQ-plots
A \alpha quantile of a cdf is defined as the area under the curve that
amasses to \alpha (from the tail end).

The function that takes a quantile and returns the score is the anti-cdf.
In this case we refer to it as F^-1 (\alpha). The inverse of the cdf.

Interestingly the quantile is a linear "distance" [made that up] from any
other cdf in the location-scale family. Let F_{a,b}^{-1} represent the quantile
function of the a + bX where X is the "standard" random variable for that
family.
F_{a,b}^{-1} = a + b F^{-1}

A QQ-plot associates each data point with an index (determined by ordering the data).
The y-value for this plot then takes quantile function with respect to the
percentage of the way through the data.

If i is the index and n is the maximum then F^{-1} (i/n).

[ZZZ Week 3 Lecture 8]
When modelling data we go through an interesting process.
We first assume our data fits a probability distribution P_\theta with
parameter (or set of parameters) \theta.

When we run our experiment we gather n independent realisations of random
variables which all share a distribution depending on some unknown \theta.

The question becomes how we find this theta and if our initial assumption
about our statistical model is valid.

We find our parameter by taking a function T that maps data to a value
close to the "true" parameter \theta.
An estimator or statistic is such a T that only depends on the data points.
An estimator cannot rely on the unknown parameter. i.e. T(X) = \expect (X)
won't work since knowing the expectation of the random variable requires
knowledge of the parameter (usually).

We now ask the question, "Can we measure the quality of the estimator?" how
quickly can we converge to the true parameter.

We create a metric called the mean-square error formed as,
MSE(\theta ; T) = \expect_\theta (T - \theta)^2
We desire a small MSE.

Note that his formula requires \theta. The final evaluation will contain
\theta in it. [see 09-08]
