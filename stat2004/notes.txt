Probability theory is concerned with mathematical
models for uncertainty.

Th starting point is the idea of a 'random experiment'

The idea of a random experiment is captured by the
mathematical idea of a *probability space*. It is a core concept
in probability theory and statistics.

Thus to represent a probability space we need a triplet.
(Omega, F, P)
A probability space comprises:
* (Omega) a set of all possible outcomes of the random experiment
* (F) a collection of all possible combinations of events that may occur (the die rolls a prime) (all subsets) (the power set of Omega)
* (P) a function that tells us how likely each even is

The powerset is trivially the event set. It is not equal to however, there
can be event sets smaller than the powerset. (Take the second example on page 02)

Definition of Omega------------------------------------------------
Omega is a non-empty set called the sample space,
F is an event set on Omega
P is a probability of F.

Definition of F------------------------------------------------------
F is nonempty
If A is in F then the complement is also in F. (Complement with respect to Omega)
The union of some unique subsets of F is an element of F.

The power set of Omega is an event set.

Claims-----------------------------------------------------------------
The subsets of an event set are closed under union.
Proof: Consider duplicates A and B, they will cancel to a single element. A union B will give A.
Repeating this sends you to the definition stated eariler. (see page 02-04)

Omega is an element of the event set but also the empty set is an element of the event set.
Proof: Since F is non-empty there exists A in F. We also now the complement of A exists in F.
The union of A and complement of A gives F. We may do this because of the previous proof.

The intersection of subsets is also a subset.
[figure out yourself]

The abritrary set of intersections is also an element of F


Definition of P---------------------------------------------------------
(aka probability measure)
P can be seen as a function that takes an event set and maps it to a real number.
The obvious limits of the function must apply,
P >= 0 for all F
P(Omega) = 1
P(Union of As) = sum of each P(A) (must be unions of disjoint) (is infinite sum/union)

Claims-----------------------------------------------------------------
P(empty set) = 0
Proof: Omega = Omega union empty set.
We convert union into sum and conclude P(empty set) = 0
(I have given up trying to write these down. See 02-07)

P is finitely additive:
If we have A1, A2 ... An which are all dijoint then
the union to sum definition holds for finite n sets.

Claims-----------------------------------------------------------------
P(A) + P(complement A) = 1
Proof: Union of A and complement of A give Omega. Using this union
and definition of P(Omega) we can prove this. (see page 02-04)

If A is a strict subset of B then P(A) <= P(B)
Proof: replaces B as A union (B interset complement A) (see page 02-08)

[Stopped at 27 July Lecture 3]

Definition of random variable------------------------------------------------
A random variable is defined as a mapping from Omega to the reals.
With an extra requirement that the sets {w in Omega : X(w) <= x} for some x are
events in F.

The idea of using an element from Omega seems confusing. I think it's a way
to decouple the idea of events with a score.

A random set of students turn up to class on particular day (Omega)
X is the number of students who attend (Omega to Integer which is a subset of reals)
Y is the average grade of the students who attended (Omega to Reals)

Clearly many random variables can be produced from Omega.

This enitre course assumes the sets {w in Omega : X(w) <= x} for some x are
events in F always holds.

Example:
Let Omega = {(h,w) : h, w > 0} be a sample space for the height and weight of
a random individual. We define a random variable X which is that persons BMI.
X(h,w) = w/h^2

We can specify the probability distribution of random variable X if we can
specify the probability of each event involving X.
As a matter of fact we can create a function that perfectly encapsulates this
distribution F_x (x) = P(X \leq x) for all x in the reals.

We've seen this function before and it's called the cumulative distribution
function. This function takes values in [0,1] is right-continuous and is
non-decreasing.

We say a random variable is discrete or has a discrete distribution if X takes
values in a set (at most countably infinite) with P(X=x_i) > 0. In this case
we define a probability mass function (PMF) by f(x) = P(X=x) (different to
what was discussed before).

A continuous random variable has a distribution defined by
P(X \in A) = \int_A f(x) dx for the event set A. This function is called
the probability density function. The function is defined on the reals
(or something that can map to it)
and takes values in [0, \infty)

Definitions of Independence---------------------------------------------------
Two events are independent if P(F intersect G) = P(F) P(G). Also note if
P(F) > 0 and P(G) > 0 then P(F|G) = P(F). This definition can be extended for
collections of events.

We say two random variables are independent if P(X;Y) = P(X) P(Y).
If the random variable is discrete then its pmf specifies the probability.
P(X \in A) = \sum P(X=x)

Definitions of expectation and variance---------------------------------------
For discrete random variables it is as discussed in previous courses.
E(X) = \sum x P(X=x) = \sum x f(x)
Var(x) = E(X-E(X))^2
standard deviation = \sqrt{Var(x)}
k^{th} moment = E(X^k)

We now list the same properties for continuous functions
E(X) = \int_{reals} x f(x) dx.
Variance and moment are defined equivalent to the discrete case.
[05-06 contains examples of continuous distributions]

Helpful tools:
E(aX+b) = a(E(X)) + b (linearity of expectation)
Var(X) = E(X^2) - E(X)^2
Var(aX+b) = a^2 Var(X)
Var(X_1 + X_2 + ... + X_n) = \sum Var(X_i) assuming they are independent

We explore an example looking for a parameter \theta. We have a set of
random variables that X_1 ... X_n which are indpendent and identically
distributed (iid) with X_i \sim Unif[0,\theta].

We can calculate the sample mean in the usual way with our samples.
We consider 2\bar{X} (where the bar is the sample mean)
as a good estimator for our parameter.
They prove this [see 05-08] by taking the expectation of 2\bar{X} and
evaluating it to \theta.
We then ask the question, what is the variance of this estimator?
We solve this by evaluating E(\bar{\theta} - \theta)^2

Chebyshev's Inequality--------------------------------------------------------
If Y is a random variable and E(Y^2) < \infty then
P(|Y|>t) \leq \frac{1}{t^2} E(Y^2) for t>0

Proof: We define a random variable Z which takes event sets
Z(w) = t if |Y(w)| > t
Z(w) = 0 if |Y(w)| \leq t
Where each Y(i) is a new result
(think of an indicator that is either 0 or is less than Y(w) at all w)

Observe Y^2 \geq Z^2 so
E(Y^2) \geq E(Z^2) = t^2 P(Z=t) + 0P(Z=0) [since Z has only two options, it's either t or 0]
= t^2 P(|y|>t)

Thus we get our result
P(|Y|>t) \leq \frac{1}{t^2} E(Y^2) for t>0

Limits and probability--------------------------------------------------------
We say that a sequence of random variables converges in probability if,
lim_{n\to\infty} P(|y_n - y| > \epsilon) = 0

Using the same kind of MATH2400 ideas.

Weak law of large numbers-----------------------------------------------------
We now attempt to prove the sample mean converges in probability to the actual mean.

P(|\frac{1}{n} \sum X_i - \mu| > \epsilon) = P((\frac{1}{n} \sum X_i - \mu)^2 > \epsilon^2)
\leq \frac{1}{\epsilon^2} E((\frac{1}{n} \sum X_i - \mu)^2) [by Chebyshev's Inequality]

Now we prove E((\frac{1}{n} \sum X_i - \mu)^2) approaches 0
E(\bar{X} - \mu) = E(\bar{X} - E(\bar{X})) (a proof for this is similar the the example above)
= Var(\bar{X})
We note that the X_i that make up \bar{X} are independent. So,
Var(\bar{X}) = \frac{1}{n^2} \sum Var(X_i) = \frac{1}{n^2} \sum_{to n} \sigma^2
= \frac{1}{n}\sigma^2
As n \to \infty our result approaches 0.

Central limit theorem---------------------------------------------------------
No proof here but consider the sequence
Z_n = \sqrt{n} \frac{\bar{X} - \mu}{\sigma}

Z_\infty \sim N(0,1)

Family of distributions-----------------------------------------------------
Given a random variable X (usually some standard like the standard normal)
Y = a + bX where b > 0 can create the set also known as the *location-scale
family of distributions* associated with X. i.e. all normal distributions
are just the standard normal.

Probability theory vs statistics--------------------------------------------
Probability theory starts with a probability space or measure and is
interested in the likelihood of certain outcomes.

Statistics starts with an outcome and is interested in the probability space.

Statistics follows this cyclic flow:
We have reality/data which, when mixed with expert knowledge, generates a model.
Mathematical analysis is applied to the model to generate conclusions and
implications from the model. Expert knowledge comes back in to make conclusions
about reality. When/if we find data that contradicts our model then we're back
to a bunch of data with no model and we start again.

When statisticians gather data they try and calculate metrics about certain
properties. Sample median, sample mean, sample variance, sample quartiles.
They might even make graphical interpretations of the data through graphs and
boxplots etc.

QQ-plots
A \alpha quantile of a cdf is defined as the area under the curve that
amasses to \alpha (from the tail end).

The function that takes a quantile and returns the score is the anti-cdf.
In this case we refer to it as F^-1 (\alpha). The inverse of the cdf.

Interestingly the quantile is a linear "distance" [made that up] from any
other cdf in the location-scale family. Let F_{a,b}^{-1} represent the quantile
function of the a + bX where X is the "standard" random variable for that
family.
F_{a,b}^{-1} = a + b F^{-1}

A QQ-plot associates each data point with an index (determined by ordering the data).
The y-value for this plot then takes quantile function with respect to the
percentage of the way through the data.

If i is the index and n is the maximum then F^{-1} (i/n).

When modelling data we go through an interesting process.
We first assume our data fits a probability distribution P_\theta with
parameter (or set of parameters) \theta.

When we run our experiment we gather n independent realisations of random
variables which all share a distribution depending on some unknown \theta.

The question becomes how we find this theta and if our initial assumption
about our statistical model is valid.

We find our parameter by taking a function T that maps data to a value
close to the "true" parameter \theta.
An estimator or statistic is such a T that only depends on the data points.
An estimator cannot rely on the unknown parameter. i.e. T(X) = \expect (X)
won't work since knowing the expectation of the random variable requires
knowledge of the parameter (usually).

We now ask the question, "Can we measure the quality of the estimator?" how
quickly can we converge to the true parameter.

We create a metric called the mean-square error formed as,
MSE(\theta ; T) = \expect_\theta (T - \theta)^2
We desire a small MSE.

Note that his formula requires \theta. The final evaluation will contain
\theta in it. [see 09-08]

We can decompose this definition into
MSE(\theta ; T) = Var_\theta (T) + (\expect_\theta T - \theta)^2

In words, this means "T is equal to the variance + the squared bias"

[10-03 shows a proof of this decomposition.

The quality of an estimator becomes this balancing act between variance and
bias. Typically aiming for a small bias leads to a large variance and vice versa.
For example, given a distribution U[0,\theta] (uniform from 0 to \theta).
Can be estimated by 2\bar{X}. This estimator is unbiased but has a large
variance (2^2 I believe). On the opposite end the estimator \bar{X} has better
variance but is now biased.

[10-06 shows an example of analysing the quality of an estimator]
The conclusion of this example is that there could be multiple estimators that
unbiased and some that are objectively better. In this example a stratified
sample of the population gives a lower MSE (which is variance for unbiased).

Moments-------------------------------------------------------------------------
The k-th moment of a random variable X with some unknown parameter \theta is
given by \expect_\theta X^k.
The moment can be estimated "sensibly" by
\bar{X}_n^k = \frac{1}{n} \sum_{i=1}^n X_i^k

This is proven with the law of large numbers. Since we're summing a bunch of
iid random variables then dividing by the number of random variables we achieve
our result.

Method of moments estimator-----------------------------------------------------
This is defined as \hat{\theta} which satisfies,
\expect_\hat{\theta} X_i^k = \bar{X}_n^k
This means that the expectation from the estimated parameter \hat{\theta} for
any of the data points we have is equal to the global estimated moment.
[11-04 examples shown]

In practice, we prefer to calculate the smallest possible moment (smallest k).
In a multi dimensional case we take the smallest k such that our system of
equations is unique.

Multidimensional random variables-----------------------------------------------
We now try and quantify a multidimensional case. not only could there be
multiple variables to consider but some of these variables may be dependent
on one another.

We denote a random vector where each element of the vector is a random
variable. The joint CDF is fully defined by

F(X_1 \leq x_1; X_2 \leq x_2 etc)

We could extend this for discrete random variables via
\prob (X=s) where s is a vector containing all x_1, x_2 etc.

To find the probability of being in a family of states A,
\prob (X \in A) = \sum \prob(X = s)

The marginal pmf of X_1 is defined by
\prob (X_1 = x_1)
= \sum_{all other dimensions} \prob(X_1 = x_1; X_2 = x_2; X_3 = x_3 ...)

We can always get the marginal pmf from the join pmf but this is not
true in general for the other direction. It does work in general for
independent RVs.

A random vector has independent elements if and only if their marginal
PMFs can construct the joint PMF via,
\prob(X_1 = x_1; X_2 = x_2 ...) = \prob(X_1=x_1)\times\prob(X_2=x_2)...

In the continuous case instead of X_1 = x_1 we have a_1 \leq X_1 \leq b_1.
This is evaulated via multiple integrals.

You can find the marginal pdf f_{x_1} via
f_{X_1}(x_1) = \int_{x_2=-\infty}^\infty f_{X_1, X_2}(x_1, x_2) \diff x_2

The same if-and-only-if holds for continuous PDFs.

Since there's no real idea of score in a multidimensional case we introduce
a function h(X) which takes a realisation of our random vector and outputs
a single number. We then take multiple sums (in the discrete case)
\sum \ldots \sum h(x_1,x_2...)f(x_1,x_2...)

We measure how linearly dependent a variable is to another using
covariance. We also introduce the correlation coefficient \rho.
This is defined via
Cov(X_i,X_j) = \expect (X_i - \expect X_i)(X_j - \expect X_j)
\rho (X_i, X_j) = \frac{Cov(X_i, X_j)}{\sqrt{Var X_i}\sqrt{Var X_j}}
Note:
Cov(X_i, X_i) = Var(X_i)
Cov(X_i, X_j) = 0 [if X_i and X_j are independent]
\rho(X_i, X_j) \in [-1,+1]

It can be derived (assuming the expectation exists) that
Cov(X_i, X_j) = \expect X_i X_j - \expect X_i \expect X_j

It's clear that independent random variables implies a covariance of 0.
This is not true the other way however. Counterexample,
X = {-1, 0, +1} with equal probability and Y = X^2.
Cov(X,Y) = 0 despite the fact that they are dependent.

The expectation vector is when you take the expectation of each
element of the random vector.
The covariance matrix \sigma contains all combinations of covariances
of the vector. What's neat is that we can calculate the covariance matrix
via our definition of expectation.
\sigma = \expect (X-\mu)(X-\mu)^T

Also note that if all elements are independent then the \sigma will be the
identity matrix.

If we construct a new random variable Y
via a linear transformation of another X;
this is done through multiplying by a matrix A.
The mean of Y is just AX.
The covariance of Y is A Cov(X) A^T

[Proofs at 14-03]

Multivariate normal-------------------------------------------------------------
Given a random vector z containing iid standard normal distribution the joint
PDF of these distributions is,
f_Z (z) = \frac{1}{\sqrt{2\pi}^d} e^{-0.5 z^t z}

This z is called a multivariate standard normal.

We can generate a multivariate standard normal distribution (not standard) via
X = \mu + AZ
The expectation vector is \mu and the covariance is AA^T.

A strange point, the PDF (not PMF) of X does not always exist.
It does exist in the special case that A is an invertible matrix.
[14-07 shows the PDF of X]

Interestingly, in the multivariate normal case the covariance = 0 implies
independence. A multivariate normal distribution is independent iff
covariance = 0.

Also, the sample mean (treated as a random variable) and the sample variance
(also treated as a random variable) are independent.

[ZZZ Week 5 Lecture 3]
